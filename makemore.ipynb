{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella', 'sophia']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = open('names.txt').read().splitlines()\n",
    "words[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e m\n",
      "m m\n",
      "m a\n"
     ]
    }
   ],
   "source": [
    "for word in words[:1]:\n",
    "    for char, char1 in zip(word, word[1:]):\n",
    "        print(char, char1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 1,\n",
       " 'b': 2,\n",
       " 'c': 3,\n",
       " 'd': 4,\n",
       " 'e': 5,\n",
       " 'f': 6,\n",
       " 'g': 7,\n",
       " 'h': 8,\n",
       " 'i': 9,\n",
       " 'j': 10,\n",
       " 'k': 11,\n",
       " 'l': 12,\n",
       " 'm': 13,\n",
       " 'n': 14,\n",
       " 'o': 15,\n",
       " 'p': 16,\n",
       " 'q': 17,\n",
       " 'r': 18,\n",
       " 's': 19,\n",
       " 't': 20,\n",
       " 'u': 21,\n",
       " 'v': 22,\n",
       " 'w': 23,\n",
       " 'x': 24,\n",
       " 'y': 25,\n",
       " 'z': 26,\n",
       " '.': 0}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "letters = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i, s in enumerate(letters)}\n",
    "stoi['.'] = 0\n",
    "stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'a',\n",
       " 2: 'b',\n",
       " 3: 'c',\n",
       " 4: 'd',\n",
       " 5: 'e',\n",
       " 6: 'f',\n",
       " 7: 'g',\n",
       " 8: 'h',\n",
       " 9: 'i',\n",
       " 10: 'j',\n",
       " 11: 'k',\n",
       " 12: 'l',\n",
       " 13: 'm',\n",
       " 14: 'n',\n",
       " 15: 'o',\n",
       " 16: 'p',\n",
       " 17: 'q',\n",
       " 18: 'r',\n",
       " 19: 's',\n",
       " 20: 't',\n",
       " 21: 'u',\n",
       " 22: 'v',\n",
       " 23: 'w',\n",
       " 24: 'x',\n",
       " 25: 'y',\n",
       " 26: 'z',\n",
       " 0: '.'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "itos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = torch.zeros((27,27), dtype=torch.int64)\n",
    "N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = {}\n",
    "for word in words:\n",
    "    w_start_end = '.' + word + '.'\n",
    "    for char, char1 in zip(w_start_end, w_start_end[1:]):\n",
    "        counts[(char, char1)] = counts.get((char, char1), 0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 556, 4410, 1306, 1542, 1690, 1531,  417,  669,  874,  591, 2422, 2963,\n",
       "         1572, 2538, 1146,  394,  515,   92, 1639, 2055, 1308,   78,  376,  307,\n",
       "          134,  535,  929],\n",
       "        [6640,  556,  541,  470, 1042,  692,  134,  168, 2332, 1650,  175,  568,\n",
       "         2528, 1634, 5438,   63,   82,   60, 3264, 1118,  687,  381,  834,  161,\n",
       "          182, 2050,  435],\n",
       "        [ 114,  321,   38,    1,   65,  655,    2,  664,   41,  217,    1,  116,\n",
       "          103,    0,    4,  105,   11,   76,  842,    8,    2,   45,    0,    3,\n",
       "          104,   83,    0],\n",
       "        [  97,  815,    3,   42,    1,  551,   25,    2,  664,  271,    3,  316,\n",
       "          116,   31,  378,  380,    1,   11,   76,    5,   35,   35,   23,    0,\n",
       "            3,  104,    4],\n",
       "        [ 516, 1303,    1,    3,  149, 1283,    5,   25,  118,  674,    9,    3,\n",
       "           60,   30,   31,  378,   14,    1,  424,   29,    4,   92,   17,   23,\n",
       "         1070,  317,    1],\n",
       "        [3983,  679,  121,  153,  384, 1271,   82,  125,  152,  818,   55,  178,\n",
       "         3248,  769, 2675,  269,   83,   14, 1958,  861,  580,   69,  463,   50,\n",
       "          132, 1070,  181],\n",
       "        [  80,  242,    0,   19,  334,  123,   44,    1,    1,  160,    0,    2,\n",
       "           20,   27,    4,   60,    0,  201,  114,    6,   18,   10,   26,    4,\n",
       "           31,   14,    2],\n",
       "        [ 108,  330,    3,   24,   19,  334,    1,   25,  360,  190,    3,  185,\n",
       "           32,    6,   27,   83,    1,  204,  201,   30,   31,   85,    1,   26,\n",
       "          213,   31,    1],\n",
       "        [2409, 2244,    8,    2,   24,  674,    2,    2,    1,  729,    9,   29,\n",
       "          185,  117,  138,  287,    1,    1,  204,   31,   71,  166,   39,   10,\n",
       "          779,  213,   20],\n",
       "        [2489, 2445,  110,  509,  440, 1653,  101,  428,   95,   82,   76,  445,\n",
       "         1345,  427, 2126,  588,   53,   52,  849, 1316,  541,  109,  269,    8,\n",
       "           89,  779,  277],\n",
       "        [  71, 1473,    1,    4,    4,  440,    0,  307,   45,  119,    2,    2,\n",
       "            9,    5,    2,  479,    1,  109,   11,    7,    2,  202,    5,    6,\n",
       "          379,   10,    0],\n",
       "        [ 363, 1731,    2,    2,    2,  895,    1,   19,  307,  509,    2,   20,\n",
       "          139,    9,   26,  344,    3,   18,  109,   95,   17,   50,    2,   34,\n",
       "         1588,  379,    2],\n",
       "        [1314, 2623,   52,   25,  138, 2921,   22,    6,   19, 2480,    6,   24,\n",
       "         1345,   60,   14,  692,   15,    3,   18,   94,   77,  324,   72,   16,\n",
       "          287, 1588,   10],\n",
       "        [ 516, 2590,  112,   51,   24,  818,    1,   26,    5, 1256,    7,    1,\n",
       "            5,  168,   20,  452,   38,   44,   97,   35,    4,  139,    3,    2,\n",
       "          465,  287,   11],\n",
       "        [6763, 2977,    8,  213,  704, 1359,   11,  273,   26, 1725,   44,   58,\n",
       "          195,   19, 1906,  496,    5,    2,   44,  278,  443,   96,   55,   11,\n",
       "            6,  465,  145],\n",
       "        [ 855,  149,  140,  114,  190,  132,   34,   44,  171,   69,   16,   68,\n",
       "          619,  261, 2411,  115,   95,    3, 1059,  504,  118,  275,  176,  114,\n",
       "           45,  103,   54],\n",
       "        [  33,  209,    2,    1,    1,  197,    1,    0,  204,   61,    1,    1,\n",
       "           16,    1,    1,   59,   39,    1,  151,   16,   17,    4,    3,    0,\n",
       "            0,   12,    0],\n",
       "        [  28,   13,   99,  187, 1697,    1,   76,  121, 3033,   13,   90,  413,\n",
       "            1,    2,  869,    2,   16,  425,    1,    2,  252,  206,   21,    3,\n",
       "          773,   23,    0],\n",
       "        [1377, 2356,   41,   99,  187, 1697,    9,   76,  121, 3033,   25,   90,\n",
       "          413,  162,  140,  869,   14,   16,  425,  190,  208,  252,   80,   21,\n",
       "            3,  773,   23],\n",
       "        [1169, 1201,   21,   60,    9,  884,    2,    2, 1285,  684,    2,   82,\n",
       "          279,   90,   24,  531,   51,    1,   55,  461,  765,  185,   14,   24,\n",
       "          341,  215,   10],\n",
       "        [ 483, 1027,    1,   17,  169,  716,    2,    2,  647,  532,    3,  301,\n",
       "          134,    4,   22,  667,   10,  414,  352,   35,  374,   78,   15,   11,\n",
       "            2,  341,  105],\n",
       "        [ 155,  163,  103,  103,  136,  169,   19,   47,   58,  121,   14,   93,\n",
       "          301,  154,  275,   10,   16,   10,  414,  474,   82,    3,   37,   86,\n",
       "           34,   13,   45],\n",
       "        [  88,  642,    1,    8,    1,  568,    1,   23,    1,  911,    6,    3,\n",
       "           14,   58,    8,  153,    0,   22,   48,    8,   25,    7,    7,    0,\n",
       "           73,  121,    0],\n",
       "        [  51,  280,    1,    5,    8,  149,    2,    1,   23,  148,    0,    6,\n",
       "           13,    2,   58,   36,    0,    0,   22,   20,    8,   25,    3,    2,\n",
       "           30,   73,    1],\n",
       "        [ 164,  103,    1,    4,    5,   36,    3,   22,    1,  102,   86, 1104,\n",
       "           39,    1,    1,   41,    6,  291,  401,   31,   70,    5,    4,    3,\n",
       "           38,   30,   19],\n",
       "        [2007, 2143,   27,  115,  272,  301,   12,   30,   22,  192,   23,   86,\n",
       "         1104,  148, 1826,  271,   15,    6,  291,  401,  104,  141,  106,    4,\n",
       "           28,   23,   78],\n",
       "        [ 160,  860,    4,    2,    2,  373,    0,    1,   43,  364,    2,    2,\n",
       "          123,   35,    4,  110,    2,    0,   32,    4,    4,   73,    2,    3,\n",
       "            1,  147,   45]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for pair, freq in counts.items():\n",
    "    i_one = stoi[pair[0]]\n",
    "    i_two = stoi[pair[1]]\n",
    "    N[i_one, i_two] = freq\n",
    "N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.2417e-03, 1.7781e-02, 5.2657e-03, 6.2172e-03, 6.8139e-03, 6.1728e-03,\n",
       "         1.6813e-03, 2.6973e-03, 3.5239e-03, 2.3829e-03, 9.7653e-03, 1.1947e-02,\n",
       "         6.3381e-03, 1.0233e-02, 4.6206e-03, 1.5886e-03, 2.0764e-03, 3.7093e-04,\n",
       "         6.6083e-03, 8.2856e-03, 5.2737e-03, 3.1449e-04, 1.5160e-03, 1.2378e-03,\n",
       "         5.4027e-04, 2.1571e-03, 3.7456e-03],\n",
       "        [2.6772e-02, 2.2417e-03, 2.1813e-03, 1.8950e-03, 4.2012e-03, 2.7901e-03,\n",
       "         5.4027e-04, 6.7736e-04, 9.4024e-03, 6.6526e-03, 7.0558e-04, 2.2901e-03,\n",
       "         1.0193e-02, 6.5881e-03, 2.1925e-02, 2.5401e-04, 3.3062e-04, 2.4191e-04,\n",
       "         1.3160e-02, 4.5077e-03, 2.7699e-03, 1.5362e-03, 3.3626e-03, 6.4914e-04,\n",
       "         7.3381e-04, 8.2654e-03, 1.7539e-03],\n",
       "        [4.5964e-04, 1.2942e-03, 1.5321e-04, 4.0319e-06, 2.6207e-04, 2.6409e-03,\n",
       "         8.0638e-06, 2.6772e-03, 1.6531e-04, 8.7492e-04, 4.0319e-06, 4.6770e-04,\n",
       "         4.1529e-04, 0.0000e+00, 1.6128e-05, 4.2335e-04, 4.4351e-05, 3.0642e-04,\n",
       "         3.3949e-03, 3.2255e-05, 8.0638e-06, 1.8144e-04, 0.0000e+00, 1.2096e-05,\n",
       "         4.1932e-04, 3.3465e-04, 0.0000e+00],\n",
       "        [3.9109e-04, 3.2860e-03, 1.2096e-05, 1.6934e-04, 4.0319e-06, 2.2216e-03,\n",
       "         1.0080e-04, 8.0638e-06, 2.6772e-03, 1.0926e-03, 1.2096e-05, 1.2741e-03,\n",
       "         4.6770e-04, 1.2499e-04, 1.5241e-03, 1.5321e-03, 4.0319e-06, 4.4351e-05,\n",
       "         3.0642e-04, 2.0160e-05, 1.4112e-04, 1.4112e-04, 9.2734e-05, 0.0000e+00,\n",
       "         1.2096e-05, 4.1932e-04, 1.6128e-05],\n",
       "        [2.0805e-03, 5.2536e-03, 4.0319e-06, 1.2096e-05, 6.0075e-04, 5.1729e-03,\n",
       "         2.0160e-05, 1.0080e-04, 4.7576e-04, 2.7175e-03, 3.6287e-05, 1.2096e-05,\n",
       "         2.4191e-04, 1.2096e-04, 1.2499e-04, 1.5241e-03, 5.6447e-05, 4.0319e-06,\n",
       "         1.7095e-03, 1.1693e-04, 1.6128e-05, 3.7093e-04, 6.8542e-05, 9.2734e-05,\n",
       "         4.3141e-03, 1.2781e-03, 4.0319e-06],\n",
       "        [1.6059e-02, 2.7377e-03, 4.8786e-04, 6.1688e-04, 1.5482e-03, 5.1245e-03,\n",
       "         3.3062e-04, 5.0399e-04, 6.1285e-04, 3.2981e-03, 2.2175e-04, 7.1768e-04,\n",
       "         1.3096e-02, 3.1005e-03, 1.0785e-02, 1.0846e-03, 3.3465e-04, 5.6447e-05,\n",
       "         7.8945e-03, 3.4715e-03, 2.3385e-03, 2.7820e-04, 1.8668e-03, 2.0160e-04,\n",
       "         5.3221e-04, 4.3141e-03, 7.2977e-04],\n",
       "        [3.2255e-04, 9.7572e-04, 0.0000e+00, 7.6606e-05, 1.3467e-03, 4.9592e-04,\n",
       "         1.7740e-04, 4.0319e-06, 4.0319e-06, 6.4510e-04, 0.0000e+00, 8.0638e-06,\n",
       "         8.0638e-05, 1.0886e-04, 1.6128e-05, 2.4191e-04, 0.0000e+00, 8.1041e-04,\n",
       "         4.5964e-04, 2.4191e-05, 7.2574e-05, 4.0319e-05, 1.0483e-04, 1.6128e-05,\n",
       "         1.2499e-04, 5.6447e-05, 8.0638e-06],\n",
       "        [4.3545e-04, 1.3305e-03, 1.2096e-05, 9.6766e-05, 7.6606e-05, 1.3467e-03,\n",
       "         4.0319e-06, 1.0080e-04, 1.4515e-03, 7.6606e-04, 1.2096e-05, 7.4590e-04,\n",
       "         1.2902e-04, 2.4191e-05, 1.0886e-04, 3.3465e-04, 4.0319e-06, 8.2251e-04,\n",
       "         8.1041e-04, 1.2096e-04, 1.2499e-04, 3.4271e-04, 4.0319e-06, 1.0483e-04,\n",
       "         8.5879e-04, 1.2499e-04, 4.0319e-06],\n",
       "        [9.7128e-03, 9.0476e-03, 3.2255e-05, 8.0638e-06, 9.6766e-05, 2.7175e-03,\n",
       "         8.0638e-06, 8.0638e-06, 4.0319e-06, 2.9393e-03, 3.6287e-05, 1.1693e-04,\n",
       "         7.4590e-04, 4.7173e-04, 5.5640e-04, 1.1572e-03, 4.0319e-06, 4.0319e-06,\n",
       "         8.2251e-04, 1.2499e-04, 2.8626e-04, 6.6930e-04, 1.5724e-04, 4.0319e-05,\n",
       "         3.1409e-03, 8.5879e-04, 8.0638e-05],\n",
       "        [1.0035e-02, 9.8580e-03, 4.4351e-04, 2.0522e-03, 1.7740e-03, 6.6647e-03,\n",
       "         4.0722e-04, 1.7257e-03, 3.8303e-04, 3.3062e-04, 3.0642e-04, 1.7942e-03,\n",
       "         5.4229e-03, 1.7216e-03, 8.5718e-03, 2.3708e-03, 2.1369e-04, 2.0966e-04,\n",
       "         3.4231e-03, 5.3060e-03, 2.1813e-03, 4.3948e-04, 1.0846e-03, 3.2255e-05,\n",
       "         3.5884e-04, 3.1409e-03, 1.1168e-03],\n",
       "        [2.8626e-04, 5.9390e-03, 4.0319e-06, 1.6128e-05, 1.6128e-05, 1.7740e-03,\n",
       "         0.0000e+00, 1.2378e-03, 1.8144e-04, 4.7980e-04, 8.0638e-06, 8.0638e-06,\n",
       "         3.6287e-05, 2.0160e-05, 8.0638e-06, 1.9313e-03, 4.0319e-06, 4.3948e-04,\n",
       "         4.4351e-05, 2.8223e-05, 8.0638e-06, 8.1444e-04, 2.0160e-05, 2.4191e-05,\n",
       "         1.5281e-03, 4.0319e-05, 0.0000e+00],\n",
       "        [1.4636e-03, 6.9792e-03, 8.0638e-06, 8.0638e-06, 8.0638e-06, 3.6086e-03,\n",
       "         4.0319e-06, 7.6606e-05, 1.2378e-03, 2.0522e-03, 8.0638e-06, 8.0638e-05,\n",
       "         5.6043e-04, 3.6287e-05, 1.0483e-04, 1.3870e-03, 1.2096e-05, 7.2574e-05,\n",
       "         4.3948e-04, 3.8303e-04, 6.8542e-05, 2.0160e-04, 8.0638e-06, 1.3708e-04,\n",
       "         6.4027e-03, 1.5281e-03, 8.0638e-06],\n",
       "        [5.2979e-03, 1.0576e-02, 2.0966e-04, 1.0080e-04, 5.5640e-04, 1.1777e-02,\n",
       "         8.8702e-05, 2.4191e-05, 7.6606e-05, 9.9991e-03, 2.4191e-05, 9.6766e-05,\n",
       "         5.4229e-03, 2.4191e-04, 5.6447e-05, 2.7901e-03, 6.0479e-05, 1.2096e-05,\n",
       "         7.2574e-05, 3.7900e-04, 3.1046e-04, 1.3063e-03, 2.9030e-04, 6.4510e-05,\n",
       "         1.1572e-03, 6.4027e-03, 4.0319e-05],\n",
       "        [2.0805e-03, 1.0443e-02, 4.5157e-04, 2.0563e-04, 9.6766e-05, 3.2981e-03,\n",
       "         4.0319e-06, 1.0483e-04, 2.0160e-05, 5.0641e-03, 2.8223e-05, 4.0319e-06,\n",
       "         2.0160e-05, 6.7736e-04, 8.0638e-05, 1.8224e-03, 1.5321e-04, 1.7740e-04,\n",
       "         3.9109e-04, 1.4112e-04, 1.6128e-05, 5.6043e-04, 1.2096e-05, 8.0638e-06,\n",
       "         1.8748e-03, 1.1572e-03, 4.4351e-05],\n",
       "        [2.7268e-02, 1.2003e-02, 3.2255e-05, 8.5879e-04, 2.8385e-03, 5.4794e-03,\n",
       "         4.4351e-05, 1.1007e-03, 1.0483e-04, 6.9550e-03, 1.7740e-04, 2.3385e-04,\n",
       "         7.8622e-04, 7.6606e-05, 7.6848e-03, 1.9998e-03, 2.0160e-05, 8.0638e-06,\n",
       "         1.7740e-04, 1.1209e-03, 1.7861e-03, 3.8706e-04, 2.2175e-04, 4.4351e-05,\n",
       "         2.4191e-05, 1.8748e-03, 5.8463e-04],\n",
       "        [3.4473e-03, 6.0075e-04, 5.6447e-04, 4.5964e-04, 7.6606e-04, 5.3221e-04,\n",
       "         1.3708e-04, 1.7740e-04, 6.8945e-04, 2.7820e-04, 6.4510e-05, 2.7417e-04,\n",
       "         2.4957e-03, 1.0523e-03, 9.7209e-03, 4.6367e-04, 3.8303e-04, 1.2096e-05,\n",
       "         4.2698e-03, 2.0321e-03, 4.7576e-04, 1.1088e-03, 7.0961e-04, 4.5964e-04,\n",
       "         1.8144e-04, 4.1529e-04, 2.1772e-04],\n",
       "        [1.3305e-04, 8.4267e-04, 8.0638e-06, 4.0319e-06, 4.0319e-06, 7.9428e-04,\n",
       "         4.0319e-06, 0.0000e+00, 8.2251e-04, 2.4595e-04, 4.0319e-06, 4.0319e-06,\n",
       "         6.4510e-05, 4.0319e-06, 4.0319e-06, 2.3788e-04, 1.5724e-04, 4.0319e-06,\n",
       "         6.0882e-04, 6.4510e-05, 6.8542e-05, 1.6128e-05, 1.2096e-05, 0.0000e+00,\n",
       "         0.0000e+00, 4.8383e-05, 0.0000e+00],\n",
       "        [1.1289e-04, 5.2415e-05, 3.9916e-04, 7.5397e-04, 6.8421e-03, 4.0319e-06,\n",
       "         3.0642e-04, 4.8786e-04, 1.2229e-02, 5.2415e-05, 3.6287e-04, 1.6652e-03,\n",
       "         4.0319e-06, 8.0638e-06, 3.5037e-03, 8.0638e-06, 6.4510e-05, 1.7136e-03,\n",
       "         4.0319e-06, 8.0638e-06, 1.0160e-03, 8.3057e-04, 8.4670e-05, 1.2096e-05,\n",
       "         3.1167e-03, 9.2734e-05, 0.0000e+00],\n",
       "        [5.5519e-03, 9.4992e-03, 1.6531e-04, 3.9916e-04, 7.5397e-04, 6.8421e-03,\n",
       "         3.6287e-05, 3.0642e-04, 4.8786e-04, 1.2229e-02, 1.0080e-04, 3.6287e-04,\n",
       "         1.6652e-03, 6.5317e-04, 5.6447e-04, 3.5037e-03, 5.6447e-05, 6.4510e-05,\n",
       "         1.7136e-03, 7.6606e-04, 8.3864e-04, 1.0160e-03, 3.2255e-04, 8.4670e-05,\n",
       "         1.2096e-05, 3.1167e-03, 9.2734e-05],\n",
       "        [4.7133e-03, 4.8423e-03, 8.4670e-05, 2.4191e-04, 3.6287e-05, 3.5642e-03,\n",
       "         8.0638e-06, 8.0638e-06, 5.1810e-03, 2.7578e-03, 8.0638e-06, 3.3062e-04,\n",
       "         1.1249e-03, 3.6287e-04, 9.6766e-05, 2.1409e-03, 2.0563e-04, 4.0319e-06,\n",
       "         2.2175e-04, 1.8587e-03, 3.0844e-03, 7.4590e-04, 5.6447e-05, 9.6766e-05,\n",
       "         1.3749e-03, 8.6686e-04, 4.0319e-05],\n",
       "        [1.9474e-03, 4.1408e-03, 4.0319e-06, 6.8542e-05, 6.8139e-04, 2.8868e-03,\n",
       "         8.0638e-06, 8.0638e-06, 2.6086e-03, 2.1450e-03, 1.2096e-05, 1.2136e-03,\n",
       "         5.4027e-04, 1.6128e-05, 8.8702e-05, 2.6893e-03, 4.0319e-05, 1.6692e-03,\n",
       "         1.4192e-03, 1.4112e-04, 1.5079e-03, 3.1449e-04, 6.0479e-05, 4.4351e-05,\n",
       "         8.0638e-06, 1.3749e-03, 4.2335e-04],\n",
       "        [6.2494e-04, 6.5720e-04, 4.1529e-04, 4.1529e-04, 5.4834e-04, 6.8139e-04,\n",
       "         7.6606e-05, 1.8950e-04, 2.3385e-04, 4.8786e-04, 5.6447e-05, 3.7497e-04,\n",
       "         1.2136e-03, 6.2091e-04, 1.1088e-03, 4.0319e-05, 6.4510e-05, 4.0319e-05,\n",
       "         1.6692e-03, 1.9111e-03, 3.3062e-04, 1.2096e-05, 1.4918e-04, 3.4674e-04,\n",
       "         1.3708e-04, 5.2415e-05, 1.8144e-04],\n",
       "        [3.5481e-04, 2.5885e-03, 4.0319e-06, 3.2255e-05, 4.0319e-06, 2.2901e-03,\n",
       "         4.0319e-06, 9.2734e-05, 4.0319e-06, 3.6731e-03, 2.4191e-05, 1.2096e-05,\n",
       "         5.6447e-05, 2.3385e-04, 3.2255e-05, 6.1688e-04, 0.0000e+00, 8.8702e-05,\n",
       "         1.9353e-04, 3.2255e-05, 1.0080e-04, 2.8223e-05, 2.8223e-05, 0.0000e+00,\n",
       "         2.9433e-04, 4.8786e-04, 0.0000e+00],\n",
       "        [2.0563e-04, 1.1289e-03, 4.0319e-06, 2.0160e-05, 3.2255e-05, 6.0075e-04,\n",
       "         8.0638e-06, 4.0319e-06, 9.2734e-05, 5.9672e-04, 0.0000e+00, 2.4191e-05,\n",
       "         5.2415e-05, 8.0638e-06, 2.3385e-04, 1.4515e-04, 0.0000e+00, 0.0000e+00,\n",
       "         8.8702e-05, 8.0638e-05, 3.2255e-05, 1.0080e-04, 1.2096e-05, 8.0638e-06,\n",
       "         1.2096e-04, 2.9433e-04, 4.0319e-06],\n",
       "        [6.6123e-04, 4.1529e-04, 4.0319e-06, 1.6128e-05, 2.0160e-05, 1.4515e-04,\n",
       "         1.2096e-05, 8.8702e-05, 4.0319e-06, 4.1125e-04, 3.4674e-04, 4.4512e-03,\n",
       "         1.5724e-04, 4.0319e-06, 4.0319e-06, 1.6531e-04, 2.4191e-05, 1.1733e-03,\n",
       "         1.6168e-03, 1.2499e-04, 2.8223e-04, 2.0160e-05, 1.6128e-05, 1.2096e-05,\n",
       "         1.5321e-04, 1.2096e-04, 7.6606e-05],\n",
       "        [8.0920e-03, 8.6404e-03, 1.0886e-04, 4.6367e-04, 1.0967e-03, 1.2136e-03,\n",
       "         4.8383e-05, 1.2096e-04, 8.8702e-05, 7.7412e-04, 9.2734e-05, 3.4674e-04,\n",
       "         4.4512e-03, 5.9672e-04, 7.3623e-03, 1.0926e-03, 6.0479e-05, 2.4191e-05,\n",
       "         1.1733e-03, 1.6168e-03, 4.1932e-04, 5.6850e-04, 4.2738e-04, 1.6128e-05,\n",
       "         1.1289e-04, 9.2734e-05, 3.1449e-04],\n",
       "        [6.4510e-04, 3.4674e-03, 1.6128e-05, 8.0638e-06, 8.0638e-06, 1.5039e-03,\n",
       "         0.0000e+00, 4.0319e-06, 1.7337e-04, 1.4676e-03, 8.0638e-06, 8.0638e-06,\n",
       "         4.9592e-04, 1.4112e-04, 1.6128e-05, 4.4351e-04, 8.0638e-06, 0.0000e+00,\n",
       "         1.2902e-04, 1.6128e-05, 1.6128e-05, 2.9433e-04, 8.0638e-06, 1.2096e-05,\n",
       "         4.0319e-06, 5.9269e-04, 1.8144e-04]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = N / N.sum()\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[20,  1, 14, 11, 20, 11,  1,  4, 13, 10],\n",
       "        [ 0, 14, 14, 18, 18,  8, 25,  1, 18,  0],\n",
       "        [18, 18, 18, 17,  7,  4, 18,  0,  5,  7],\n",
       "        [ 1,  1,  9,  5, 12,  5,  5, 15,  8,  1],\n",
       "        [ 5,  0, 18, 12,  8,  9,  5,  5, 24,  1],\n",
       "        [12,  0,  5, 12,  0, 14,  1, 12, 12, 18],\n",
       "        [19,  6,  9,  9,  1,  4,  4,  9,  5, 18],\n",
       "        [ 5, 11, 24,  8, 24, 20, 12, 15,  1, 17],\n",
       "        [ 1,  1,  0,  9,  0, 11,  5,  1,  1, 24],\n",
       "        [12, 14,  1, 13, 12, 12, 13, 14,  0, 14],\n",
       "        [ 1,  1,  7,  5,  5, 15,  1,  0,  7,  1],\n",
       "        [ 1, 21, 15,  1,  1,  1,  1, 24, 24,  0],\n",
       "        [ 1,  9,  5,  1,  5,  1, 25, 25,  9,  9],\n",
       "        [ 1,  1,  1, 24,  2,  1, 15,  5,  1,  1],\n",
       "        [ 1,  9,  0,  0,  0,  1,  0, 26,  0,  0],\n",
       "        [13, 14, 14, 19, 22, 14,  0, 12, 22, 14],\n",
       "        [ 8,  8,  0, 16,  1,  1,  8,  1,  1,  8],\n",
       "        [ 8, 14, 21, 14,  4, 24,  8,  8, 14,  8],\n",
       "        [ 9,  0,  9, 18, 15,  0,  9, 18,  3,  9],\n",
       "        [ 0,  1, 15,  0,  5,  5,  1, 21,  8,  1],\n",
       "        [ 9,  8,  1,  9, 15, 17,  1, 15,  1, 17],\n",
       "        [12,  4,  1, 19,  0, 12, 18, 19, 14, 20],\n",
       "        [ 5,  5,  5,  5,  5,  9,  1,  5,  9, 20],\n",
       "        [ 1,  9,  5,  1, 14, 20,  8, 14,  5,  1],\n",
       "        [18, 10, 18,  1, 11,  0,  0, 11, 11, 11],\n",
       "        [14,  0, 12, 12,  3,  1, 18,  1,  1, 14],\n",
       "        [ 5, 25, 13,  1,  9, 18,  0,  5,  1,  1]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2)\n",
    "samples = torch.multinomial(p, 10, replacement=True, generator=g)\n",
    "samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([27, 1])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_lens = [len(w) for w in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 556, 4410, 1306, 1542, 1690, 1531,  417,  669,  874,  591, 2422, 2963,\n",
       "         1572, 2538, 1146,  394,  515,   92, 1639, 2055, 1308,   78,  376,  307,\n",
       "          134,  535,  929],\n",
       "        [6640,  556,  541,  470, 1042,  692,  134,  168, 2332, 1650,  175,  568,\n",
       "         2528, 1634, 5438,   63,   82,   60, 3264, 1118,  687,  381,  834,  161,\n",
       "          182, 2050,  435],\n",
       "        [ 114,  321,   38,    1,   65,  655,    2,  664,   41,  217,    1,  116,\n",
       "          103,    0,    4,  105,   11,   76,  842,    8,    2,   45,    0,    3,\n",
       "          104,   83,    0],\n",
       "        [  97,  815,    3,   42,    1,  551,   25,    2,  664,  271,    3,  316,\n",
       "          116,   31,  378,  380,    1,   11,   76,    5,   35,   35,   23,    0,\n",
       "            3,  104,    4],\n",
       "        [ 516, 1303,    1,    3,  149, 1283,    5,   25,  118,  674,    9,    3,\n",
       "           60,   30,   31,  378,   14,    1,  424,   29,    4,   92,   17,   23,\n",
       "         1070,  317,    1],\n",
       "        [3983,  679,  121,  153,  384, 1271,   82,  125,  152,  818,   55,  178,\n",
       "         3248,  769, 2675,  269,   83,   14, 1958,  861,  580,   69,  463,   50,\n",
       "          132, 1070,  181],\n",
       "        [  80,  242,    0,   19,  334,  123,   44,    1,    1,  160,    0,    2,\n",
       "           20,   27,    4,   60,    0,  201,  114,    6,   18,   10,   26,    4,\n",
       "           31,   14,    2],\n",
       "        [ 108,  330,    3,   24,   19,  334,    1,   25,  360,  190,    3,  185,\n",
       "           32,    6,   27,   83,    1,  204,  201,   30,   31,   85,    1,   26,\n",
       "          213,   31,    1],\n",
       "        [2409, 2244,    8,    2,   24,  674,    2,    2,    1,  729,    9,   29,\n",
       "          185,  117,  138,  287,    1,    1,  204,   31,   71,  166,   39,   10,\n",
       "          779,  213,   20],\n",
       "        [2489, 2445,  110,  509,  440, 1653,  101,  428,   95,   82,   76,  445,\n",
       "         1345,  427, 2126,  588,   53,   52,  849, 1316,  541,  109,  269,    8,\n",
       "           89,  779,  277],\n",
       "        [  71, 1473,    1,    4,    4,  440,    0,  307,   45,  119,    2,    2,\n",
       "            9,    5,    2,  479,    1,  109,   11,    7,    2,  202,    5,    6,\n",
       "          379,   10,    0],\n",
       "        [ 363, 1731,    2,    2,    2,  895,    1,   19,  307,  509,    2,   20,\n",
       "          139,    9,   26,  344,    3,   18,  109,   95,   17,   50,    2,   34,\n",
       "         1588,  379,    2],\n",
       "        [1314, 2623,   52,   25,  138, 2921,   22,    6,   19, 2480,    6,   24,\n",
       "         1345,   60,   14,  692,   15,    3,   18,   94,   77,  324,   72,   16,\n",
       "          287, 1588,   10],\n",
       "        [ 516, 2590,  112,   51,   24,  818,    1,   26,    5, 1256,    7,    1,\n",
       "            5,  168,   20,  452,   38,   44,   97,   35,    4,  139,    3,    2,\n",
       "          465,  287,   11],\n",
       "        [6763, 2977,    8,  213,  704, 1359,   11,  273,   26, 1725,   44,   58,\n",
       "          195,   19, 1906,  496,    5,    2,   44,  278,  443,   96,   55,   11,\n",
       "            6,  465,  145],\n",
       "        [ 855,  149,  140,  114,  190,  132,   34,   44,  171,   69,   16,   68,\n",
       "          619,  261, 2411,  115,   95,    3, 1059,  504,  118,  275,  176,  114,\n",
       "           45,  103,   54],\n",
       "        [  33,  209,    2,    1,    1,  197,    1,    0,  204,   61,    1,    1,\n",
       "           16,    1,    1,   59,   39,    1,  151,   16,   17,    4,    3,    0,\n",
       "            0,   12,    0],\n",
       "        [  28,   13,   99,  187, 1697,    1,   76,  121, 3033,   13,   90,  413,\n",
       "            1,    2,  869,    2,   16,  425,    1,    2,  252,  206,   21,    3,\n",
       "          773,   23,    0],\n",
       "        [1377, 2356,   41,   99,  187, 1697,    9,   76,  121, 3033,   25,   90,\n",
       "          413,  162,  140,  869,   14,   16,  425,  190,  208,  252,   80,   21,\n",
       "            3,  773,   23],\n",
       "        [1169, 1201,   21,   60,    9,  884,    2,    2, 1285,  684,    2,   82,\n",
       "          279,   90,   24,  531,   51,    1,   55,  461,  765,  185,   14,   24,\n",
       "          341,  215,   10],\n",
       "        [ 483, 1027,    1,   17,  169,  716,    2,    2,  647,  532,    3,  301,\n",
       "          134,    4,   22,  667,   10,  414,  352,   35,  374,   78,   15,   11,\n",
       "            2,  341,  105],\n",
       "        [ 155,  163,  103,  103,  136,  169,   19,   47,   58,  121,   14,   93,\n",
       "          301,  154,  275,   10,   16,   10,  414,  474,   82,    3,   37,   86,\n",
       "           34,   13,   45],\n",
       "        [  88,  642,    1,    8,    1,  568,    1,   23,    1,  911,    6,    3,\n",
       "           14,   58,    8,  153,    0,   22,   48,    8,   25,    7,    7,    0,\n",
       "           73,  121,    0],\n",
       "        [  51,  280,    1,    5,    8,  149,    2,    1,   23,  148,    0,    6,\n",
       "           13,    2,   58,   36,    0,    0,   22,   20,    8,   25,    3,    2,\n",
       "           30,   73,    1],\n",
       "        [ 164,  103,    1,    4,    5,   36,    3,   22,    1,  102,   86, 1104,\n",
       "           39,    1,    1,   41,    6,  291,  401,   31,   70,    5,    4,    3,\n",
       "           38,   30,   19],\n",
       "        [2007, 2143,   27,  115,  272,  301,   12,   30,   22,  192,   23,   86,\n",
       "         1104,  148, 1826,  271,   15,    6,  291,  401,  104,  141,  106,    4,\n",
       "           28,   23,   78],\n",
       "        [ 160,  860,    4,    2,    2,  373,    0,    1,   43,  364,    2,    2,\n",
       "          123,   35,    4,  110,    2,    0,   32,    4,    4,   73,    2,    3,\n",
       "            1,  147,   45]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 1., 1., 1.],\n",
       "         [2., 2., 2., 2.]]),\n",
       " tensor([[4.],\n",
       "         [8.]]))"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ones = torch.ones(4)\n",
    "twos = torch.ones(4) * 2\n",
    "test = torch.stack([ones, twos])\n",
    "test, test.sum(dim=1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P = N.float()\n",
    "P /= P.sum(dim=1, keepdim=True)\n",
    "P[0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "junide\n",
      "janasah\n",
      "p\n",
      "cfqh\n",
      "a\n",
      "nn\n",
      "kxi\n",
      "ritolian\n",
      "jgee\n",
      "kxqhnaauranilevias\n",
      "dedainrwieta\n",
      "ssonielylarte\n",
      "faveumerifontume\n",
      "phynslenaruani\n",
      "core\n",
      "yaenon\n",
      "ka\n",
      "jabdinerimikimaynin\n",
      "anaasn\n",
      "ssorionsushxdxossmitqn\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "words_to_gen = 20\n",
    "for wi in range(words_to_gen):\n",
    "    # word_len = torch.randint(min(word_lens), max(word_lens), (1,)).item()\n",
    "    # for i in range(int(word_len)):\n",
    "    li = 0\n",
    "    next_letter = ''\n",
    "    genned_word = '.'\n",
    "    while next_letter != '.':\n",
    "        prior_letter = genned_word[li]\n",
    "        prior_letter_i = stoi[prior_letter]\n",
    "        next_sample_p = P[prior_letter_i]\n",
    "        next_letter_idx = torch.multinomial(\n",
    "                next_sample_p, 1, replacement=True, generator=g\n",
    "            ).item()\n",
    "        next_letter = itos[next_letter_idx]\n",
    "        genned_word += next_letter\n",
    "        li += 1\n",
    "    print(genned_word.strip('.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neg log likelihood: 570686.625\n",
      "avg neg log likelhood: 2.5014097690582275\n"
     ]
    }
   ],
   "source": [
    "n = 0\n",
    "log_likelihood = 0\n",
    "for word in words:\n",
    "    w_start_end = '.' + word + '.'\n",
    "    for char, char1 in zip(w_start_end, w_start_end[1:]):\n",
    "        prob = P[stoi[char], stoi[char1]]\n",
    "        log_likelihood += torch.log(prob)\n",
    "        n += 1\n",
    "        # print(f'{char}{char1}: {prob:.4f}')\n",
    "\n",
    "print(f\"neg log likelihood: {-log_likelihood}\")\n",
    "print(f\"avg neg log likelhood: {-log_likelihood / n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.e',\n",
       " 'em',\n",
       " 'mm',\n",
       " 'ma',\n",
       " 'a.',\n",
       " '.o',\n",
       " 'ol',\n",
       " 'li',\n",
       " 'iv',\n",
       " 'vi',\n",
       " 'ia',\n",
       " 'a.',\n",
       " '.a',\n",
       " 'av',\n",
       " 'va',\n",
       " 'a.',\n",
       " '.i',\n",
       " 'is',\n",
       " 'sa',\n",
       " 'ab',\n",
       " 'be',\n",
       " 'el',\n",
       " 'll',\n",
       " 'la',\n",
       " 'a.',\n",
       " '.s',\n",
       " 'so',\n",
       " 'op',\n",
       " 'ph',\n",
       " 'hi',\n",
       " 'ia',\n",
       " 'a.',\n",
       " '.c',\n",
       " 'ch',\n",
       " 'ha',\n",
       " 'ar',\n",
       " 'rl',\n",
       " 'lo',\n",
       " 'ot',\n",
       " 'tt',\n",
       " 'te',\n",
       " 'e.',\n",
       " '.m',\n",
       " 'mi',\n",
       " 'ia',\n",
       " 'a.',\n",
       " '.a',\n",
       " 'am',\n",
       " 'me',\n",
       " 'el',\n",
       " 'li',\n",
       " 'ia',\n",
       " 'a.',\n",
       " '.h',\n",
       " 'ha',\n",
       " 'ar',\n",
       " 'rp',\n",
       " 'pe',\n",
       " 'er',\n",
       " 'r.',\n",
       " '.e',\n",
       " 'ev',\n",
       " 've',\n",
       " 'el',\n",
       " 'ly',\n",
       " 'yn',\n",
       " 'n.',\n",
       " '.a',\n",
       " 'ab',\n",
       " 'bi',\n",
       " 'ig',\n",
       " 'ga',\n",
       " 'ai',\n",
       " 'il',\n",
       " 'l.',\n",
       " '.e',\n",
       " 'em',\n",
       " 'mi',\n",
       " 'il',\n",
       " 'ly',\n",
       " 'y.',\n",
       " '.e',\n",
       " 'el',\n",
       " 'li',\n",
       " 'iz',\n",
       " 'za',\n",
       " 'ab',\n",
       " 'be',\n",
       " 'et',\n",
       " 'th',\n",
       " 'h.',\n",
       " '.m',\n",
       " 'mi',\n",
       " 'il',\n",
       " 'la',\n",
       " 'a.',\n",
       " '.e',\n",
       " 'el',\n",
       " 'll',\n",
       " 'la',\n",
       " 'a.',\n",
       " '.a',\n",
       " 'av',\n",
       " 've',\n",
       " 'er',\n",
       " 'ry',\n",
       " 'y.',\n",
       " '.s',\n",
       " 'so',\n",
       " 'of',\n",
       " 'fi',\n",
       " 'ia',\n",
       " 'a.',\n",
       " '.c',\n",
       " 'ca',\n",
       " 'am',\n",
       " 'mi',\n",
       " 'il',\n",
       " 'la',\n",
       " 'a.',\n",
       " '.a',\n",
       " 'ar',\n",
       " 'ri',\n",
       " 'ia',\n",
       " 'a.',\n",
       " '.s',\n",
       " 'sc',\n",
       " 'ca',\n",
       " 'ar',\n",
       " 'rl',\n",
       " 'le',\n",
       " 'et',\n",
       " 'tt',\n",
       " 't.',\n",
       " '.v',\n",
       " 'vi',\n",
       " 'ic',\n",
       " 'ct',\n",
       " 'to',\n",
       " 'or',\n",
       " 'ri',\n",
       " 'ia',\n",
       " 'a.',\n",
       " '.m',\n",
       " 'ma',\n",
       " 'ad',\n",
       " 'di',\n",
       " 'is',\n",
       " 'so',\n",
       " 'on',\n",
       " 'n.',\n",
       " '.l',\n",
       " 'lu',\n",
       " 'un',\n",
       " 'na',\n",
       " 'a.',\n",
       " '.g',\n",
       " 'gr',\n",
       " 'ra',\n",
       " 'ac',\n",
       " 'ce',\n",
       " 'e.',\n",
       " '.c',\n",
       " 'ch',\n",
       " 'hl',\n",
       " 'lo',\n",
       " 'oe',\n",
       " 'e.',\n",
       " '.p',\n",
       " 'pe',\n",
       " 'en',\n",
       " 'ne',\n",
       " 'el',\n",
       " 'lo',\n",
       " 'op',\n",
       " 'pe',\n",
       " 'e.',\n",
       " '.l',\n",
       " 'la',\n",
       " 'ay',\n",
       " 'yl',\n",
       " 'la',\n",
       " 'a.',\n",
       " '.r',\n",
       " 'ri',\n",
       " 'il',\n",
       " 'le',\n",
       " 'ey',\n",
       " 'y.',\n",
       " '.z',\n",
       " 'zo',\n",
       " 'oe',\n",
       " 'ey',\n",
       " 'y.',\n",
       " '.n',\n",
       " 'no',\n",
       " 'or',\n",
       " 'ra',\n",
       " 'a.',\n",
       " '.l',\n",
       " 'li',\n",
       " 'il',\n",
       " 'ly',\n",
       " 'y.',\n",
       " '.e',\n",
       " 'el',\n",
       " 'le',\n",
       " 'ea',\n",
       " 'an',\n",
       " 'no',\n",
       " 'or',\n",
       " 'r.',\n",
       " '.h',\n",
       " 'ha',\n",
       " 'an',\n",
       " 'nn',\n",
       " 'na',\n",
       " 'ah',\n",
       " 'h.',\n",
       " '.l',\n",
       " 'li',\n",
       " 'il',\n",
       " 'll',\n",
       " 'li',\n",
       " 'ia',\n",
       " 'an',\n",
       " 'n.',\n",
       " '.a',\n",
       " 'ad',\n",
       " 'dd',\n",
       " 'di',\n",
       " 'is',\n",
       " 'so',\n",
       " 'on',\n",
       " 'n.',\n",
       " '.a',\n",
       " 'au',\n",
       " 'ub',\n",
       " 'br',\n",
       " 're',\n",
       " 'ey',\n",
       " 'y.',\n",
       " '.e',\n",
       " 'el',\n",
       " 'll',\n",
       " 'li',\n",
       " 'ie',\n",
       " 'e.',\n",
       " '.s',\n",
       " 'st',\n",
       " 'te',\n",
       " 'el',\n",
       " 'll',\n",
       " 'la',\n",
       " 'a.',\n",
       " '.n',\n",
       " 'na',\n",
       " 'at',\n",
       " 'ta',\n",
       " 'al',\n",
       " 'li',\n",
       " 'ie',\n",
       " 'e.',\n",
       " '.z',\n",
       " 'zo',\n",
       " 'oe',\n",
       " 'e.',\n",
       " '.l',\n",
       " 'le',\n",
       " 'ea',\n",
       " 'ah',\n",
       " 'h.',\n",
       " '.h',\n",
       " 'ha',\n",
       " 'az',\n",
       " 'ze',\n",
       " 'el',\n",
       " 'l.',\n",
       " '.v',\n",
       " 'vi',\n",
       " 'io',\n",
       " 'ol',\n",
       " 'le',\n",
       " 'et',\n",
       " 't.',\n",
       " '.a',\n",
       " 'au',\n",
       " 'ur',\n",
       " 'ro',\n",
       " 'or',\n",
       " 'ra',\n",
       " 'a.',\n",
       " '.s',\n",
       " 'sa',\n",
       " 'av',\n",
       " 'va',\n",
       " 'an',\n",
       " 'nn',\n",
       " 'na',\n",
       " 'ah',\n",
       " 'h.',\n",
       " '.a',\n",
       " 'au',\n",
       " 'ud',\n",
       " 'dr',\n",
       " 're',\n",
       " 'ey',\n",
       " 'y.',\n",
       " '.b',\n",
       " 'br',\n",
       " 'ro',\n",
       " 'oo',\n",
       " 'ok',\n",
       " 'kl',\n",
       " 'ly',\n",
       " 'yn',\n",
       " 'n.',\n",
       " '.b',\n",
       " 'be',\n",
       " 'el',\n",
       " 'll',\n",
       " 'la',\n",
       " 'a.',\n",
       " '.c',\n",
       " 'cl',\n",
       " 'la',\n",
       " 'ai',\n",
       " 'ir',\n",
       " 're',\n",
       " 'e.',\n",
       " '.s',\n",
       " 'sk',\n",
       " 'ky',\n",
       " 'yl',\n",
       " 'la',\n",
       " 'ar',\n",
       " 'r.',\n",
       " '.l',\n",
       " 'lu',\n",
       " 'uc',\n",
       " 'cy',\n",
       " 'y.',\n",
       " '.p',\n",
       " 'pa',\n",
       " 'ai',\n",
       " 'is',\n",
       " 'sl',\n",
       " 'le',\n",
       " 'ey',\n",
       " 'y.',\n",
       " '.e',\n",
       " 'ev',\n",
       " 've',\n",
       " 'er',\n",
       " 'rl',\n",
       " 'ly',\n",
       " 'y.',\n",
       " '.a',\n",
       " 'an',\n",
       " 'nn',\n",
       " 'na',\n",
       " 'a.',\n",
       " '.c',\n",
       " 'ca',\n",
       " 'ar',\n",
       " 'ro',\n",
       " 'ol',\n",
       " 'li',\n",
       " 'in',\n",
       " 'ne',\n",
       " 'e.',\n",
       " '.n',\n",
       " 'no',\n",
       " 'ov',\n",
       " 'va',\n",
       " 'a.',\n",
       " '.g',\n",
       " 'ge',\n",
       " 'en',\n",
       " 'ne',\n",
       " 'es',\n",
       " 'si',\n",
       " 'is',\n",
       " 's.',\n",
       " '.e',\n",
       " 'em',\n",
       " 'mi',\n",
       " 'il',\n",
       " 'li',\n",
       " 'ia',\n",
       " 'a.',\n",
       " '.k',\n",
       " 'ke',\n",
       " 'en',\n",
       " 'nn',\n",
       " 'ne',\n",
       " 'ed',\n",
       " 'dy',\n",
       " 'y.',\n",
       " '.s',\n",
       " 'sa',\n",
       " 'am',\n",
       " 'ma',\n",
       " 'an',\n",
       " 'nt',\n",
       " 'th',\n",
       " 'ha',\n",
       " 'a.',\n",
       " '.m',\n",
       " 'ma',\n",
       " 'ay',\n",
       " 'ya',\n",
       " 'a.',\n",
       " '.w',\n",
       " 'wi',\n",
       " 'il',\n",
       " 'll',\n",
       " 'lo',\n",
       " 'ow',\n",
       " 'w.',\n",
       " '.k',\n",
       " 'ki',\n",
       " 'in',\n",
       " 'ns',\n",
       " 'sl',\n",
       " 'le',\n",
       " 'ey',\n",
       " 'y.',\n",
       " '.n',\n",
       " 'na',\n",
       " 'ao',\n",
       " 'om',\n",
       " 'mi',\n",
       " 'i.',\n",
       " '.a',\n",
       " 'aa',\n",
       " 'al',\n",
       " 'li',\n",
       " 'iy',\n",
       " 'ya',\n",
       " 'ah',\n",
       " 'h.',\n",
       " '.e',\n",
       " 'el',\n",
       " 'le',\n",
       " 'en',\n",
       " 'na',\n",
       " 'a.',\n",
       " '.s',\n",
       " 'sa',\n",
       " 'ar',\n",
       " 'ra',\n",
       " 'ah',\n",
       " 'h.',\n",
       " '.a',\n",
       " 'ar',\n",
       " 'ri',\n",
       " 'ia',\n",
       " 'an',\n",
       " 'na',\n",
       " 'a.',\n",
       " '.a',\n",
       " 'al',\n",
       " 'll',\n",
       " 'li',\n",
       " 'is',\n",
       " 'so',\n",
       " 'on',\n",
       " 'n.',\n",
       " '.g',\n",
       " 'ga',\n",
       " 'ab',\n",
       " 'br',\n",
       " 'ri',\n",
       " 'ie',\n",
       " 'el',\n",
       " 'll',\n",
       " 'la',\n",
       " 'a.',\n",
       " '.a',\n",
       " 'al',\n",
       " 'li',\n",
       " 'ic',\n",
       " 'ce',\n",
       " 'e.',\n",
       " '.m',\n",
       " 'ma',\n",
       " 'ad',\n",
       " 'de',\n",
       " 'el',\n",
       " 'ly',\n",
       " 'yn',\n",
       " 'n.',\n",
       " '.c',\n",
       " 'co',\n",
       " 'or',\n",
       " 'ra',\n",
       " 'a.',\n",
       " '.r',\n",
       " 'ru',\n",
       " 'ub',\n",
       " 'by',\n",
       " 'y.',\n",
       " '.e',\n",
       " 'ev',\n",
       " 'va',\n",
       " 'a.',\n",
       " '.s',\n",
       " 'se',\n",
       " 'er',\n",
       " 're',\n",
       " 'en',\n",
       " 'ni',\n",
       " 'it',\n",
       " 'ty',\n",
       " 'y.',\n",
       " '.a',\n",
       " 'au',\n",
       " 'ut',\n",
       " 'tu',\n",
       " 'um',\n",
       " 'mn',\n",
       " 'n.',\n",
       " '.a',\n",
       " 'ad',\n",
       " 'de',\n",
       " 'el',\n",
       " 'li',\n",
       " 'in',\n",
       " 'ne',\n",
       " 'e.',\n",
       " '.h',\n",
       " 'ha',\n",
       " 'ai',\n",
       " 'il',\n",
       " 'le',\n",
       " 'ey',\n",
       " 'y.',\n",
       " '.g',\n",
       " 'gi',\n",
       " 'ia',\n",
       " 'an',\n",
       " 'nn',\n",
       " 'na',\n",
       " 'a.',\n",
       " '.v',\n",
       " 'va',\n",
       " 'al',\n",
       " 'le',\n",
       " 'en',\n",
       " 'nt',\n",
       " 'ti',\n",
       " 'in',\n",
       " 'na',\n",
       " 'a.',\n",
       " '.i',\n",
       " 'is',\n",
       " 'sl',\n",
       " 'la',\n",
       " 'a.',\n",
       " '.e',\n",
       " 'el',\n",
       " 'li',\n",
       " 'ia',\n",
       " 'an',\n",
       " 'na',\n",
       " 'a.',\n",
       " '.q',\n",
       " 'qu',\n",
       " 'ui',\n",
       " 'in',\n",
       " 'nn',\n",
       " 'n.',\n",
       " '.n',\n",
       " 'ne',\n",
       " 'ev',\n",
       " 'va',\n",
       " 'ae',\n",
       " 'eh',\n",
       " 'h.',\n",
       " '.i',\n",
       " 'iv',\n",
       " 'vy',\n",
       " 'y.',\n",
       " '.s',\n",
       " 'sa',\n",
       " 'ad',\n",
       " 'di',\n",
       " 'ie',\n",
       " 'e.',\n",
       " '.p',\n",
       " 'pi',\n",
       " 'ip',\n",
       " 'pe',\n",
       " 'er',\n",
       " 'r.',\n",
       " '.l',\n",
       " 'ly',\n",
       " 'yd',\n",
       " 'di',\n",
       " 'ia',\n",
       " 'a.',\n",
       " '.a',\n",
       " 'al',\n",
       " 'le',\n",
       " 'ex',\n",
       " 'xa',\n",
       " 'a.',\n",
       " '.j',\n",
       " 'jo',\n",
       " 'os',\n",
       " 'se',\n",
       " 'ep',\n",
       " 'ph',\n",
       " 'hi',\n",
       " 'in',\n",
       " 'ne',\n",
       " 'e.',\n",
       " '.e',\n",
       " 'em',\n",
       " 'me',\n",
       " 'er',\n",
       " 'ry',\n",
       " 'y.',\n",
       " '.j',\n",
       " 'ju',\n",
       " 'ul',\n",
       " 'li',\n",
       " 'ia',\n",
       " 'a.',\n",
       " '.d',\n",
       " 'de',\n",
       " 'el',\n",
       " 'li',\n",
       " 'il',\n",
       " 'la',\n",
       " 'ah',\n",
       " 'h.',\n",
       " '.a',\n",
       " 'ar',\n",
       " 'ri',\n",
       " 'ia',\n",
       " 'an',\n",
       " 'nn',\n",
       " 'na',\n",
       " 'a.',\n",
       " '.v',\n",
       " 'vi',\n",
       " 'iv',\n",
       " 'vi',\n",
       " 'ia',\n",
       " 'an',\n",
       " 'n.',\n",
       " '.k',\n",
       " 'ka',\n",
       " 'ay',\n",
       " 'yl',\n",
       " 'le',\n",
       " 'ee',\n",
       " 'e.',\n",
       " '.s',\n",
       " 'so',\n",
       " 'op',\n",
       " 'ph',\n",
       " 'hi',\n",
       " 'ie',\n",
       " 'e.',\n",
       " '.b',\n",
       " 'br',\n",
       " 'ri',\n",
       " 'ie',\n",
       " 'el',\n",
       " 'll',\n",
       " 'le',\n",
       " 'e.',\n",
       " '.m',\n",
       " 'ma',\n",
       " 'ad',\n",
       " 'de',\n",
       " 'el',\n",
       " 'li',\n",
       " 'in',\n",
       " 'ne',\n",
       " 'e.',\n",
       " '.p',\n",
       " 'pe',\n",
       " 'ey',\n",
       " 'yt',\n",
       " 'to',\n",
       " 'on',\n",
       " 'n.',\n",
       " '.r',\n",
       " 'ry',\n",
       " 'yl',\n",
       " 'le',\n",
       " 'ee',\n",
       " 'e.',\n",
       " '.c',\n",
       " 'cl',\n",
       " 'la',\n",
       " 'ar',\n",
       " 'ra',\n",
       " 'a.',\n",
       " '.h',\n",
       " 'ha',\n",
       " 'ad',\n",
       " 'dl',\n",
       " 'le',\n",
       " 'ey',\n",
       " 'y.',\n",
       " '.m',\n",
       " 'me',\n",
       " 'el',\n",
       " 'la',\n",
       " 'an',\n",
       " 'ni',\n",
       " 'ie',\n",
       " 'e.',\n",
       " '.m',\n",
       " 'ma',\n",
       " 'ac',\n",
       " 'ck',\n",
       " 'ke',\n",
       " 'en',\n",
       " 'nz',\n",
       " 'zi',\n",
       " 'ie',\n",
       " 'e.',\n",
       " '.r',\n",
       " 're',\n",
       " 'ea',\n",
       " 'ag',\n",
       " 'ga',\n",
       " 'an',\n",
       " 'n.',\n",
       " '.a',\n",
       " 'ad',\n",
       " 'da',\n",
       " 'al',\n",
       " 'ly',\n",
       " 'yn',\n",
       " 'nn',\n",
       " 'n.',\n",
       " '.l',\n",
       " 'li',\n",
       " 'il',\n",
       " 'li',\n",
       " 'ia',\n",
       " 'an',\n",
       " 'na',\n",
       " 'a.',\n",
       " '.a',\n",
       " 'au',\n",
       " 'ub',\n",
       " 'br',\n",
       " 're',\n",
       " 'ee',\n",
       " 'e.',\n",
       " '.j',\n",
       " 'ja',\n",
       " 'ad',\n",
       " 'de',\n",
       " 'e.',\n",
       " '.k',\n",
       " 'ka',\n",
       " 'at',\n",
       " 'th',\n",
       " 'he',\n",
       " 'er',\n",
       " 'ri',\n",
       " 'in',\n",
       " 'ne',\n",
       " 'e.',\n",
       " '.i',\n",
       " 'is',\n",
       " 'sa',\n",
       " 'ab',\n",
       " 'be',\n",
       " 'el',\n",
       " 'll',\n",
       " 'le',\n",
       " 'e.',\n",
       " '.n',\n",
       " 'na',\n",
       " 'at',\n",
       " 'ta',\n",
       " 'al',\n",
       " 'li',\n",
       " 'ia',\n",
       " 'a.',\n",
       " '.r',\n",
       " 'ra',\n",
       " 'ae',\n",
       " 'el',\n",
       " 'ly',\n",
       " 'yn',\n",
       " 'nn',\n",
       " 'n.',\n",
       " '.m',\n",
       " 'ma',\n",
       " 'ar',\n",
       " 'ri',\n",
       " 'ia',\n",
       " 'a.',\n",
       " '.a',\n",
       " 'at',\n",
       " 'th',\n",
       " 'he',\n",
       " 'en',\n",
       " 'na',\n",
       " 'a.',\n",
       " '.x',\n",
       " 'xi',\n",
       " 'im',\n",
       " 'me',\n",
       " 'en',\n",
       " 'na',\n",
       " 'a.',\n",
       " '.a',\n",
       " 'ar',\n",
       " 'ry',\n",
       " 'ya',\n",
       " 'a.',\n",
       " '.l',\n",
       " 'le',\n",
       " 'ei',\n",
       " 'il',\n",
       " 'la',\n",
       " 'an',\n",
       " 'ni',\n",
       " 'i.',\n",
       " '.t',\n",
       " 'ta',\n",
       " 'ay',\n",
       " 'yl',\n",
       " 'lo',\n",
       " 'or',\n",
       " 'r.',\n",
       " '.f',\n",
       " 'fa',\n",
       " 'ai',\n",
       " 'it',\n",
       " 'th',\n",
       " 'h.',\n",
       " '.r',\n",
       " 'ro',\n",
       " 'os',\n",
       " 'se',\n",
       " 'e.',\n",
       " '.k',\n",
       " 'ky',\n",
       " 'yl',\n",
       " 'li',\n",
       " 'ie',\n",
       " 'e.',\n",
       " '.a',\n",
       " 'al',\n",
       " 'le',\n",
       " 'ex',\n",
       " 'xa',\n",
       " 'an',\n",
       " 'nd',\n",
       " 'dr',\n",
       " 'ra',\n",
       " 'a.',\n",
       " '.m',\n",
       " 'ma',\n",
       " 'ar',\n",
       " 'ry',\n",
       " 'y.',\n",
       " '.m',\n",
       " 'ma',\n",
       " 'ar',\n",
       " 'rg',\n",
       " 'ga',\n",
       " 'ar',\n",
       " 're',\n",
       " 'et',\n",
       " 't.',\n",
       " '.l',\n",
       " 'ly',\n",
       " 'yl',\n",
       " 'la',\n",
       " 'a.',\n",
       " '.a',\n",
       " 'as',\n",
       " 'sh',\n",
       " 'hl',\n",
       " 'le',\n",
       " 'ey',\n",
       " 'y.',\n",
       " '.a',\n",
       " 'am',\n",
       " 'ma',\n",
       " 'ay',\n",
       " 'ya',\n",
       " 'a.',\n",
       " '.e',\n",
       " 'el',\n",
       " 'li',\n",
       " 'iz',\n",
       " 'za',\n",
       " 'a.',\n",
       " '.b',\n",
       " 'br',\n",
       " 'ri',\n",
       " 'ia',\n",
       " 'an',\n",
       " 'nn',\n",
       " 'na',\n",
       " 'a.',\n",
       " '.b',\n",
       " 'ba',\n",
       " 'ai',\n",
       " 'il',\n",
       " 'le',\n",
       " 'ey',\n",
       " 'y.',\n",
       " '.a',\n",
       " 'an',\n",
       " 'nd',\n",
       " 'dr',\n",
       " 're',\n",
       " 'ea',\n",
       " 'a.',\n",
       " '.k',\n",
       " 'kh',\n",
       " 'hl',\n",
       " 'lo',\n",
       " 'oe',\n",
       " 'e.',\n",
       " '.j',\n",
       " 'ja',\n",
       " 'as',\n",
       " 'sm',\n",
       " 'mi',\n",
       " 'in',\n",
       " 'ne',\n",
       " 'e.',\n",
       " '.m',\n",
       " 'me',\n",
       " 'el',\n",
       " 'lo',\n",
       " 'od',\n",
       " 'dy',\n",
       " 'y.',\n",
       " '.i',\n",
       " 'ir',\n",
       " 'ri',\n",
       " 'is',\n",
       " 's.',\n",
       " '.i',\n",
       " 'is',\n",
       " 'sa',\n",
       " 'ab',\n",
       " 'be',\n",
       " 'el',\n",
       " 'l.',\n",
       " '.n',\n",
       " 'no',\n",
       " 'or',\n",
       " 'ra',\n",
       " 'ah',\n",
       " 'h.',\n",
       " '.a',\n",
       " 'an',\n",
       " 'nn',\n",
       " 'na',\n",
       " 'ab',\n",
       " 'be',\n",
       " 'el',\n",
       " 'll',\n",
       " 'le',\n",
       " 'e.',\n",
       " '.v',\n",
       " 'va',\n",
       " 'al',\n",
       " 'le',\n",
       " 'er',\n",
       " 'ri',\n",
       " 'ia',\n",
       " 'a.',\n",
       " '.e',\n",
       " 'em',\n",
       " 'me',\n",
       " 'er',\n",
       " 'rs',\n",
       " 'so',\n",
       " 'on',\n",
       " 'n.',\n",
       " '.a',\n",
       " 'ad',\n",
       " 'da',\n",
       " 'al',\n",
       " 'ly',\n",
       " 'yn',\n",
       " 'n.',\n",
       " '.r',\n",
       " 'ry',\n",
       " 'yl',\n",
       " ...]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#[char for char in w for w in words]\n",
    "[f'{c}{c1}' for w in words for c, c1 in zip(f'.{w}.', f'.{w}.'[1:])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  5.],\n",
       "        [ 5., 13.],\n",
       "        [13., 13.],\n",
       "        ...,\n",
       "        [25., 26.],\n",
       "        [26., 24.],\n",
       "        [24.,  0.]])"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_pairs = torch.tensor(\n",
    "    [[stoi[c], stoi[c1]] for w in words for c, c1 in zip(f'.{w}.', f'.{w}.'[1:])]\n",
    ").float()\n",
    "word_pairs\n",
    "# xs = torch.tensor(xs).float()\n",
    "# xs[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.],\n",
       "         [ 5.],\n",
       "         [13.],\n",
       "         ...,\n",
       "         [25.],\n",
       "         [26.],\n",
       "         [24.]]),\n",
       " tensor([[ 5.],\n",
       "         [13.],\n",
       "         [13.],\n",
       "         ...,\n",
       "         [26.],\n",
       "         [24.],\n",
       "         [ 0.]]))"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs = word_pairs[:,:1]\n",
    "ys = word_pairs[:,1:]\n",
    "xs,ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_neurons = 100\n",
    "predictable_chars = len(letters) + 1\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(1, hidden_neurons),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(hidden_neurons, predictable_chars),\n",
    "    torch.nn.Softmax()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daynil/mambaforge-pypy3/envs/ai-base/lib/python3.11/site-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([228146, 27]),\n",
       " tensor([[3.6331e-02, 4.2283e-02, 2.6865e-02,  ..., 6.9518e-02, 4.0645e-02,\n",
       "          4.6356e-02],\n",
       "         [1.2353e-02, 2.9820e-02, 1.5131e-02,  ..., 5.5166e-03, 5.5027e-02,\n",
       "          1.0606e-01],\n",
       "         [3.1652e-04, 4.7240e-03, 6.8943e-04,  ..., 1.7975e-05, 7.4913e-03,\n",
       "          1.3982e-01],\n",
       "         ...,\n",
       "         [5.3601e-07, 1.3013e-04, 2.6604e-06,  ..., 1.3688e-09, 1.5724e-04,\n",
       "          9.4873e-02],\n",
       "         [3.1181e-07, 9.5391e-05, 1.6579e-06,  ..., 6.1497e-10, 1.1272e-04,\n",
       "          9.0825e-02],\n",
       "         [9.2065e-07, 1.7737e-04, 4.2655e-06,  ..., 3.0443e-09, 2.1915e-04,\n",
       "          9.9017e-02]], grad_fn=<SoftmaxBackward0>))"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preds = model(xs)\n",
    "test_preds.shape, test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1., grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preds[0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0695, grad_fn=<MaxBackward1>),\n",
       " tensor([0.0363, 0.0423, 0.0269, 0.0517, 0.0351, 0.0348, 0.0326, 0.0354, 0.0337,\n",
       "         0.0281, 0.0306, 0.0683, 0.0330, 0.0371, 0.0187, 0.0365, 0.0233, 0.0478,\n",
       "         0.0226, 0.0310, 0.0381, 0.0333, 0.0393, 0.0270, 0.0695, 0.0406, 0.0464],\n",
       "        grad_fn=<SelectBackward0>))"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preds[0].max(), test_preds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(24)"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preds[0].argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0695, grad_fn=<MaxBackward1>)"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preds[0].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([0.0695, 0.2577, 0.5700,  ..., 0.7608, 0.7715, 0.7496],\n",
       "       grad_fn=<MaxBackward0>),\n",
       "indices=tensor([24, 11, 11,  ..., 11, 11, 11]))"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preds.max(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "228146"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preds.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(preds: torch.Tensor):\n",
    "    return (-torch.log(preds.max(dim=1).values).sum()) / test_preds.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.2578, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn(test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0219, 0.0348, 0.0533, 0.0416, 0.0252, 0.0760, 0.0333, 0.0720, 0.0300,\n",
       "        0.0654, 0.0357, 0.0850, 0.0332, 0.0318, 0.0059, 0.0129, 0.0217, 0.0211,\n",
       "        0.0780, 0.0352, 0.0114, 0.0077, 0.0165, 0.0219, 0.0156, 0.0724, 0.0403],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.2794, grad_fn=<DivBackward0>)\n",
      "tensor(0.0179, grad_fn=<DivBackward0>)\n",
      "tensor(0.0165, grad_fn=<DivBackward0>)\n",
      "tensor(0.0154, grad_fn=<DivBackward0>)\n",
      "tensor(0.0143, grad_fn=<DivBackward0>)\n",
      "tensor(0.0134, grad_fn=<DivBackward0>)\n",
      "tensor(0.0126, grad_fn=<DivBackward0>)\n",
      "tensor(0.0119, grad_fn=<DivBackward0>)\n",
      "tensor(0.0113, grad_fn=<DivBackward0>)\n",
      "tensor(0.0107, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "hidden_neurons = 100\n",
    "predictable_chars = len(letters) + 1\n",
    "\n",
    "epochs = 10\n",
    "lr = 0.2\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(1, hidden_neurons),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(hidden_neurons, predictable_chars),\n",
    "    torch.nn.Softmax()\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr)\n",
    "\n",
    "def train_step(xb):\n",
    "    preds = model(xb)\n",
    "    loss = loss_fn(preds)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    return loss\n",
    "\n",
    "def valid_step(xb):\n",
    "    preds = model(xb)\n",
    "    loss = loss_fn(preds)\n",
    "    return loss\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    loss = train_step(xs)\n",
    "    print(loss)\n",
    "    # for batch in dls.train:\n",
    "    #     xb, yb = batch\n",
    "    #     xb = to_bw_flattened(xb)\n",
    "    #     xb, yb = to_torch_tensor(xb, yb)\n",
    "    #     loss, acc = train_step(xb, yb)\n",
    "\n",
    "    # print(f\"Avg loss: {tot_loss / valid_batches}, Avg acc: {tot_acc / valid_batches}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thinking about it, the above is actually wrong because what I'm calculating loss on doesn't make sense. I'm basically giving it the first letter, then calculating loss on the confidence of whatever it predicts as the second.\n",
    "\n",
    "What I need to do is cross entropy loss - give it the xs and the labels, and calculate cross entropy loss on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.],\n",
       "         [ 5.],\n",
       "         [13.],\n",
       "         ...,\n",
       "         [25.],\n",
       "         [26.],\n",
       "         [24.]]),\n",
       " tensor([[ 5.],\n",
       "         [13.],\n",
       "         [13.],\n",
       "         ...,\n",
       "         [26.],\n",
       "         [24.],\n",
       "         [ 0.]]))"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs, ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.],\n",
       "         [ 5.],\n",
       "         [13.],\n",
       "         ...,\n",
       "         [25.],\n",
       "         [26.],\n",
       "         [24.]]),\n",
       " tensor([ 5., 13., 13.,  ..., 26., 24.,  0.]))"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs, ys.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0],\n",
       "        [ 5],\n",
       "        [13],\n",
       "        ...,\n",
       "        [25],\n",
       "        [26],\n",
       "        [24]], dtype=torch.int32)"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs.int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  5, 13,  ..., 25, 26, 24], dtype=torch.int32)"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs.to(torch.int64).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5, 13, 13,  ..., 26, 24,  0])"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys.to(torch.int64).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 0, 1, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 1],\n",
       "        [0, 0, 0,  ..., 1, 0, 0]])"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.one_hot(xs.squeeze().to(torch.int64), 27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 0, 0, 1],\n",
       "        [0, 0, 0,  ..., 1, 0, 0],\n",
       "        [1, 0, 0,  ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.one_hot(ys.squeeze().to(torch.int64), 27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.3066, grad_fn=<DivBackward1>)\n",
      "tensor(3.0894, grad_fn=<DivBackward1>)\n",
      "tensor(2.9425, grad_fn=<DivBackward1>)\n",
      "tensor(2.8668, grad_fn=<DivBackward1>)\n",
      "tensor(2.8288, grad_fn=<DivBackward1>)\n",
      "tensor(2.8046, grad_fn=<DivBackward1>)\n",
      "tensor(2.7865, grad_fn=<DivBackward1>)\n",
      "tensor(2.7711, grad_fn=<DivBackward1>)\n",
      "tensor(2.7569, grad_fn=<DivBackward1>)\n",
      "tensor(2.7435, grad_fn=<DivBackward1>)\n",
      "tensor(2.7306, grad_fn=<DivBackward1>)\n",
      "tensor(2.7179, grad_fn=<DivBackward1>)\n",
      "tensor(2.7058, grad_fn=<DivBackward1>)\n",
      "tensor(2.6938, grad_fn=<DivBackward1>)\n",
      "tensor(2.6828, grad_fn=<DivBackward1>)\n",
      "tensor(2.6719, grad_fn=<DivBackward1>)\n",
      "tensor(2.6621, grad_fn=<DivBackward1>)\n",
      "tensor(2.6541, grad_fn=<DivBackward1>)\n",
      "tensor(2.6500, grad_fn=<DivBackward1>)\n",
      "tensor(2.6603, grad_fn=<DivBackward1>)\n",
      "tensor(2.7022, grad_fn=<DivBackward1>)\n",
      "tensor(2.7783, grad_fn=<DivBackward1>)\n",
      "tensor(2.7740, grad_fn=<DivBackward1>)\n",
      "tensor(2.6420, grad_fn=<DivBackward1>)\n",
      "tensor(2.6152, grad_fn=<DivBackward1>)\n",
      "tensor(2.6088, grad_fn=<DivBackward1>)\n",
      "tensor(2.6025, grad_fn=<DivBackward1>)\n",
      "tensor(2.6039, grad_fn=<DivBackward1>)\n",
      "tensor(2.6071, grad_fn=<DivBackward1>)\n",
      "tensor(2.6207, grad_fn=<DivBackward1>)\n",
      "tensor(2.6328, grad_fn=<DivBackward1>)\n",
      "tensor(2.6316, grad_fn=<DivBackward1>)\n",
      "tensor(2.6217, grad_fn=<DivBackward1>)\n",
      "tensor(2.6018, grad_fn=<DivBackward1>)\n",
      "tensor(2.5935, grad_fn=<DivBackward1>)\n",
      "tensor(2.5859, grad_fn=<DivBackward1>)\n",
      "tensor(2.5828, grad_fn=<DivBackward1>)\n",
      "tensor(2.5794, grad_fn=<DivBackward1>)\n",
      "tensor(2.5773, grad_fn=<DivBackward1>)\n",
      "tensor(2.5745, grad_fn=<DivBackward1>)\n",
      "tensor(2.5742, grad_fn=<DivBackward1>)\n",
      "tensor(2.5707, grad_fn=<DivBackward1>)\n",
      "tensor(2.5678, grad_fn=<DivBackward1>)\n",
      "tensor(2.5651, grad_fn=<DivBackward1>)\n",
      "tensor(2.5623, grad_fn=<DivBackward1>)\n",
      "tensor(2.5606, grad_fn=<DivBackward1>)\n",
      "tensor(2.5587, grad_fn=<DivBackward1>)\n",
      "tensor(2.5559, grad_fn=<DivBackward1>)\n",
      "tensor(2.5549, grad_fn=<DivBackward1>)\n",
      "tensor(2.5520, grad_fn=<DivBackward1>)\n"
     ]
    }
   ],
   "source": [
    "hidden_neurons = 100\n",
    "predictable_chars = len(letters) + 1\n",
    "\n",
    "epochs = 50\n",
    "lr = 3\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(27, hidden_neurons),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(hidden_neurons, predictable_chars),\n",
    "    # cross entropy loss expects raw unnormalized logits\n",
    "    # torch.nn.Softmax()\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr)\n",
    "\n",
    "def train_step(xb, yb):\n",
    "    preds = model(xb)\n",
    "    loss = loss_fn(preds, yb)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    return loss\n",
    "\n",
    "# def valid_step(xb):\n",
    "#     preds = model(xb)\n",
    "#     loss = loss_fn(preds)\n",
    "#     return loss\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    loss = train_step(\n",
    "        F.one_hot(xs.squeeze().to(torch.int64), 27).float(), \n",
    "        F.one_hot(ys.squeeze().to(torch.int64), 27).float()\n",
    "    )\n",
    "    print(loss)\n",
    "    # for batch in dls.train:\n",
    "    #     xb, yb = batch\n",
    "    #     xb = to_bw_flattened(xb)\n",
    "    #     xb, yb = to_torch_tensor(xb, yb)\n",
    "    #     loss, acc = train_step(xb, yb)\n",
    "\n",
    "    # print(f\"Avg loss: {tot_loss / valid_batches}, Avg acc: {tot_acc / valid_batches}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2.6328, -0.0816, -0.4266, -0.3451,  0.4298,  0.1665, -1.5470, -1.0255,\n",
       "         1.2468,  0.7366, -1.3115, -0.2626,  1.4293,  0.8547,  2.3013, -0.5743,\n",
       "        -1.5736, -2.0400,  1.6903,  0.7525,  0.2714, -0.3738,  0.0182, -1.4725,\n",
       "        -1.6215,  1.0940, -0.5529], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "letter_a = torch.zeros(27)\n",
    "letter_a[1] = 1\n",
    "single_pred_logits = model(letter_a)\n",
    "single_pred_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daynil/mambaforge-pypy3/envs/ai-base/lib/python3.11/site-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_pred = torch.nn.Softmax()(single_pred_logits)\n",
    "single_pred.argmax(dim=0)\n",
    "# itos[single_pred.argmax().item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keen\n",
      "aely\n",
      "droma\n",
      "lien\n",
      "\n",
      "b\n",
      "miacun\n",
      "ble\n",
      "crtya\n",
      "kin\n",
      "lhrinnyledel\n",
      "wesadalauhaon\n",
      "gevesch\n",
      "pmilipnren\n",
      "anlealtisn\n",
      "com\n",
      "dllslefsi\n",
      "tant\n",
      "ryi\n",
      "san\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "words_to_gen = 20\n",
    "for wi in range(words_to_gen):\n",
    "    # word_len = torch.randint(min(word_lens), max(word_lens), (1,)).item()\n",
    "    # for i in range(int(word_len)):\n",
    "    li = 0\n",
    "    next_letter = ''\n",
    "    genned_word = '.'\n",
    "    while next_letter != '.' and len(genned_word) < max(word_lens):\n",
    "        prior_letter = genned_word[li]\n",
    "        prior_letter_i = stoi[prior_letter]\n",
    "        prior_letter_i_one_hot = F.one_hot(\n",
    "            torch.tensor(prior_letter_i), 27\n",
    "        ).float()\n",
    "\n",
    "        probs = torch.nn.Softmax()(model(prior_letter_i_one_hot))\n",
    "        pred = torch.multinomial(probs, 1, replacement=True).item()\n",
    "        next_letter = itos[int(pred)]\n",
    "\n",
    "        # next_sample_p = P[prior_letter_i]\n",
    "        # next_letter_idx = torch.multinomial(\n",
    "        #         next_sample_p, 1, replacement=True, generator=g\n",
    "        #     ).item()\n",
    "        # next_letter = itos[next_letter_idx]\n",
    "        genned_word += next_letter\n",
    "        li += 1\n",
    "    print(genned_word.strip('.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Karpathy:\n",
    "Update: I added some suggested exercises to the description of the video. imo learning requires in-person tinkering and work, watching a video is not enough. If you complete the exercises please feel free to link your work here. (+Feel free to suggest other good exercises!)\n",
    "\n",
    "E01: train a trigram language model, i.e. take two characters as an input to predict the 3rd one. Feel free to use either counting or a neural net. Evaluate the loss; Did it improve over a bigram model?\n",
    "\n",
    "E02: split up the dataset randomly into 80% train set, 10% dev set, 10% test set. Train the bigram and trigram models only on the training set. Evaluate them on dev and test splits. What can you see?\n",
    "\n",
    "E03: use the dev set to tune the strength of smoothing (or regularization) for the trigram model - i.e. try many possibilities and see which one works best based on the dev set loss. What patterns can you see in the train and dev set loss as you tune this strength? Take the best setting of the smoothing and evaluate on the test set once and at the end. How good of a loss do you achieve?\n",
    "\n",
    "E04: we saw that our 1-hot vectors merely select a row of W, so producing these vectors explicitly feels wasteful. Can you delete our use of F.one_hot in favor of simply indexing into rows of W?\n",
    "\n",
    "E05: look up and use F.cross_entropy instead. You should achieve the same result. Can you think of why we'd prefer to use F.cross_entropy instead?\n",
    "\n",
    "E06: meta-exercise! Think of a fun/interesting exercise and complete it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train a trigram language model\n",
    "E01: train a trigram language model, i.e. take two characters as an input to predict the 3rd one. Feel free to use either counting or a neural net. Evaluate the loss; Did it improve over a bigram model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to create a new vocab. The neural net wouldn't know what to do with 2 letters passed individually - we can't one hot encode 2 different answers. So to create a trigram, we create a vocab of all possible 2 letter combos with indices.\n",
    "\n",
    "Note that we have to generate this from the alphabet rather than the vocab since we may come up with novel word pairs that didn't exist in the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'q',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'w',\n",
       " 'x',\n",
       " 'y',\n",
       " 'z',\n",
       " '.']"
      ]
     },
     "execution_count": 411,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "letters + ['.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'..': 0,\n",
       " '.a': 1,\n",
       " '.b': 2,\n",
       " '.c': 3,\n",
       " '.d': 4,\n",
       " '.e': 5,\n",
       " '.f': 6,\n",
       " '.g': 7,\n",
       " '.h': 8,\n",
       " '.i': 9,\n",
       " '.j': 10,\n",
       " '.k': 11,\n",
       " '.l': 12,\n",
       " '.m': 13,\n",
       " '.n': 14,\n",
       " '.o': 15,\n",
       " '.p': 16,\n",
       " '.q': 17,\n",
       " '.r': 18,\n",
       " '.s': 19,\n",
       " '.t': 20,\n",
       " '.u': 21,\n",
       " '.v': 22,\n",
       " '.w': 23,\n",
       " '.x': 24,\n",
       " '.y': 25,\n",
       " '.z': 26,\n",
       " 'a.': 27,\n",
       " 'aa': 28,\n",
       " 'ab': 29,\n",
       " 'ac': 30,\n",
       " 'ad': 31,\n",
       " 'ae': 32,\n",
       " 'af': 33,\n",
       " 'ag': 34,\n",
       " 'ah': 35,\n",
       " 'ai': 36,\n",
       " 'aj': 37,\n",
       " 'ak': 38,\n",
       " 'al': 39,\n",
       " 'am': 40,\n",
       " 'an': 41,\n",
       " 'ao': 42,\n",
       " 'ap': 43,\n",
       " 'aq': 44,\n",
       " 'ar': 45,\n",
       " 'as': 46,\n",
       " 'at': 47,\n",
       " 'au': 48,\n",
       " 'av': 49,\n",
       " 'aw': 50,\n",
       " 'ax': 51,\n",
       " 'ay': 52,\n",
       " 'az': 53,\n",
       " 'b.': 54,\n",
       " 'ba': 55,\n",
       " 'bb': 56,\n",
       " 'bc': 57,\n",
       " 'bd': 58,\n",
       " 'be': 59,\n",
       " 'bf': 60,\n",
       " 'bg': 61,\n",
       " 'bh': 62,\n",
       " 'bi': 63,\n",
       " 'bj': 64,\n",
       " 'bk': 65,\n",
       " 'bl': 66,\n",
       " 'bm': 67,\n",
       " 'bn': 68,\n",
       " 'bo': 69,\n",
       " 'bp': 70,\n",
       " 'bq': 71,\n",
       " 'br': 72,\n",
       " 'bs': 73,\n",
       " 'bt': 74,\n",
       " 'bu': 75,\n",
       " 'bv': 76,\n",
       " 'bw': 77,\n",
       " 'bx': 78,\n",
       " 'by': 79,\n",
       " 'bz': 80,\n",
       " 'c.': 81,\n",
       " 'ca': 82,\n",
       " 'cb': 83,\n",
       " 'cc': 84,\n",
       " 'cd': 85,\n",
       " 'ce': 86,\n",
       " 'cf': 87,\n",
       " 'cg': 88,\n",
       " 'ch': 89,\n",
       " 'ci': 90,\n",
       " 'cj': 91,\n",
       " 'ck': 92,\n",
       " 'cl': 93,\n",
       " 'cm': 94,\n",
       " 'cn': 95,\n",
       " 'co': 96,\n",
       " 'cp': 97,\n",
       " 'cq': 98,\n",
       " 'cr': 99,\n",
       " 'cs': 100,\n",
       " 'ct': 101,\n",
       " 'cu': 102,\n",
       " 'cv': 103,\n",
       " 'cw': 104,\n",
       " 'cx': 105,\n",
       " 'cy': 106,\n",
       " 'cz': 107,\n",
       " 'd.': 108,\n",
       " 'da': 109,\n",
       " 'db': 110,\n",
       " 'dc': 111,\n",
       " 'dd': 112,\n",
       " 'de': 113,\n",
       " 'df': 114,\n",
       " 'dg': 115,\n",
       " 'dh': 116,\n",
       " 'di': 117,\n",
       " 'dj': 118,\n",
       " 'dk': 119,\n",
       " 'dl': 120,\n",
       " 'dm': 121,\n",
       " 'dn': 122,\n",
       " 'do': 123,\n",
       " 'dp': 124,\n",
       " 'dq': 125,\n",
       " 'dr': 126,\n",
       " 'ds': 127,\n",
       " 'dt': 128,\n",
       " 'du': 129,\n",
       " 'dv': 130,\n",
       " 'dw': 131,\n",
       " 'dx': 132,\n",
       " 'dy': 133,\n",
       " 'dz': 134,\n",
       " 'e.': 135,\n",
       " 'ea': 136,\n",
       " 'eb': 137,\n",
       " 'ec': 138,\n",
       " 'ed': 139,\n",
       " 'ee': 140,\n",
       " 'ef': 141,\n",
       " 'eg': 142,\n",
       " 'eh': 143,\n",
       " 'ei': 144,\n",
       " 'ej': 145,\n",
       " 'ek': 146,\n",
       " 'el': 147,\n",
       " 'em': 148,\n",
       " 'en': 149,\n",
       " 'eo': 150,\n",
       " 'ep': 151,\n",
       " 'eq': 152,\n",
       " 'er': 153,\n",
       " 'es': 154,\n",
       " 'et': 155,\n",
       " 'eu': 156,\n",
       " 'ev': 157,\n",
       " 'ew': 158,\n",
       " 'ex': 159,\n",
       " 'ey': 160,\n",
       " 'ez': 161,\n",
       " 'f.': 162,\n",
       " 'fa': 163,\n",
       " 'fb': 164,\n",
       " 'fc': 165,\n",
       " 'fd': 166,\n",
       " 'fe': 167,\n",
       " 'ff': 168,\n",
       " 'fg': 169,\n",
       " 'fh': 170,\n",
       " 'fi': 171,\n",
       " 'fj': 172,\n",
       " 'fk': 173,\n",
       " 'fl': 174,\n",
       " 'fm': 175,\n",
       " 'fn': 176,\n",
       " 'fo': 177,\n",
       " 'fp': 178,\n",
       " 'fq': 179,\n",
       " 'fr': 180,\n",
       " 'fs': 181,\n",
       " 'ft': 182,\n",
       " 'fu': 183,\n",
       " 'fv': 184,\n",
       " 'fw': 185,\n",
       " 'fx': 186,\n",
       " 'fy': 187,\n",
       " 'fz': 188,\n",
       " 'g.': 189,\n",
       " 'ga': 190,\n",
       " 'gb': 191,\n",
       " 'gc': 192,\n",
       " 'gd': 193,\n",
       " 'ge': 194,\n",
       " 'gf': 195,\n",
       " 'gg': 196,\n",
       " 'gh': 197,\n",
       " 'gi': 198,\n",
       " 'gj': 199,\n",
       " 'gk': 200,\n",
       " 'gl': 201,\n",
       " 'gm': 202,\n",
       " 'gn': 203,\n",
       " 'go': 204,\n",
       " 'gp': 205,\n",
       " 'gq': 206,\n",
       " 'gr': 207,\n",
       " 'gs': 208,\n",
       " 'gt': 209,\n",
       " 'gu': 210,\n",
       " 'gv': 211,\n",
       " 'gw': 212,\n",
       " 'gx': 213,\n",
       " 'gy': 214,\n",
       " 'gz': 215,\n",
       " 'h.': 216,\n",
       " 'ha': 217,\n",
       " 'hb': 218,\n",
       " 'hc': 219,\n",
       " 'hd': 220,\n",
       " 'he': 221,\n",
       " 'hf': 222,\n",
       " 'hg': 223,\n",
       " 'hh': 224,\n",
       " 'hi': 225,\n",
       " 'hj': 226,\n",
       " 'hk': 227,\n",
       " 'hl': 228,\n",
       " 'hm': 229,\n",
       " 'hn': 230,\n",
       " 'ho': 231,\n",
       " 'hp': 232,\n",
       " 'hq': 233,\n",
       " 'hr': 234,\n",
       " 'hs': 235,\n",
       " 'ht': 236,\n",
       " 'hu': 237,\n",
       " 'hv': 238,\n",
       " 'hw': 239,\n",
       " 'hx': 240,\n",
       " 'hy': 241,\n",
       " 'hz': 242,\n",
       " 'i.': 243,\n",
       " 'ia': 244,\n",
       " 'ib': 245,\n",
       " 'ic': 246,\n",
       " 'id': 247,\n",
       " 'ie': 248,\n",
       " 'if': 249,\n",
       " 'ig': 250,\n",
       " 'ih': 251,\n",
       " 'ii': 252,\n",
       " 'ij': 253,\n",
       " 'ik': 254,\n",
       " 'il': 255,\n",
       " 'im': 256,\n",
       " 'in': 257,\n",
       " 'io': 258,\n",
       " 'ip': 259,\n",
       " 'iq': 260,\n",
       " 'ir': 261,\n",
       " 'is': 262,\n",
       " 'it': 263,\n",
       " 'iu': 264,\n",
       " 'iv': 265,\n",
       " 'iw': 266,\n",
       " 'ix': 267,\n",
       " 'iy': 268,\n",
       " 'iz': 269,\n",
       " 'j.': 270,\n",
       " 'ja': 271,\n",
       " 'jb': 272,\n",
       " 'jc': 273,\n",
       " 'jd': 274,\n",
       " 'je': 275,\n",
       " 'jf': 276,\n",
       " 'jg': 277,\n",
       " 'jh': 278,\n",
       " 'ji': 279,\n",
       " 'jj': 280,\n",
       " 'jk': 281,\n",
       " 'jl': 282,\n",
       " 'jm': 283,\n",
       " 'jn': 284,\n",
       " 'jo': 285,\n",
       " 'jp': 286,\n",
       " 'jq': 287,\n",
       " 'jr': 288,\n",
       " 'js': 289,\n",
       " 'jt': 290,\n",
       " 'ju': 291,\n",
       " 'jv': 292,\n",
       " 'jw': 293,\n",
       " 'jx': 294,\n",
       " 'jy': 295,\n",
       " 'jz': 296,\n",
       " 'k.': 297,\n",
       " 'ka': 298,\n",
       " 'kb': 299,\n",
       " 'kc': 300,\n",
       " 'kd': 301,\n",
       " 'ke': 302,\n",
       " 'kf': 303,\n",
       " 'kg': 304,\n",
       " 'kh': 305,\n",
       " 'ki': 306,\n",
       " 'kj': 307,\n",
       " 'kk': 308,\n",
       " 'kl': 309,\n",
       " 'km': 310,\n",
       " 'kn': 311,\n",
       " 'ko': 312,\n",
       " 'kp': 313,\n",
       " 'kq': 314,\n",
       " 'kr': 315,\n",
       " 'ks': 316,\n",
       " 'kt': 317,\n",
       " 'ku': 318,\n",
       " 'kv': 319,\n",
       " 'kw': 320,\n",
       " 'kx': 321,\n",
       " 'ky': 322,\n",
       " 'kz': 323,\n",
       " 'l.': 324,\n",
       " 'la': 325,\n",
       " 'lb': 326,\n",
       " 'lc': 327,\n",
       " 'ld': 328,\n",
       " 'le': 329,\n",
       " 'lf': 330,\n",
       " 'lg': 331,\n",
       " 'lh': 332,\n",
       " 'li': 333,\n",
       " 'lj': 334,\n",
       " 'lk': 335,\n",
       " 'll': 336,\n",
       " 'lm': 337,\n",
       " 'ln': 338,\n",
       " 'lo': 339,\n",
       " 'lp': 340,\n",
       " 'lq': 341,\n",
       " 'lr': 342,\n",
       " 'ls': 343,\n",
       " 'lt': 344,\n",
       " 'lu': 345,\n",
       " 'lv': 346,\n",
       " 'lw': 347,\n",
       " 'lx': 348,\n",
       " 'ly': 349,\n",
       " 'lz': 350,\n",
       " 'm.': 351,\n",
       " 'ma': 352,\n",
       " 'mb': 353,\n",
       " 'mc': 354,\n",
       " 'md': 355,\n",
       " 'me': 356,\n",
       " 'mf': 357,\n",
       " 'mg': 358,\n",
       " 'mh': 359,\n",
       " 'mi': 360,\n",
       " 'mj': 361,\n",
       " 'mk': 362,\n",
       " 'ml': 363,\n",
       " 'mm': 364,\n",
       " 'mn': 365,\n",
       " 'mo': 366,\n",
       " 'mp': 367,\n",
       " 'mq': 368,\n",
       " 'mr': 369,\n",
       " 'ms': 370,\n",
       " 'mt': 371,\n",
       " 'mu': 372,\n",
       " 'mv': 373,\n",
       " 'mw': 374,\n",
       " 'mx': 375,\n",
       " 'my': 376,\n",
       " 'mz': 377,\n",
       " 'n.': 378,\n",
       " 'na': 379,\n",
       " 'nb': 380,\n",
       " 'nc': 381,\n",
       " 'nd': 382,\n",
       " 'ne': 383,\n",
       " 'nf': 384,\n",
       " 'ng': 385,\n",
       " 'nh': 386,\n",
       " 'ni': 387,\n",
       " 'nj': 388,\n",
       " 'nk': 389,\n",
       " 'nl': 390,\n",
       " 'nm': 391,\n",
       " 'nn': 392,\n",
       " 'no': 393,\n",
       " 'np': 394,\n",
       " 'nq': 395,\n",
       " 'nr': 396,\n",
       " 'ns': 397,\n",
       " 'nt': 398,\n",
       " 'nu': 399,\n",
       " 'nv': 400,\n",
       " 'nw': 401,\n",
       " 'nx': 402,\n",
       " 'ny': 403,\n",
       " 'nz': 404,\n",
       " 'o.': 405,\n",
       " 'oa': 406,\n",
       " 'ob': 407,\n",
       " 'oc': 408,\n",
       " 'od': 409,\n",
       " 'oe': 410,\n",
       " 'of': 411,\n",
       " 'og': 412,\n",
       " 'oh': 413,\n",
       " 'oi': 414,\n",
       " 'oj': 415,\n",
       " 'ok': 416,\n",
       " 'ol': 417,\n",
       " 'om': 418,\n",
       " 'on': 419,\n",
       " 'oo': 420,\n",
       " 'op': 421,\n",
       " 'oq': 422,\n",
       " 'or': 423,\n",
       " 'os': 424,\n",
       " 'ot': 425,\n",
       " 'ou': 426,\n",
       " 'ov': 427,\n",
       " 'ow': 428,\n",
       " 'ox': 429,\n",
       " 'oy': 430,\n",
       " 'oz': 431,\n",
       " 'p.': 432,\n",
       " 'pa': 433,\n",
       " 'pb': 434,\n",
       " 'pc': 435,\n",
       " 'pd': 436,\n",
       " 'pe': 437,\n",
       " 'pf': 438,\n",
       " 'pg': 439,\n",
       " 'ph': 440,\n",
       " 'pi': 441,\n",
       " 'pj': 442,\n",
       " 'pk': 443,\n",
       " 'pl': 444,\n",
       " 'pm': 445,\n",
       " 'pn': 446,\n",
       " 'po': 447,\n",
       " 'pp': 448,\n",
       " 'pq': 449,\n",
       " 'pr': 450,\n",
       " 'ps': 451,\n",
       " 'pt': 452,\n",
       " 'pu': 453,\n",
       " 'pv': 454,\n",
       " 'pw': 455,\n",
       " 'px': 456,\n",
       " 'py': 457,\n",
       " 'pz': 458,\n",
       " 'q.': 459,\n",
       " 'qa': 460,\n",
       " 'qb': 461,\n",
       " 'qc': 462,\n",
       " 'qd': 463,\n",
       " 'qe': 464,\n",
       " 'qf': 465,\n",
       " 'qg': 466,\n",
       " 'qh': 467,\n",
       " 'qi': 468,\n",
       " 'qj': 469,\n",
       " 'qk': 470,\n",
       " 'ql': 471,\n",
       " 'qm': 472,\n",
       " 'qn': 473,\n",
       " 'qo': 474,\n",
       " 'qp': 475,\n",
       " 'qq': 476,\n",
       " 'qr': 477,\n",
       " 'qs': 478,\n",
       " 'qt': 479,\n",
       " 'qu': 480,\n",
       " 'qv': 481,\n",
       " 'qw': 482,\n",
       " 'qx': 483,\n",
       " 'qy': 484,\n",
       " 'qz': 485,\n",
       " 'r.': 486,\n",
       " 'ra': 487,\n",
       " 'rb': 488,\n",
       " 'rc': 489,\n",
       " 'rd': 490,\n",
       " 're': 491,\n",
       " 'rf': 492,\n",
       " 'rg': 493,\n",
       " 'rh': 494,\n",
       " 'ri': 495,\n",
       " 'rj': 496,\n",
       " 'rk': 497,\n",
       " 'rl': 498,\n",
       " 'rm': 499,\n",
       " 'rn': 500,\n",
       " 'ro': 501,\n",
       " 'rp': 502,\n",
       " 'rq': 503,\n",
       " 'rr': 504,\n",
       " 'rs': 505,\n",
       " 'rt': 506,\n",
       " 'ru': 507,\n",
       " 'rv': 508,\n",
       " 'rw': 509,\n",
       " 'rx': 510,\n",
       " 'ry': 511,\n",
       " 'rz': 512,\n",
       " 's.': 513,\n",
       " 'sa': 514,\n",
       " 'sb': 515,\n",
       " 'sc': 516,\n",
       " 'sd': 517,\n",
       " 'se': 518,\n",
       " 'sf': 519,\n",
       " 'sg': 520,\n",
       " 'sh': 521,\n",
       " 'si': 522,\n",
       " 'sj': 523,\n",
       " 'sk': 524,\n",
       " 'sl': 525,\n",
       " 'sm': 526,\n",
       " 'sn': 527,\n",
       " 'so': 528,\n",
       " 'sp': 529,\n",
       " 'sq': 530,\n",
       " 'sr': 531,\n",
       " 'ss': 532,\n",
       " 'st': 533,\n",
       " 'su': 534,\n",
       " 'sv': 535,\n",
       " 'sw': 536,\n",
       " 'sx': 537,\n",
       " 'sy': 538,\n",
       " 'sz': 539,\n",
       " 't.': 540,\n",
       " 'ta': 541,\n",
       " 'tb': 542,\n",
       " 'tc': 543,\n",
       " 'td': 544,\n",
       " 'te': 545,\n",
       " 'tf': 546,\n",
       " 'tg': 547,\n",
       " 'th': 548,\n",
       " 'ti': 549,\n",
       " 'tj': 550,\n",
       " 'tk': 551,\n",
       " 'tl': 552,\n",
       " 'tm': 553,\n",
       " 'tn': 554,\n",
       " 'to': 555,\n",
       " 'tp': 556,\n",
       " 'tq': 557,\n",
       " 'tr': 558,\n",
       " 'ts': 559,\n",
       " 'tt': 560,\n",
       " 'tu': 561,\n",
       " 'tv': 562,\n",
       " 'tw': 563,\n",
       " 'tx': 564,\n",
       " 'ty': 565,\n",
       " 'tz': 566,\n",
       " 'u.': 567,\n",
       " 'ua': 568,\n",
       " 'ub': 569,\n",
       " 'uc': 570,\n",
       " 'ud': 571,\n",
       " 'ue': 572,\n",
       " 'uf': 573,\n",
       " 'ug': 574,\n",
       " 'uh': 575,\n",
       " 'ui': 576,\n",
       " 'uj': 577,\n",
       " 'uk': 578,\n",
       " 'ul': 579,\n",
       " 'um': 580,\n",
       " 'un': 581,\n",
       " 'uo': 582,\n",
       " 'up': 583,\n",
       " 'uq': 584,\n",
       " 'ur': 585,\n",
       " 'us': 586,\n",
       " 'ut': 587,\n",
       " 'uu': 588,\n",
       " 'uv': 589,\n",
       " 'uw': 590,\n",
       " 'ux': 591,\n",
       " 'uy': 592,\n",
       " 'uz': 593,\n",
       " 'v.': 594,\n",
       " 'va': 595,\n",
       " 'vb': 596,\n",
       " 'vc': 597,\n",
       " 'vd': 598,\n",
       " 've': 599,\n",
       " 'vf': 600,\n",
       " 'vg': 601,\n",
       " 'vh': 602,\n",
       " 'vi': 603,\n",
       " 'vj': 604,\n",
       " 'vk': 605,\n",
       " 'vl': 606,\n",
       " 'vm': 607,\n",
       " 'vn': 608,\n",
       " 'vo': 609,\n",
       " 'vp': 610,\n",
       " 'vq': 611,\n",
       " 'vr': 612,\n",
       " 'vs': 613,\n",
       " 'vt': 614,\n",
       " 'vu': 615,\n",
       " 'vv': 616,\n",
       " 'vw': 617,\n",
       " 'vx': 618,\n",
       " 'vy': 619,\n",
       " 'vz': 620,\n",
       " 'w.': 621,\n",
       " 'wa': 622,\n",
       " 'wb': 623,\n",
       " 'wc': 624,\n",
       " 'wd': 625,\n",
       " 'we': 626,\n",
       " 'wf': 627,\n",
       " 'wg': 628,\n",
       " 'wh': 629,\n",
       " 'wi': 630,\n",
       " 'wj': 631,\n",
       " 'wk': 632,\n",
       " 'wl': 633,\n",
       " 'wm': 634,\n",
       " 'wn': 635,\n",
       " 'wo': 636,\n",
       " 'wp': 637,\n",
       " 'wq': 638,\n",
       " 'wr': 639,\n",
       " 'ws': 640,\n",
       " 'wt': 641,\n",
       " 'wu': 642,\n",
       " 'wv': 643,\n",
       " 'ww': 644,\n",
       " 'wx': 645,\n",
       " 'wy': 646,\n",
       " 'wz': 647,\n",
       " 'x.': 648,\n",
       " 'xa': 649,\n",
       " 'xb': 650,\n",
       " 'xc': 651,\n",
       " 'xd': 652,\n",
       " 'xe': 653,\n",
       " 'xf': 654,\n",
       " 'xg': 655,\n",
       " 'xh': 656,\n",
       " 'xi': 657,\n",
       " 'xj': 658,\n",
       " 'xk': 659,\n",
       " 'xl': 660,\n",
       " 'xm': 661,\n",
       " 'xn': 662,\n",
       " 'xo': 663,\n",
       " 'xp': 664,\n",
       " 'xq': 665,\n",
       " 'xr': 666,\n",
       " 'xs': 667,\n",
       " 'xt': 668,\n",
       " 'xu': 669,\n",
       " 'xv': 670,\n",
       " 'xw': 671,\n",
       " 'xx': 672,\n",
       " 'xy': 673,\n",
       " 'xz': 674,\n",
       " 'y.': 675,\n",
       " 'ya': 676,\n",
       " 'yb': 677,\n",
       " 'yc': 678,\n",
       " 'yd': 679,\n",
       " 'ye': 680,\n",
       " 'yf': 681,\n",
       " 'yg': 682,\n",
       " 'yh': 683,\n",
       " 'yi': 684,\n",
       " 'yj': 685,\n",
       " 'yk': 686,\n",
       " 'yl': 687,\n",
       " 'ym': 688,\n",
       " 'yn': 689,\n",
       " 'yo': 690,\n",
       " 'yp': 691,\n",
       " 'yq': 692,\n",
       " 'yr': 693,\n",
       " 'ys': 694,\n",
       " 'yt': 695,\n",
       " 'yu': 696,\n",
       " 'yv': 697,\n",
       " 'yw': 698,\n",
       " 'yx': 699,\n",
       " 'yy': 700,\n",
       " 'yz': 701,\n",
       " 'z.': 702,\n",
       " 'za': 703,\n",
       " 'zb': 704,\n",
       " 'zc': 705,\n",
       " 'zd': 706,\n",
       " 'ze': 707,\n",
       " 'zf': 708,\n",
       " 'zg': 709,\n",
       " 'zh': 710,\n",
       " 'zi': 711,\n",
       " 'zj': 712,\n",
       " 'zk': 713,\n",
       " 'zl': 714,\n",
       " 'zm': 715,\n",
       " 'zn': 716,\n",
       " 'zo': 717,\n",
       " 'zp': 718,\n",
       " 'zq': 719,\n",
       " 'zr': 720,\n",
       " 'zs': 721,\n",
       " 'zt': 722,\n",
       " 'zu': 723,\n",
       " 'zv': 724,\n",
       " 'zw': 725,\n",
       " 'zx': 726,\n",
       " 'zy': 727,\n",
       " 'zz': 728}"
      ]
     },
     "execution_count": 414,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs = []\n",
    "for l in letters + ['.']:\n",
    "    for l2 in letters + ['.']:\n",
    "        pair = l + l2\n",
    "        pairs.append(pair)\n",
    "\n",
    "pairs = sorted(pairs)\n",
    "ptoi = {p:i for i, p in enumerate(pairs)}\n",
    "ptoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '..',\n",
       " 1: '.a',\n",
       " 2: '.b',\n",
       " 3: '.c',\n",
       " 4: '.d',\n",
       " 5: '.e',\n",
       " 6: '.f',\n",
       " 7: '.g',\n",
       " 8: '.h',\n",
       " 9: '.i',\n",
       " 10: '.j',\n",
       " 11: '.k',\n",
       " 12: '.l',\n",
       " 13: '.m',\n",
       " 14: '.n',\n",
       " 15: '.o',\n",
       " 16: '.p',\n",
       " 17: '.q',\n",
       " 18: '.r',\n",
       " 19: '.s',\n",
       " 20: '.t',\n",
       " 21: '.u',\n",
       " 22: '.v',\n",
       " 23: '.w',\n",
       " 24: '.x',\n",
       " 25: '.y',\n",
       " 26: '.z',\n",
       " 27: 'a.',\n",
       " 28: 'aa',\n",
       " 29: 'ab',\n",
       " 30: 'ac',\n",
       " 31: 'ad',\n",
       " 32: 'ae',\n",
       " 33: 'af',\n",
       " 34: 'ag',\n",
       " 35: 'ah',\n",
       " 36: 'ai',\n",
       " 37: 'aj',\n",
       " 38: 'ak',\n",
       " 39: 'al',\n",
       " 40: 'am',\n",
       " 41: 'an',\n",
       " 42: 'ao',\n",
       " 43: 'ap',\n",
       " 44: 'aq',\n",
       " 45: 'ar',\n",
       " 46: 'as',\n",
       " 47: 'at',\n",
       " 48: 'au',\n",
       " 49: 'av',\n",
       " 50: 'aw',\n",
       " 51: 'ax',\n",
       " 52: 'ay',\n",
       " 53: 'az',\n",
       " 54: 'b.',\n",
       " 55: 'ba',\n",
       " 56: 'bb',\n",
       " 57: 'bc',\n",
       " 58: 'bd',\n",
       " 59: 'be',\n",
       " 60: 'bf',\n",
       " 61: 'bg',\n",
       " 62: 'bh',\n",
       " 63: 'bi',\n",
       " 64: 'bj',\n",
       " 65: 'bk',\n",
       " 66: 'bl',\n",
       " 67: 'bm',\n",
       " 68: 'bn',\n",
       " 69: 'bo',\n",
       " 70: 'bp',\n",
       " 71: 'bq',\n",
       " 72: 'br',\n",
       " 73: 'bs',\n",
       " 74: 'bt',\n",
       " 75: 'bu',\n",
       " 76: 'bv',\n",
       " 77: 'bw',\n",
       " 78: 'bx',\n",
       " 79: 'by',\n",
       " 80: 'bz',\n",
       " 81: 'c.',\n",
       " 82: 'ca',\n",
       " 83: 'cb',\n",
       " 84: 'cc',\n",
       " 85: 'cd',\n",
       " 86: 'ce',\n",
       " 87: 'cf',\n",
       " 88: 'cg',\n",
       " 89: 'ch',\n",
       " 90: 'ci',\n",
       " 91: 'cj',\n",
       " 92: 'ck',\n",
       " 93: 'cl',\n",
       " 94: 'cm',\n",
       " 95: 'cn',\n",
       " 96: 'co',\n",
       " 97: 'cp',\n",
       " 98: 'cq',\n",
       " 99: 'cr',\n",
       " 100: 'cs',\n",
       " 101: 'ct',\n",
       " 102: 'cu',\n",
       " 103: 'cv',\n",
       " 104: 'cw',\n",
       " 105: 'cx',\n",
       " 106: 'cy',\n",
       " 107: 'cz',\n",
       " 108: 'd.',\n",
       " 109: 'da',\n",
       " 110: 'db',\n",
       " 111: 'dc',\n",
       " 112: 'dd',\n",
       " 113: 'de',\n",
       " 114: 'df',\n",
       " 115: 'dg',\n",
       " 116: 'dh',\n",
       " 117: 'di',\n",
       " 118: 'dj',\n",
       " 119: 'dk',\n",
       " 120: 'dl',\n",
       " 121: 'dm',\n",
       " 122: 'dn',\n",
       " 123: 'do',\n",
       " 124: 'dp',\n",
       " 125: 'dq',\n",
       " 126: 'dr',\n",
       " 127: 'ds',\n",
       " 128: 'dt',\n",
       " 129: 'du',\n",
       " 130: 'dv',\n",
       " 131: 'dw',\n",
       " 132: 'dx',\n",
       " 133: 'dy',\n",
       " 134: 'dz',\n",
       " 135: 'e.',\n",
       " 136: 'ea',\n",
       " 137: 'eb',\n",
       " 138: 'ec',\n",
       " 139: 'ed',\n",
       " 140: 'ee',\n",
       " 141: 'ef',\n",
       " 142: 'eg',\n",
       " 143: 'eh',\n",
       " 144: 'ei',\n",
       " 145: 'ej',\n",
       " 146: 'ek',\n",
       " 147: 'el',\n",
       " 148: 'em',\n",
       " 149: 'en',\n",
       " 150: 'eo',\n",
       " 151: 'ep',\n",
       " 152: 'eq',\n",
       " 153: 'er',\n",
       " 154: 'es',\n",
       " 155: 'et',\n",
       " 156: 'eu',\n",
       " 157: 'ev',\n",
       " 158: 'ew',\n",
       " 159: 'ex',\n",
       " 160: 'ey',\n",
       " 161: 'ez',\n",
       " 162: 'f.',\n",
       " 163: 'fa',\n",
       " 164: 'fb',\n",
       " 165: 'fc',\n",
       " 166: 'fd',\n",
       " 167: 'fe',\n",
       " 168: 'ff',\n",
       " 169: 'fg',\n",
       " 170: 'fh',\n",
       " 171: 'fi',\n",
       " 172: 'fj',\n",
       " 173: 'fk',\n",
       " 174: 'fl',\n",
       " 175: 'fm',\n",
       " 176: 'fn',\n",
       " 177: 'fo',\n",
       " 178: 'fp',\n",
       " 179: 'fq',\n",
       " 180: 'fr',\n",
       " 181: 'fs',\n",
       " 182: 'ft',\n",
       " 183: 'fu',\n",
       " 184: 'fv',\n",
       " 185: 'fw',\n",
       " 186: 'fx',\n",
       " 187: 'fy',\n",
       " 188: 'fz',\n",
       " 189: 'g.',\n",
       " 190: 'ga',\n",
       " 191: 'gb',\n",
       " 192: 'gc',\n",
       " 193: 'gd',\n",
       " 194: 'ge',\n",
       " 195: 'gf',\n",
       " 196: 'gg',\n",
       " 197: 'gh',\n",
       " 198: 'gi',\n",
       " 199: 'gj',\n",
       " 200: 'gk',\n",
       " 201: 'gl',\n",
       " 202: 'gm',\n",
       " 203: 'gn',\n",
       " 204: 'go',\n",
       " 205: 'gp',\n",
       " 206: 'gq',\n",
       " 207: 'gr',\n",
       " 208: 'gs',\n",
       " 209: 'gt',\n",
       " 210: 'gu',\n",
       " 211: 'gv',\n",
       " 212: 'gw',\n",
       " 213: 'gx',\n",
       " 214: 'gy',\n",
       " 215: 'gz',\n",
       " 216: 'h.',\n",
       " 217: 'ha',\n",
       " 218: 'hb',\n",
       " 219: 'hc',\n",
       " 220: 'hd',\n",
       " 221: 'he',\n",
       " 222: 'hf',\n",
       " 223: 'hg',\n",
       " 224: 'hh',\n",
       " 225: 'hi',\n",
       " 226: 'hj',\n",
       " 227: 'hk',\n",
       " 228: 'hl',\n",
       " 229: 'hm',\n",
       " 230: 'hn',\n",
       " 231: 'ho',\n",
       " 232: 'hp',\n",
       " 233: 'hq',\n",
       " 234: 'hr',\n",
       " 235: 'hs',\n",
       " 236: 'ht',\n",
       " 237: 'hu',\n",
       " 238: 'hv',\n",
       " 239: 'hw',\n",
       " 240: 'hx',\n",
       " 241: 'hy',\n",
       " 242: 'hz',\n",
       " 243: 'i.',\n",
       " 244: 'ia',\n",
       " 245: 'ib',\n",
       " 246: 'ic',\n",
       " 247: 'id',\n",
       " 248: 'ie',\n",
       " 249: 'if',\n",
       " 250: 'ig',\n",
       " 251: 'ih',\n",
       " 252: 'ii',\n",
       " 253: 'ij',\n",
       " 254: 'ik',\n",
       " 255: 'il',\n",
       " 256: 'im',\n",
       " 257: 'in',\n",
       " 258: 'io',\n",
       " 259: 'ip',\n",
       " 260: 'iq',\n",
       " 261: 'ir',\n",
       " 262: 'is',\n",
       " 263: 'it',\n",
       " 264: 'iu',\n",
       " 265: 'iv',\n",
       " 266: 'iw',\n",
       " 267: 'ix',\n",
       " 268: 'iy',\n",
       " 269: 'iz',\n",
       " 270: 'j.',\n",
       " 271: 'ja',\n",
       " 272: 'jb',\n",
       " 273: 'jc',\n",
       " 274: 'jd',\n",
       " 275: 'je',\n",
       " 276: 'jf',\n",
       " 277: 'jg',\n",
       " 278: 'jh',\n",
       " 279: 'ji',\n",
       " 280: 'jj',\n",
       " 281: 'jk',\n",
       " 282: 'jl',\n",
       " 283: 'jm',\n",
       " 284: 'jn',\n",
       " 285: 'jo',\n",
       " 286: 'jp',\n",
       " 287: 'jq',\n",
       " 288: 'jr',\n",
       " 289: 'js',\n",
       " 290: 'jt',\n",
       " 291: 'ju',\n",
       " 292: 'jv',\n",
       " 293: 'jw',\n",
       " 294: 'jx',\n",
       " 295: 'jy',\n",
       " 296: 'jz',\n",
       " 297: 'k.',\n",
       " 298: 'ka',\n",
       " 299: 'kb',\n",
       " 300: 'kc',\n",
       " 301: 'kd',\n",
       " 302: 'ke',\n",
       " 303: 'kf',\n",
       " 304: 'kg',\n",
       " 305: 'kh',\n",
       " 306: 'ki',\n",
       " 307: 'kj',\n",
       " 308: 'kk',\n",
       " 309: 'kl',\n",
       " 310: 'km',\n",
       " 311: 'kn',\n",
       " 312: 'ko',\n",
       " 313: 'kp',\n",
       " 314: 'kq',\n",
       " 315: 'kr',\n",
       " 316: 'ks',\n",
       " 317: 'kt',\n",
       " 318: 'ku',\n",
       " 319: 'kv',\n",
       " 320: 'kw',\n",
       " 321: 'kx',\n",
       " 322: 'ky',\n",
       " 323: 'kz',\n",
       " 324: 'l.',\n",
       " 325: 'la',\n",
       " 326: 'lb',\n",
       " 327: 'lc',\n",
       " 328: 'ld',\n",
       " 329: 'le',\n",
       " 330: 'lf',\n",
       " 331: 'lg',\n",
       " 332: 'lh',\n",
       " 333: 'li',\n",
       " 334: 'lj',\n",
       " 335: 'lk',\n",
       " 336: 'll',\n",
       " 337: 'lm',\n",
       " 338: 'ln',\n",
       " 339: 'lo',\n",
       " 340: 'lp',\n",
       " 341: 'lq',\n",
       " 342: 'lr',\n",
       " 343: 'ls',\n",
       " 344: 'lt',\n",
       " 345: 'lu',\n",
       " 346: 'lv',\n",
       " 347: 'lw',\n",
       " 348: 'lx',\n",
       " 349: 'ly',\n",
       " 350: 'lz',\n",
       " 351: 'm.',\n",
       " 352: 'ma',\n",
       " 353: 'mb',\n",
       " 354: 'mc',\n",
       " 355: 'md',\n",
       " 356: 'me',\n",
       " 357: 'mf',\n",
       " 358: 'mg',\n",
       " 359: 'mh',\n",
       " 360: 'mi',\n",
       " 361: 'mj',\n",
       " 362: 'mk',\n",
       " 363: 'ml',\n",
       " 364: 'mm',\n",
       " 365: 'mn',\n",
       " 366: 'mo',\n",
       " 367: 'mp',\n",
       " 368: 'mq',\n",
       " 369: 'mr',\n",
       " 370: 'ms',\n",
       " 371: 'mt',\n",
       " 372: 'mu',\n",
       " 373: 'mv',\n",
       " 374: 'mw',\n",
       " 375: 'mx',\n",
       " 376: 'my',\n",
       " 377: 'mz',\n",
       " 378: 'n.',\n",
       " 379: 'na',\n",
       " 380: 'nb',\n",
       " 381: 'nc',\n",
       " 382: 'nd',\n",
       " 383: 'ne',\n",
       " 384: 'nf',\n",
       " 385: 'ng',\n",
       " 386: 'nh',\n",
       " 387: 'ni',\n",
       " 388: 'nj',\n",
       " 389: 'nk',\n",
       " 390: 'nl',\n",
       " 391: 'nm',\n",
       " 392: 'nn',\n",
       " 393: 'no',\n",
       " 394: 'np',\n",
       " 395: 'nq',\n",
       " 396: 'nr',\n",
       " 397: 'ns',\n",
       " 398: 'nt',\n",
       " 399: 'nu',\n",
       " 400: 'nv',\n",
       " 401: 'nw',\n",
       " 402: 'nx',\n",
       " 403: 'ny',\n",
       " 404: 'nz',\n",
       " 405: 'o.',\n",
       " 406: 'oa',\n",
       " 407: 'ob',\n",
       " 408: 'oc',\n",
       " 409: 'od',\n",
       " 410: 'oe',\n",
       " 411: 'of',\n",
       " 412: 'og',\n",
       " 413: 'oh',\n",
       " 414: 'oi',\n",
       " 415: 'oj',\n",
       " 416: 'ok',\n",
       " 417: 'ol',\n",
       " 418: 'om',\n",
       " 419: 'on',\n",
       " 420: 'oo',\n",
       " 421: 'op',\n",
       " 422: 'oq',\n",
       " 423: 'or',\n",
       " 424: 'os',\n",
       " 425: 'ot',\n",
       " 426: 'ou',\n",
       " 427: 'ov',\n",
       " 428: 'ow',\n",
       " 429: 'ox',\n",
       " 430: 'oy',\n",
       " 431: 'oz',\n",
       " 432: 'p.',\n",
       " 433: 'pa',\n",
       " 434: 'pb',\n",
       " 435: 'pc',\n",
       " 436: 'pd',\n",
       " 437: 'pe',\n",
       " 438: 'pf',\n",
       " 439: 'pg',\n",
       " 440: 'ph',\n",
       " 441: 'pi',\n",
       " 442: 'pj',\n",
       " 443: 'pk',\n",
       " 444: 'pl',\n",
       " 445: 'pm',\n",
       " 446: 'pn',\n",
       " 447: 'po',\n",
       " 448: 'pp',\n",
       " 449: 'pq',\n",
       " 450: 'pr',\n",
       " 451: 'ps',\n",
       " 452: 'pt',\n",
       " 453: 'pu',\n",
       " 454: 'pv',\n",
       " 455: 'pw',\n",
       " 456: 'px',\n",
       " 457: 'py',\n",
       " 458: 'pz',\n",
       " 459: 'q.',\n",
       " 460: 'qa',\n",
       " 461: 'qb',\n",
       " 462: 'qc',\n",
       " 463: 'qd',\n",
       " 464: 'qe',\n",
       " 465: 'qf',\n",
       " 466: 'qg',\n",
       " 467: 'qh',\n",
       " 468: 'qi',\n",
       " 469: 'qj',\n",
       " 470: 'qk',\n",
       " 471: 'ql',\n",
       " 472: 'qm',\n",
       " 473: 'qn',\n",
       " 474: 'qo',\n",
       " 475: 'qp',\n",
       " 476: 'qq',\n",
       " 477: 'qr',\n",
       " 478: 'qs',\n",
       " 479: 'qt',\n",
       " 480: 'qu',\n",
       " 481: 'qv',\n",
       " 482: 'qw',\n",
       " 483: 'qx',\n",
       " 484: 'qy',\n",
       " 485: 'qz',\n",
       " 486: 'r.',\n",
       " 487: 'ra',\n",
       " 488: 'rb',\n",
       " 489: 'rc',\n",
       " 490: 'rd',\n",
       " 491: 're',\n",
       " 492: 'rf',\n",
       " 493: 'rg',\n",
       " 494: 'rh',\n",
       " 495: 'ri',\n",
       " 496: 'rj',\n",
       " 497: 'rk',\n",
       " 498: 'rl',\n",
       " 499: 'rm',\n",
       " 500: 'rn',\n",
       " 501: 'ro',\n",
       " 502: 'rp',\n",
       " 503: 'rq',\n",
       " 504: 'rr',\n",
       " 505: 'rs',\n",
       " 506: 'rt',\n",
       " 507: 'ru',\n",
       " 508: 'rv',\n",
       " 509: 'rw',\n",
       " 510: 'rx',\n",
       " 511: 'ry',\n",
       " 512: 'rz',\n",
       " 513: 's.',\n",
       " 514: 'sa',\n",
       " 515: 'sb',\n",
       " 516: 'sc',\n",
       " 517: 'sd',\n",
       " 518: 'se',\n",
       " 519: 'sf',\n",
       " 520: 'sg',\n",
       " 521: 'sh',\n",
       " 522: 'si',\n",
       " 523: 'sj',\n",
       " 524: 'sk',\n",
       " 525: 'sl',\n",
       " 526: 'sm',\n",
       " 527: 'sn',\n",
       " 528: 'so',\n",
       " 529: 'sp',\n",
       " 530: 'sq',\n",
       " 531: 'sr',\n",
       " 532: 'ss',\n",
       " 533: 'st',\n",
       " 534: 'su',\n",
       " 535: 'sv',\n",
       " 536: 'sw',\n",
       " 537: 'sx',\n",
       " 538: 'sy',\n",
       " 539: 'sz',\n",
       " 540: 't.',\n",
       " 541: 'ta',\n",
       " 542: 'tb',\n",
       " 543: 'tc',\n",
       " 544: 'td',\n",
       " 545: 'te',\n",
       " 546: 'tf',\n",
       " 547: 'tg',\n",
       " 548: 'th',\n",
       " 549: 'ti',\n",
       " 550: 'tj',\n",
       " 551: 'tk',\n",
       " 552: 'tl',\n",
       " 553: 'tm',\n",
       " 554: 'tn',\n",
       " 555: 'to',\n",
       " 556: 'tp',\n",
       " 557: 'tq',\n",
       " 558: 'tr',\n",
       " 559: 'ts',\n",
       " 560: 'tt',\n",
       " 561: 'tu',\n",
       " 562: 'tv',\n",
       " 563: 'tw',\n",
       " 564: 'tx',\n",
       " 565: 'ty',\n",
       " 566: 'tz',\n",
       " 567: 'u.',\n",
       " 568: 'ua',\n",
       " 569: 'ub',\n",
       " 570: 'uc',\n",
       " 571: 'ud',\n",
       " 572: 'ue',\n",
       " 573: 'uf',\n",
       " 574: 'ug',\n",
       " 575: 'uh',\n",
       " 576: 'ui',\n",
       " 577: 'uj',\n",
       " 578: 'uk',\n",
       " 579: 'ul',\n",
       " 580: 'um',\n",
       " 581: 'un',\n",
       " 582: 'uo',\n",
       " 583: 'up',\n",
       " 584: 'uq',\n",
       " 585: 'ur',\n",
       " 586: 'us',\n",
       " 587: 'ut',\n",
       " 588: 'uu',\n",
       " 589: 'uv',\n",
       " 590: 'uw',\n",
       " 591: 'ux',\n",
       " 592: 'uy',\n",
       " 593: 'uz',\n",
       " 594: 'v.',\n",
       " 595: 'va',\n",
       " 596: 'vb',\n",
       " 597: 'vc',\n",
       " 598: 'vd',\n",
       " 599: 've',\n",
       " 600: 'vf',\n",
       " 601: 'vg',\n",
       " 602: 'vh',\n",
       " 603: 'vi',\n",
       " 604: 'vj',\n",
       " 605: 'vk',\n",
       " 606: 'vl',\n",
       " 607: 'vm',\n",
       " 608: 'vn',\n",
       " 609: 'vo',\n",
       " 610: 'vp',\n",
       " 611: 'vq',\n",
       " 612: 'vr',\n",
       " 613: 'vs',\n",
       " 614: 'vt',\n",
       " 615: 'vu',\n",
       " 616: 'vv',\n",
       " 617: 'vw',\n",
       " 618: 'vx',\n",
       " 619: 'vy',\n",
       " 620: 'vz',\n",
       " 621: 'w.',\n",
       " 622: 'wa',\n",
       " 623: 'wb',\n",
       " 624: 'wc',\n",
       " 625: 'wd',\n",
       " 626: 'we',\n",
       " 627: 'wf',\n",
       " 628: 'wg',\n",
       " 629: 'wh',\n",
       " 630: 'wi',\n",
       " 631: 'wj',\n",
       " 632: 'wk',\n",
       " 633: 'wl',\n",
       " 634: 'wm',\n",
       " 635: 'wn',\n",
       " 636: 'wo',\n",
       " 637: 'wp',\n",
       " 638: 'wq',\n",
       " 639: 'wr',\n",
       " 640: 'ws',\n",
       " 641: 'wt',\n",
       " 642: 'wu',\n",
       " 643: 'wv',\n",
       " 644: 'ww',\n",
       " 645: 'wx',\n",
       " 646: 'wy',\n",
       " 647: 'wz',\n",
       " 648: 'x.',\n",
       " 649: 'xa',\n",
       " 650: 'xb',\n",
       " 651: 'xc',\n",
       " 652: 'xd',\n",
       " 653: 'xe',\n",
       " 654: 'xf',\n",
       " 655: 'xg',\n",
       " 656: 'xh',\n",
       " 657: 'xi',\n",
       " 658: 'xj',\n",
       " 659: 'xk',\n",
       " 660: 'xl',\n",
       " 661: 'xm',\n",
       " 662: 'xn',\n",
       " 663: 'xo',\n",
       " 664: 'xp',\n",
       " 665: 'xq',\n",
       " 666: 'xr',\n",
       " 667: 'xs',\n",
       " 668: 'xt',\n",
       " 669: 'xu',\n",
       " 670: 'xv',\n",
       " 671: 'xw',\n",
       " 672: 'xx',\n",
       " 673: 'xy',\n",
       " 674: 'xz',\n",
       " 675: 'y.',\n",
       " 676: 'ya',\n",
       " 677: 'yb',\n",
       " 678: 'yc',\n",
       " 679: 'yd',\n",
       " 680: 'ye',\n",
       " 681: 'yf',\n",
       " 682: 'yg',\n",
       " 683: 'yh',\n",
       " 684: 'yi',\n",
       " 685: 'yj',\n",
       " 686: 'yk',\n",
       " 687: 'yl',\n",
       " 688: 'ym',\n",
       " 689: 'yn',\n",
       " 690: 'yo',\n",
       " 691: 'yp',\n",
       " 692: 'yq',\n",
       " 693: 'yr',\n",
       " 694: 'ys',\n",
       " 695: 'yt',\n",
       " 696: 'yu',\n",
       " 697: 'yv',\n",
       " 698: 'yw',\n",
       " 699: 'yx',\n",
       " 700: 'yy',\n",
       " 701: 'yz',\n",
       " 702: 'z.',\n",
       " 703: 'za',\n",
       " 704: 'zb',\n",
       " 705: 'zc',\n",
       " 706: 'zd',\n",
       " 707: 'ze',\n",
       " 708: 'zf',\n",
       " 709: 'zg',\n",
       " 710: 'zh',\n",
       " 711: 'zi',\n",
       " 712: 'zj',\n",
       " 713: 'zk',\n",
       " 714: 'zl',\n",
       " 715: 'zm',\n",
       " 716: 'zn',\n",
       " 717: 'zo',\n",
       " 718: 'zp',\n",
       " 719: 'zq',\n",
       " 720: 'zr',\n",
       " 721: 'zs',\n",
       " 722: 'zt',\n",
       " 723: 'zu',\n",
       " 724: 'zv',\n",
       " 725: 'zw',\n",
       " 726: 'zx',\n",
       " 727: 'zy',\n",
       " 728: 'zz'}"
      ]
     },
     "execution_count": 415,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "itop = {i:p for p, i in ptoi.items()}\n",
    "itop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "729"
      ]
     },
     "execution_count": 416,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ptoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([5,\n",
       "  148,\n",
       "  364,\n",
       "  352,\n",
       "  15,\n",
       "  417,\n",
       "  333,\n",
       "  265,\n",
       "  603,\n",
       "  244,\n",
       "  1,\n",
       "  49,\n",
       "  595,\n",
       "  9,\n",
       "  262,\n",
       "  514,\n",
       "  29,\n",
       "  59,\n",
       "  147,\n",
       "  336,\n",
       "  325,\n",
       "  19,\n",
       "  528,\n",
       "  421,\n",
       "  440,\n",
       "  225,\n",
       "  244,\n",
       "  3,\n",
       "  89,\n",
       "  217,\n",
       "  45,\n",
       "  498,\n",
       "  339,\n",
       "  425,\n",
       "  560,\n",
       "  545,\n",
       "  13,\n",
       "  360,\n",
       "  244,\n",
       "  1,\n",
       "  40,\n",
       "  356,\n",
       "  147,\n",
       "  333,\n",
       "  244,\n",
       "  8,\n",
       "  217,\n",
       "  45,\n",
       "  502,\n",
       "  437,\n",
       "  153,\n",
       "  5,\n",
       "  157,\n",
       "  599,\n",
       "  147,\n",
       "  349,\n",
       "  689,\n",
       "  1,\n",
       "  29,\n",
       "  63,\n",
       "  250,\n",
       "  190,\n",
       "  36,\n",
       "  255,\n",
       "  5,\n",
       "  148,\n",
       "  360,\n",
       "  255,\n",
       "  349,\n",
       "  5,\n",
       "  147,\n",
       "  333,\n",
       "  269,\n",
       "  703,\n",
       "  29,\n",
       "  59,\n",
       "  155,\n",
       "  548,\n",
       "  13,\n",
       "  360,\n",
       "  255,\n",
       "  325,\n",
       "  5,\n",
       "  147,\n",
       "  336,\n",
       "  325,\n",
       "  1,\n",
       "  49,\n",
       "  599,\n",
       "  153,\n",
       "  511,\n",
       "  19,\n",
       "  528,\n",
       "  411,\n",
       "  171,\n",
       "  244,\n",
       "  3,\n",
       "  82,\n",
       "  40,\n",
       "  360,\n",
       "  255,\n",
       "  325,\n",
       "  1,\n",
       "  45,\n",
       "  495,\n",
       "  244,\n",
       "  19,\n",
       "  516,\n",
       "  82,\n",
       "  45,\n",
       "  498,\n",
       "  329,\n",
       "  155,\n",
       "  560,\n",
       "  22,\n",
       "  603,\n",
       "  246,\n",
       "  101,\n",
       "  555,\n",
       "  423,\n",
       "  495,\n",
       "  244,\n",
       "  13,\n",
       "  352,\n",
       "  31,\n",
       "  117,\n",
       "  262,\n",
       "  528,\n",
       "  419,\n",
       "  12,\n",
       "  345,\n",
       "  581,\n",
       "  379,\n",
       "  7,\n",
       "  207,\n",
       "  487,\n",
       "  30,\n",
       "  86,\n",
       "  3,\n",
       "  89,\n",
       "  228,\n",
       "  339,\n",
       "  410,\n",
       "  16,\n",
       "  437,\n",
       "  149,\n",
       "  383,\n",
       "  147,\n",
       "  339,\n",
       "  421,\n",
       "  437,\n",
       "  12,\n",
       "  325,\n",
       "  52,\n",
       "  687,\n",
       "  325,\n",
       "  18,\n",
       "  495,\n",
       "  255,\n",
       "  329,\n",
       "  160,\n",
       "  26,\n",
       "  717,\n",
       "  410,\n",
       "  160,\n",
       "  14,\n",
       "  393,\n",
       "  423,\n",
       "  487,\n",
       "  12,\n",
       "  333,\n",
       "  255,\n",
       "  349,\n",
       "  5,\n",
       "  147,\n",
       "  329,\n",
       "  136,\n",
       "  41,\n",
       "  393,\n",
       "  423,\n",
       "  8,\n",
       "  217,\n",
       "  41,\n",
       "  392,\n",
       "  379,\n",
       "  35,\n",
       "  12,\n",
       "  333,\n",
       "  255,\n",
       "  336,\n",
       "  333,\n",
       "  244,\n",
       "  41,\n",
       "  1,\n",
       "  31,\n",
       "  112,\n",
       "  117,\n",
       "  262,\n",
       "  528,\n",
       "  419,\n",
       "  1,\n",
       "  48,\n",
       "  569,\n",
       "  72,\n",
       "  491,\n",
       "  160,\n",
       "  5,\n",
       "  147,\n",
       "  336,\n",
       "  333,\n",
       "  248,\n",
       "  19,\n",
       "  533,\n",
       "  545,\n",
       "  147,\n",
       "  336,\n",
       "  325,\n",
       "  14,\n",
       "  379,\n",
       "  47,\n",
       "  541,\n",
       "  39,\n",
       "  333,\n",
       "  248,\n",
       "  26,\n",
       "  717,\n",
       "  410,\n",
       "  12,\n",
       "  329,\n",
       "  136,\n",
       "  35,\n",
       "  8,\n",
       "  217,\n",
       "  53,\n",
       "  707,\n",
       "  147,\n",
       "  22,\n",
       "  603,\n",
       "  258,\n",
       "  417,\n",
       "  329,\n",
       "  155,\n",
       "  1,\n",
       "  48,\n",
       "  585,\n",
       "  501,\n",
       "  423,\n",
       "  487,\n",
       "  19,\n",
       "  514,\n",
       "  49,\n",
       "  595,\n",
       "  41,\n",
       "  392,\n",
       "  379,\n",
       "  35,\n",
       "  1,\n",
       "  48,\n",
       "  571,\n",
       "  126,\n",
       "  491,\n",
       "  160,\n",
       "  2,\n",
       "  72,\n",
       "  501,\n",
       "  420,\n",
       "  416,\n",
       "  309,\n",
       "  349,\n",
       "  689,\n",
       "  2,\n",
       "  59,\n",
       "  147,\n",
       "  336,\n",
       "  325,\n",
       "  3,\n",
       "  93,\n",
       "  325,\n",
       "  36,\n",
       "  261,\n",
       "  491,\n",
       "  19,\n",
       "  524,\n",
       "  322,\n",
       "  687,\n",
       "  325,\n",
       "  45,\n",
       "  12,\n",
       "  345,\n",
       "  570,\n",
       "  106,\n",
       "  16,\n",
       "  433,\n",
       "  36,\n",
       "  262,\n",
       "  525,\n",
       "  329,\n",
       "  160,\n",
       "  5,\n",
       "  157,\n",
       "  599,\n",
       "  153,\n",
       "  498,\n",
       "  349,\n",
       "  1,\n",
       "  41,\n",
       "  392,\n",
       "  379,\n",
       "  3,\n",
       "  82,\n",
       "  45,\n",
       "  501,\n",
       "  417,\n",
       "  333,\n",
       "  257,\n",
       "  383,\n",
       "  14,\n",
       "  393,\n",
       "  427,\n",
       "  595,\n",
       "  7,\n",
       "  194,\n",
       "  149,\n",
       "  383,\n",
       "  154,\n",
       "  522,\n",
       "  262,\n",
       "  5,\n",
       "  148,\n",
       "  360,\n",
       "  255,\n",
       "  333,\n",
       "  244,\n",
       "  11,\n",
       "  302,\n",
       "  149,\n",
       "  392,\n",
       "  383,\n",
       "  139,\n",
       "  133,\n",
       "  19,\n",
       "  514,\n",
       "  40,\n",
       "  352,\n",
       "  41,\n",
       "  398,\n",
       "  548,\n",
       "  217,\n",
       "  13,\n",
       "  352,\n",
       "  52,\n",
       "  676,\n",
       "  23,\n",
       "  630,\n",
       "  255,\n",
       "  336,\n",
       "  339,\n",
       "  428,\n",
       "  11,\n",
       "  306,\n",
       "  257,\n",
       "  397,\n",
       "  525,\n",
       "  329,\n",
       "  160,\n",
       "  14,\n",
       "  379,\n",
       "  42,\n",
       "  418,\n",
       "  360,\n",
       "  1,\n",
       "  28,\n",
       "  39,\n",
       "  333,\n",
       "  268,\n",
       "  676,\n",
       "  35,\n",
       "  5,\n",
       "  147,\n",
       "  329,\n",
       "  149,\n",
       "  379,\n",
       "  19,\n",
       "  514,\n",
       "  45,\n",
       "  487,\n",
       "  35,\n",
       "  1,\n",
       "  45,\n",
       "  495,\n",
       "  244,\n",
       "  41,\n",
       "  379,\n",
       "  1,\n",
       "  39,\n",
       "  336,\n",
       "  333,\n",
       "  262,\n",
       "  528,\n",
       "  419,\n",
       "  7,\n",
       "  190,\n",
       "  29,\n",
       "  72,\n",
       "  495,\n",
       "  248,\n",
       "  147,\n",
       "  336,\n",
       "  325,\n",
       "  1,\n",
       "  39,\n",
       "  333,\n",
       "  246,\n",
       "  86,\n",
       "  13,\n",
       "  352,\n",
       "  31,\n",
       "  113,\n",
       "  147,\n",
       "  349,\n",
       "  689,\n",
       "  3,\n",
       "  96,\n",
       "  423,\n",
       "  487,\n",
       "  18,\n",
       "  507,\n",
       "  569,\n",
       "  79,\n",
       "  5,\n",
       "  157,\n",
       "  595,\n",
       "  19,\n",
       "  518,\n",
       "  153,\n",
       "  491,\n",
       "  149,\n",
       "  387,\n",
       "  263,\n",
       "  565,\n",
       "  1,\n",
       "  48,\n",
       "  587,\n",
       "  561,\n",
       "  580,\n",
       "  365,\n",
       "  1,\n",
       "  31,\n",
       "  113,\n",
       "  147,\n",
       "  333,\n",
       "  257,\n",
       "  383,\n",
       "  8,\n",
       "  217,\n",
       "  36,\n",
       "  255,\n",
       "  329,\n",
       "  160,\n",
       "  7,\n",
       "  198,\n",
       "  244,\n",
       "  41,\n",
       "  392,\n",
       "  379,\n",
       "  22,\n",
       "  595,\n",
       "  39,\n",
       "  329,\n",
       "  149,\n",
       "  398,\n",
       "  549,\n",
       "  257,\n",
       "  379,\n",
       "  9,\n",
       "  262,\n",
       "  525,\n",
       "  325,\n",
       "  5,\n",
       "  147,\n",
       "  333,\n",
       "  244,\n",
       "  41,\n",
       "  379,\n",
       "  17,\n",
       "  480,\n",
       "  576,\n",
       "  257,\n",
       "  392,\n",
       "  14,\n",
       "  383,\n",
       "  157,\n",
       "  595,\n",
       "  32,\n",
       "  143,\n",
       "  9,\n",
       "  265,\n",
       "  619,\n",
       "  19,\n",
       "  514,\n",
       "  31,\n",
       "  117,\n",
       "  248,\n",
       "  16,\n",
       "  441,\n",
       "  259,\n",
       "  437,\n",
       "  153,\n",
       "  12,\n",
       "  349,\n",
       "  679,\n",
       "  117,\n",
       "  244,\n",
       "  1,\n",
       "  39,\n",
       "  329,\n",
       "  159,\n",
       "  649,\n",
       "  10,\n",
       "  285,\n",
       "  424,\n",
       "  518,\n",
       "  151,\n",
       "  440,\n",
       "  225,\n",
       "  257,\n",
       "  383,\n",
       "  5,\n",
       "  148,\n",
       "  356,\n",
       "  153,\n",
       "  511,\n",
       "  10,\n",
       "  291,\n",
       "  579,\n",
       "  333,\n",
       "  244,\n",
       "  4,\n",
       "  113,\n",
       "  147,\n",
       "  333,\n",
       "  255,\n",
       "  325,\n",
       "  35,\n",
       "  1,\n",
       "  45,\n",
       "  495,\n",
       "  244,\n",
       "  41,\n",
       "  392,\n",
       "  379,\n",
       "  22,\n",
       "  603,\n",
       "  265,\n",
       "  603,\n",
       "  244,\n",
       "  41,\n",
       "  11,\n",
       "  298,\n",
       "  52,\n",
       "  687,\n",
       "  329,\n",
       "  140,\n",
       "  19,\n",
       "  528,\n",
       "  421,\n",
       "  440,\n",
       "  225,\n",
       "  248,\n",
       "  2,\n",
       "  72,\n",
       "  495,\n",
       "  248,\n",
       "  147,\n",
       "  336,\n",
       "  329,\n",
       "  13,\n",
       "  352,\n",
       "  31,\n",
       "  113,\n",
       "  147,\n",
       "  333,\n",
       "  257,\n",
       "  383,\n",
       "  16,\n",
       "  437,\n",
       "  160,\n",
       "  695,\n",
       "  555,\n",
       "  419,\n",
       "  18,\n",
       "  511,\n",
       "  687,\n",
       "  329,\n",
       "  140,\n",
       "  3,\n",
       "  93,\n",
       "  325,\n",
       "  45,\n",
       "  487,\n",
       "  8,\n",
       "  217,\n",
       "  31,\n",
       "  120,\n",
       "  329,\n",
       "  160,\n",
       "  13,\n",
       "  356,\n",
       "  147,\n",
       "  325,\n",
       "  41,\n",
       "  387,\n",
       "  248,\n",
       "  13,\n",
       "  352,\n",
       "  30,\n",
       "  92,\n",
       "  302,\n",
       "  149,\n",
       "  404,\n",
       "  711,\n",
       "  248,\n",
       "  18,\n",
       "  491,\n",
       "  136,\n",
       "  34,\n",
       "  190,\n",
       "  41,\n",
       "  1,\n",
       "  31,\n",
       "  109,\n",
       "  39,\n",
       "  349,\n",
       "  689,\n",
       "  392,\n",
       "  12,\n",
       "  333,\n",
       "  255,\n",
       "  333,\n",
       "  244,\n",
       "  41,\n",
       "  379,\n",
       "  1,\n",
       "  48,\n",
       "  569,\n",
       "  72,\n",
       "  491,\n",
       "  140,\n",
       "  10,\n",
       "  271,\n",
       "  31,\n",
       "  113,\n",
       "  11,\n",
       "  298,\n",
       "  47,\n",
       "  548,\n",
       "  221,\n",
       "  153,\n",
       "  495,\n",
       "  257,\n",
       "  383,\n",
       "  9,\n",
       "  262,\n",
       "  514,\n",
       "  29,\n",
       "  59,\n",
       "  147,\n",
       "  336,\n",
       "  329,\n",
       "  14,\n",
       "  379,\n",
       "  47,\n",
       "  541,\n",
       "  39,\n",
       "  333,\n",
       "  244,\n",
       "  18,\n",
       "  487,\n",
       "  32,\n",
       "  147,\n",
       "  349,\n",
       "  689,\n",
       "  392,\n",
       "  13,\n",
       "  352,\n",
       "  45,\n",
       "  495,\n",
       "  244,\n",
       "  1,\n",
       "  47,\n",
       "  548,\n",
       "  221,\n",
       "  149,\n",
       "  379,\n",
       "  24,\n",
       "  657,\n",
       "  256,\n",
       "  356,\n",
       "  149,\n",
       "  379,\n",
       "  1,\n",
       "  45,\n",
       "  511,\n",
       "  676,\n",
       "  12,\n",
       "  329,\n",
       "  144,\n",
       "  255,\n",
       "  325,\n",
       "  41,\n",
       "  387,\n",
       "  20,\n",
       "  541,\n",
       "  52,\n",
       "  687,\n",
       "  339,\n",
       "  423,\n",
       "  6,\n",
       "  163,\n",
       "  36,\n",
       "  263,\n",
       "  548,\n",
       "  18,\n",
       "  501,\n",
       "  424,\n",
       "  518,\n",
       "  11,\n",
       "  322,\n",
       "  687,\n",
       "  333,\n",
       "  248,\n",
       "  1,\n",
       "  39,\n",
       "  329,\n",
       "  159,\n",
       "  649,\n",
       "  41,\n",
       "  382,\n",
       "  126,\n",
       "  487,\n",
       "  13,\n",
       "  352,\n",
       "  45,\n",
       "  511,\n",
       "  13,\n",
       "  352,\n",
       "  45,\n",
       "  493,\n",
       "  190,\n",
       "  45,\n",
       "  491,\n",
       "  155,\n",
       "  12,\n",
       "  349,\n",
       "  687,\n",
       "  325,\n",
       "  1,\n",
       "  46,\n",
       "  521,\n",
       "  228,\n",
       "  329,\n",
       "  160,\n",
       "  1,\n",
       "  40,\n",
       "  352,\n",
       "  52,\n",
       "  676,\n",
       "  5,\n",
       "  147,\n",
       "  333,\n",
       "  269,\n",
       "  703,\n",
       "  2,\n",
       "  72,\n",
       "  495,\n",
       "  244,\n",
       "  41,\n",
       "  392,\n",
       "  379,\n",
       "  2,\n",
       "  55,\n",
       "  36,\n",
       "  255,\n",
       "  329,\n",
       "  160,\n",
       "  1,\n",
       "  41,\n",
       "  382,\n",
       "  126,\n",
       "  491,\n",
       "  136,\n",
       "  11,\n",
       "  305,\n",
       "  228,\n",
       "  339,\n",
       "  410,\n",
       "  10,\n",
       "  271,\n",
       "  46,\n",
       "  526,\n",
       "  360,\n",
       "  257,\n",
       "  383,\n",
       "  13,\n",
       "  356,\n",
       "  147,\n",
       "  339,\n",
       "  409,\n",
       "  133,\n",
       "  9,\n",
       "  261,\n",
       "  495,\n",
       "  262,\n",
       "  9,\n",
       "  262,\n",
       "  514,\n",
       "  29,\n",
       "  59,\n",
       "  147,\n",
       "  14,\n",
       "  393,\n",
       "  423,\n",
       "  487,\n",
       "  35,\n",
       "  1,\n",
       "  41,\n",
       "  392,\n",
       "  379,\n",
       "  29,\n",
       "  59,\n",
       "  147,\n",
       "  336,\n",
       "  329,\n",
       "  22,\n",
       "  595,\n",
       "  39,\n",
       "  329,\n",
       "  153,\n",
       "  495,\n",
       "  244,\n",
       "  5,\n",
       "  148,\n",
       "  356,\n",
       "  153,\n",
       "  505,\n",
       "  528,\n",
       "  419,\n",
       "  1,\n",
       "  31,\n",
       "  109,\n",
       "  39,\n",
       "  349,\n",
       "  689,\n",
       "  18,\n",
       "  511,\n",
       "  687,\n",
       "  329,\n",
       "  144,\n",
       "  250,\n",
       "  197,\n",
       "  5,\n",
       "  139,\n",
       "  113,\n",
       "  149,\n",
       "  5,\n",
       "  148,\n",
       "  356,\n",
       "  153,\n",
       "  505,\n",
       "  538,\n",
       "  689,\n",
       "  1,\n",
       "  41,\n",
       "  379,\n",
       "  46,\n",
       "  533,\n",
       "  541,\n",
       "  46,\n",
       "  522,\n",
       "  244,\n",
       "  11,\n",
       "  298,\n",
       "  52,\n",
       "  687,\n",
       "  325,\n",
       "  1,\n",
       "  39,\n",
       "  349,\n",
       "  694,\n",
       "  532,\n",
       "  514,\n",
       "  10,\n",
       "  291,\n",
       "  579,\n",
       "  333,\n",
       "  244,\n",
       "  41,\n",
       "  379,\n",
       "  3,\n",
       "  89,\n",
       "  217,\n",
       "  45,\n",
       "  498,\n",
       "  333,\n",
       "  248,\n",
       "  5,\n",
       "  154,\n",
       "  533,\n",
       "  548,\n",
       "  221,\n",
       "  153,\n",
       "  1,\n",
       "  45,\n",
       "  495,\n",
       "  248,\n",
       "  147,\n",
       "  3,\n",
       "  86,\n",
       "  138,\n",
       "  90,\n",
       "  255,\n",
       "  333,\n",
       "  244,\n",
       "  22,\n",
       "  595,\n",
       "  39,\n",
       "  329,\n",
       "  153,\n",
       "  495,\n",
       "  248,\n",
       "  1,\n",
       "  39,\n",
       "  333,\n",
       "  257,\n",
       "  379,\n",
       "  13,\n",
       "  366,\n",
       "  417,\n",
       "  336,\n",
       "  349,\n",
       "  18,\n",
       "  491,\n",
       "  140,\n",
       "  154,\n",
       "  518,\n",
       "  1,\n",
       "  39,\n",
       "  333,\n",
       "  268,\n",
       "  676,\n",
       "  35,\n",
       "  12,\n",
       "  333,\n",
       "  255,\n",
       "  336,\n",
       "  349,\n",
       "  16,\n",
       "  433,\n",
       "  45,\n",
       "  497,\n",
       "  302,\n",
       "  153,\n",
       "  6,\n",
       "  171,\n",
       "  257,\n",
       "  390,\n",
       "  329,\n",
       "  160,\n",
       "  13,\n",
       "  366,\n",
       "  423,\n",
       "  493,\n",
       "  190,\n",
       "  41,\n",
       "  19,\n",
       "  538,\n",
       "  679,\n",
       "  122,\n",
       "  383,\n",
       "  160,\n",
       "  10,\n",
       "  285,\n",
       "  423,\n",
       "  490,\n",
       "  133,\n",
       "  689,\n",
       "  5,\n",
       "  147,\n",
       "  339,\n",
       "  414,\n",
       "  262,\n",
       "  518,\n",
       "  20,\n",
       "  558,\n",
       "  495,\n",
       "  257,\n",
       "  387,\n",
       "  263,\n",
       "  565,\n",
       "  4,\n",
       "  ...],\n",
       " [13,\n",
       "  13,\n",
       "  1,\n",
       "  0,\n",
       "  12,\n",
       "  9,\n",
       "  22,\n",
       "  9,\n",
       "  1,\n",
       "  0,\n",
       "  22,\n",
       "  1,\n",
       "  0,\n",
       "  19,\n",
       "  1,\n",
       "  2,\n",
       "  5,\n",
       "  12,\n",
       "  12,\n",
       "  1,\n",
       "  0,\n",
       "  15,\n",
       "  16,\n",
       "  8,\n",
       "  9,\n",
       "  1,\n",
       "  0,\n",
       "  8,\n",
       "  1,\n",
       "  18,\n",
       "  12,\n",
       "  15,\n",
       "  20,\n",
       "  20,\n",
       "  5,\n",
       "  0,\n",
       "  9,\n",
       "  1,\n",
       "  0,\n",
       "  13,\n",
       "  5,\n",
       "  12,\n",
       "  9,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  18,\n",
       "  16,\n",
       "  5,\n",
       "  18,\n",
       "  0,\n",
       "  22,\n",
       "  5,\n",
       "  12,\n",
       "  25,\n",
       "  14,\n",
       "  0,\n",
       "  2,\n",
       "  9,\n",
       "  7,\n",
       "  1,\n",
       "  9,\n",
       "  12,\n",
       "  0,\n",
       "  13,\n",
       "  9,\n",
       "  12,\n",
       "  25,\n",
       "  0,\n",
       "  12,\n",
       "  9,\n",
       "  26,\n",
       "  1,\n",
       "  2,\n",
       "  5,\n",
       "  20,\n",
       "  8,\n",
       "  0,\n",
       "  9,\n",
       "  12,\n",
       "  1,\n",
       "  0,\n",
       "  12,\n",
       "  12,\n",
       "  1,\n",
       "  0,\n",
       "  22,\n",
       "  5,\n",
       "  18,\n",
       "  25,\n",
       "  0,\n",
       "  15,\n",
       "  6,\n",
       "  9,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  13,\n",
       "  9,\n",
       "  12,\n",
       "  1,\n",
       "  0,\n",
       "  18,\n",
       "  9,\n",
       "  1,\n",
       "  0,\n",
       "  3,\n",
       "  1,\n",
       "  18,\n",
       "  12,\n",
       "  5,\n",
       "  20,\n",
       "  20,\n",
       "  0,\n",
       "  9,\n",
       "  3,\n",
       "  20,\n",
       "  15,\n",
       "  18,\n",
       "  9,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  4,\n",
       "  9,\n",
       "  19,\n",
       "  15,\n",
       "  14,\n",
       "  0,\n",
       "  21,\n",
       "  14,\n",
       "  1,\n",
       "  0,\n",
       "  18,\n",
       "  1,\n",
       "  3,\n",
       "  5,\n",
       "  0,\n",
       "  8,\n",
       "  12,\n",
       "  15,\n",
       "  5,\n",
       "  0,\n",
       "  5,\n",
       "  14,\n",
       "  5,\n",
       "  12,\n",
       "  15,\n",
       "  16,\n",
       "  5,\n",
       "  0,\n",
       "  1,\n",
       "  25,\n",
       "  12,\n",
       "  1,\n",
       "  0,\n",
       "  9,\n",
       "  12,\n",
       "  5,\n",
       "  25,\n",
       "  0,\n",
       "  15,\n",
       "  5,\n",
       "  25,\n",
       "  0,\n",
       "  15,\n",
       "  18,\n",
       "  1,\n",
       "  0,\n",
       "  9,\n",
       "  12,\n",
       "  25,\n",
       "  0,\n",
       "  12,\n",
       "  5,\n",
       "  1,\n",
       "  14,\n",
       "  15,\n",
       "  18,\n",
       "  0,\n",
       "  1,\n",
       "  14,\n",
       "  14,\n",
       "  1,\n",
       "  8,\n",
       "  0,\n",
       "  9,\n",
       "  12,\n",
       "  12,\n",
       "  9,\n",
       "  1,\n",
       "  14,\n",
       "  0,\n",
       "  4,\n",
       "  4,\n",
       "  9,\n",
       "  19,\n",
       "  15,\n",
       "  14,\n",
       "  0,\n",
       "  21,\n",
       "  2,\n",
       "  18,\n",
       "  5,\n",
       "  25,\n",
       "  0,\n",
       "  12,\n",
       "  12,\n",
       "  9,\n",
       "  5,\n",
       "  0,\n",
       "  20,\n",
       "  5,\n",
       "  12,\n",
       "  12,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  20,\n",
       "  1,\n",
       "  12,\n",
       "  9,\n",
       "  5,\n",
       "  0,\n",
       "  15,\n",
       "  5,\n",
       "  0,\n",
       "  5,\n",
       "  1,\n",
       "  8,\n",
       "  0,\n",
       "  1,\n",
       "  26,\n",
       "  5,\n",
       "  12,\n",
       "  0,\n",
       "  9,\n",
       "  15,\n",
       "  12,\n",
       "  5,\n",
       "  20,\n",
       "  0,\n",
       "  21,\n",
       "  18,\n",
       "  15,\n",
       "  18,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  22,\n",
       "  1,\n",
       "  14,\n",
       "  14,\n",
       "  1,\n",
       "  8,\n",
       "  0,\n",
       "  21,\n",
       "  4,\n",
       "  18,\n",
       "  5,\n",
       "  25,\n",
       "  0,\n",
       "  18,\n",
       "  15,\n",
       "  15,\n",
       "  11,\n",
       "  12,\n",
       "  25,\n",
       "  14,\n",
       "  0,\n",
       "  5,\n",
       "  12,\n",
       "  12,\n",
       "  1,\n",
       "  0,\n",
       "  12,\n",
       "  1,\n",
       "  9,\n",
       "  18,\n",
       "  5,\n",
       "  0,\n",
       "  11,\n",
       "  25,\n",
       "  12,\n",
       "  1,\n",
       "  18,\n",
       "  0,\n",
       "  21,\n",
       "  3,\n",
       "  25,\n",
       "  0,\n",
       "  1,\n",
       "  9,\n",
       "  19,\n",
       "  12,\n",
       "  5,\n",
       "  25,\n",
       "  0,\n",
       "  22,\n",
       "  5,\n",
       "  18,\n",
       "  12,\n",
       "  25,\n",
       "  0,\n",
       "  14,\n",
       "  14,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  18,\n",
       "  15,\n",
       "  12,\n",
       "  9,\n",
       "  14,\n",
       "  5,\n",
       "  0,\n",
       "  15,\n",
       "  22,\n",
       "  1,\n",
       "  0,\n",
       "  5,\n",
       "  14,\n",
       "  5,\n",
       "  19,\n",
       "  9,\n",
       "  19,\n",
       "  0,\n",
       "  13,\n",
       "  9,\n",
       "  12,\n",
       "  9,\n",
       "  1,\n",
       "  0,\n",
       "  5,\n",
       "  14,\n",
       "  14,\n",
       "  5,\n",
       "  4,\n",
       "  25,\n",
       "  0,\n",
       "  1,\n",
       "  13,\n",
       "  1,\n",
       "  14,\n",
       "  20,\n",
       "  8,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  25,\n",
       "  1,\n",
       "  0,\n",
       "  9,\n",
       "  12,\n",
       "  12,\n",
       "  15,\n",
       "  23,\n",
       "  0,\n",
       "  9,\n",
       "  14,\n",
       "  19,\n",
       "  12,\n",
       "  5,\n",
       "  25,\n",
       "  0,\n",
       "  1,\n",
       "  15,\n",
       "  13,\n",
       "  9,\n",
       "  0,\n",
       "  1,\n",
       "  12,\n",
       "  9,\n",
       "  25,\n",
       "  1,\n",
       "  8,\n",
       "  0,\n",
       "  12,\n",
       "  5,\n",
       "  14,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  18,\n",
       "  1,\n",
       "  8,\n",
       "  0,\n",
       "  18,\n",
       "  9,\n",
       "  1,\n",
       "  14,\n",
       "  1,\n",
       "  0,\n",
       "  12,\n",
       "  12,\n",
       "  9,\n",
       "  19,\n",
       "  15,\n",
       "  14,\n",
       "  0,\n",
       "  1,\n",
       "  2,\n",
       "  18,\n",
       "  9,\n",
       "  5,\n",
       "  12,\n",
       "  12,\n",
       "  1,\n",
       "  0,\n",
       "  12,\n",
       "  9,\n",
       "  3,\n",
       "  5,\n",
       "  0,\n",
       "  1,\n",
       "  4,\n",
       "  5,\n",
       "  12,\n",
       "  25,\n",
       "  14,\n",
       "  0,\n",
       "  15,\n",
       "  18,\n",
       "  1,\n",
       "  0,\n",
       "  21,\n",
       "  2,\n",
       "  25,\n",
       "  0,\n",
       "  22,\n",
       "  1,\n",
       "  0,\n",
       "  5,\n",
       "  18,\n",
       "  5,\n",
       "  14,\n",
       "  9,\n",
       "  20,\n",
       "  25,\n",
       "  0,\n",
       "  21,\n",
       "  20,\n",
       "  21,\n",
       "  13,\n",
       "  14,\n",
       "  0,\n",
       "  4,\n",
       "  5,\n",
       "  12,\n",
       "  9,\n",
       "  14,\n",
       "  5,\n",
       "  0,\n",
       "  1,\n",
       "  9,\n",
       "  12,\n",
       "  5,\n",
       "  25,\n",
       "  0,\n",
       "  9,\n",
       "  1,\n",
       "  14,\n",
       "  14,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  12,\n",
       "  5,\n",
       "  14,\n",
       "  20,\n",
       "  9,\n",
       "  14,\n",
       "  1,\n",
       "  0,\n",
       "  19,\n",
       "  12,\n",
       "  1,\n",
       "  0,\n",
       "  12,\n",
       "  9,\n",
       "  1,\n",
       "  14,\n",
       "  1,\n",
       "  0,\n",
       "  21,\n",
       "  9,\n",
       "  14,\n",
       "  14,\n",
       "  0,\n",
       "  5,\n",
       "  22,\n",
       "  1,\n",
       "  5,\n",
       "  8,\n",
       "  0,\n",
       "  22,\n",
       "  25,\n",
       "  0,\n",
       "  1,\n",
       "  4,\n",
       "  9,\n",
       "  5,\n",
       "  0,\n",
       "  9,\n",
       "  16,\n",
       "  5,\n",
       "  18,\n",
       "  0,\n",
       "  25,\n",
       "  4,\n",
       "  9,\n",
       "  1,\n",
       "  0,\n",
       "  12,\n",
       "  5,\n",
       "  24,\n",
       "  1,\n",
       "  0,\n",
       "  15,\n",
       "  19,\n",
       "  5,\n",
       "  16,\n",
       "  8,\n",
       "  9,\n",
       "  14,\n",
       "  5,\n",
       "  0,\n",
       "  13,\n",
       "  5,\n",
       "  18,\n",
       "  25,\n",
       "  0,\n",
       "  21,\n",
       "  12,\n",
       "  9,\n",
       "  1,\n",
       "  0,\n",
       "  5,\n",
       "  12,\n",
       "  9,\n",
       "  12,\n",
       "  1,\n",
       "  8,\n",
       "  0,\n",
       "  18,\n",
       "  9,\n",
       "  1,\n",
       "  14,\n",
       "  14,\n",
       "  1,\n",
       "  0,\n",
       "  9,\n",
       "  22,\n",
       "  9,\n",
       "  1,\n",
       "  14,\n",
       "  0,\n",
       "  1,\n",
       "  25,\n",
       "  12,\n",
       "  5,\n",
       "  5,\n",
       "  0,\n",
       "  15,\n",
       "  16,\n",
       "  8,\n",
       "  9,\n",
       "  5,\n",
       "  0,\n",
       "  18,\n",
       "  9,\n",
       "  5,\n",
       "  12,\n",
       "  12,\n",
       "  5,\n",
       "  0,\n",
       "  1,\n",
       "  4,\n",
       "  5,\n",
       "  12,\n",
       "  9,\n",
       "  14,\n",
       "  5,\n",
       "  0,\n",
       "  5,\n",
       "  25,\n",
       "  20,\n",
       "  15,\n",
       "  14,\n",
       "  0,\n",
       "  25,\n",
       "  12,\n",
       "  5,\n",
       "  5,\n",
       "  0,\n",
       "  12,\n",
       "  1,\n",
       "  18,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  4,\n",
       "  12,\n",
       "  5,\n",
       "  25,\n",
       "  0,\n",
       "  5,\n",
       "  12,\n",
       "  1,\n",
       "  14,\n",
       "  9,\n",
       "  5,\n",
       "  0,\n",
       "  1,\n",
       "  3,\n",
       "  11,\n",
       "  5,\n",
       "  14,\n",
       "  26,\n",
       "  9,\n",
       "  5,\n",
       "  0,\n",
       "  5,\n",
       "  1,\n",
       "  7,\n",
       "  1,\n",
       "  14,\n",
       "  0,\n",
       "  4,\n",
       "  1,\n",
       "  12,\n",
       "  25,\n",
       "  14,\n",
       "  14,\n",
       "  0,\n",
       "  9,\n",
       "  12,\n",
       "  9,\n",
       "  1,\n",
       "  14,\n",
       "  1,\n",
       "  0,\n",
       "  21,\n",
       "  2,\n",
       "  18,\n",
       "  5,\n",
       "  5,\n",
       "  0,\n",
       "  1,\n",
       "  4,\n",
       "  5,\n",
       "  0,\n",
       "  1,\n",
       "  20,\n",
       "  8,\n",
       "  5,\n",
       "  18,\n",
       "  9,\n",
       "  14,\n",
       "  5,\n",
       "  0,\n",
       "  19,\n",
       "  1,\n",
       "  2,\n",
       "  5,\n",
       "  12,\n",
       "  12,\n",
       "  5,\n",
       "  0,\n",
       "  1,\n",
       "  20,\n",
       "  1,\n",
       "  12,\n",
       "  9,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  5,\n",
       "  12,\n",
       "  25,\n",
       "  14,\n",
       "  14,\n",
       "  0,\n",
       "  1,\n",
       "  18,\n",
       "  9,\n",
       "  1,\n",
       "  0,\n",
       "  20,\n",
       "  8,\n",
       "  5,\n",
       "  14,\n",
       "  1,\n",
       "  0,\n",
       "  9,\n",
       "  13,\n",
       "  5,\n",
       "  14,\n",
       "  1,\n",
       "  0,\n",
       "  18,\n",
       "  25,\n",
       "  1,\n",
       "  0,\n",
       "  5,\n",
       "  9,\n",
       "  12,\n",
       "  1,\n",
       "  14,\n",
       "  9,\n",
       "  0,\n",
       "  1,\n",
       "  25,\n",
       "  12,\n",
       "  15,\n",
       "  18,\n",
       "  0,\n",
       "  1,\n",
       "  9,\n",
       "  20,\n",
       "  8,\n",
       "  0,\n",
       "  15,\n",
       "  19,\n",
       "  5,\n",
       "  0,\n",
       "  25,\n",
       "  12,\n",
       "  9,\n",
       "  5,\n",
       "  0,\n",
       "  12,\n",
       "  5,\n",
       "  24,\n",
       "  1,\n",
       "  14,\n",
       "  4,\n",
       "  18,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  18,\n",
       "  25,\n",
       "  0,\n",
       "  1,\n",
       "  18,\n",
       "  7,\n",
       "  1,\n",
       "  18,\n",
       "  5,\n",
       "  20,\n",
       "  0,\n",
       "  25,\n",
       "  12,\n",
       "  1,\n",
       "  0,\n",
       "  19,\n",
       "  8,\n",
       "  12,\n",
       "  5,\n",
       "  25,\n",
       "  0,\n",
       "  13,\n",
       "  1,\n",
       "  25,\n",
       "  1,\n",
       "  0,\n",
       "  12,\n",
       "  9,\n",
       "  26,\n",
       "  1,\n",
       "  0,\n",
       "  18,\n",
       "  9,\n",
       "  1,\n",
       "  14,\n",
       "  14,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  9,\n",
       "  12,\n",
       "  5,\n",
       "  25,\n",
       "  0,\n",
       "  14,\n",
       "  4,\n",
       "  18,\n",
       "  5,\n",
       "  1,\n",
       "  0,\n",
       "  8,\n",
       "  12,\n",
       "  15,\n",
       "  5,\n",
       "  0,\n",
       "  1,\n",
       "  19,\n",
       "  13,\n",
       "  9,\n",
       "  14,\n",
       "  5,\n",
       "  0,\n",
       "  5,\n",
       "  12,\n",
       "  15,\n",
       "  4,\n",
       "  25,\n",
       "  0,\n",
       "  18,\n",
       "  9,\n",
       "  19,\n",
       "  0,\n",
       "  19,\n",
       "  1,\n",
       "  2,\n",
       "  5,\n",
       "  12,\n",
       "  0,\n",
       "  15,\n",
       "  18,\n",
       "  1,\n",
       "  8,\n",
       "  0,\n",
       "  14,\n",
       "  14,\n",
       "  1,\n",
       "  2,\n",
       "  5,\n",
       "  12,\n",
       "  12,\n",
       "  5,\n",
       "  0,\n",
       "  1,\n",
       "  12,\n",
       "  5,\n",
       "  18,\n",
       "  9,\n",
       "  1,\n",
       "  0,\n",
       "  13,\n",
       "  5,\n",
       "  18,\n",
       "  19,\n",
       "  15,\n",
       "  14,\n",
       "  0,\n",
       "  4,\n",
       "  1,\n",
       "  12,\n",
       "  25,\n",
       "  14,\n",
       "  0,\n",
       "  25,\n",
       "  12,\n",
       "  5,\n",
       "  9,\n",
       "  7,\n",
       "  8,\n",
       "  0,\n",
       "  4,\n",
       "  5,\n",
       "  14,\n",
       "  0,\n",
       "  13,\n",
       "  5,\n",
       "  18,\n",
       "  19,\n",
       "  25,\n",
       "  14,\n",
       "  0,\n",
       "  14,\n",
       "  1,\n",
       "  19,\n",
       "  20,\n",
       "  1,\n",
       "  19,\n",
       "  9,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  25,\n",
       "  12,\n",
       "  1,\n",
       "  0,\n",
       "  12,\n",
       "  25,\n",
       "  19,\n",
       "  19,\n",
       "  1,\n",
       "  0,\n",
       "  21,\n",
       "  12,\n",
       "  9,\n",
       "  1,\n",
       "  14,\n",
       "  1,\n",
       "  0,\n",
       "  8,\n",
       "  1,\n",
       "  18,\n",
       "  12,\n",
       "  9,\n",
       "  5,\n",
       "  0,\n",
       "  19,\n",
       "  20,\n",
       "  8,\n",
       "  5,\n",
       "  18,\n",
       "  0,\n",
       "  18,\n",
       "  9,\n",
       "  5,\n",
       "  12,\n",
       "  0,\n",
       "  5,\n",
       "  3,\n",
       "  9,\n",
       "  12,\n",
       "  9,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  12,\n",
       "  5,\n",
       "  18,\n",
       "  9,\n",
       "  5,\n",
       "  0,\n",
       "  12,\n",
       "  9,\n",
       "  14,\n",
       "  1,\n",
       "  0,\n",
       "  15,\n",
       "  12,\n",
       "  12,\n",
       "  25,\n",
       "  0,\n",
       "  5,\n",
       "  5,\n",
       "  19,\n",
       "  5,\n",
       "  0,\n",
       "  12,\n",
       "  9,\n",
       "  25,\n",
       "  1,\n",
       "  8,\n",
       "  0,\n",
       "  9,\n",
       "  12,\n",
       "  12,\n",
       "  25,\n",
       "  0,\n",
       "  1,\n",
       "  18,\n",
       "  11,\n",
       "  5,\n",
       "  18,\n",
       "  0,\n",
       "  9,\n",
       "  14,\n",
       "  12,\n",
       "  5,\n",
       "  25,\n",
       "  0,\n",
       "  15,\n",
       "  18,\n",
       "  7,\n",
       "  1,\n",
       "  14,\n",
       "  0,\n",
       "  25,\n",
       "  4,\n",
       "  14,\n",
       "  5,\n",
       "  25,\n",
       "  0,\n",
       "  15,\n",
       "  18,\n",
       "  4,\n",
       "  25,\n",
       "  14,\n",
       "  0,\n",
       "  12,\n",
       "  15,\n",
       "  9,\n",
       "  19,\n",
       "  5,\n",
       "  0,\n",
       "  18,\n",
       "  9,\n",
       "  14,\n",
       "  9,\n",
       "  20,\n",
       "  25,\n",
       "  0,\n",
       "  1,\n",
       "  ...])"
      ]
     },
     "execution_count": 424,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs = []\n",
    "ys = []\n",
    "for w in words:\n",
    "    wdot = f'.{w}.'\n",
    "    for i in range(len(wdot) - 2):\n",
    "        pair = wdot[i] + wdot[i+1]\n",
    "        xs.append(ptoi[pair])\n",
    "        ys.append(stoi[wdot[i+2]])\n",
    "\n",
    "xs, ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = torch.tensor(xs).float()\n",
    "ys = torch.tensor(ys).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.3091, grad_fn=<DivBackward1>)\n",
      "tensor(3.1411, grad_fn=<DivBackward1>)\n",
      "tensor(3.0107, grad_fn=<DivBackward1>)\n",
      "tensor(2.9063, grad_fn=<DivBackward1>)\n",
      "tensor(2.8301, grad_fn=<DivBackward1>)\n",
      "tensor(2.7835, grad_fn=<DivBackward1>)\n",
      "tensor(2.7566, grad_fn=<DivBackward1>)\n",
      "tensor(2.7404, grad_fn=<DivBackward1>)\n",
      "tensor(2.7300, grad_fn=<DivBackward1>)\n",
      "tensor(2.7231, grad_fn=<DivBackward1>)\n",
      "tensor(2.7183, grad_fn=<DivBackward1>)\n",
      "tensor(2.7148, grad_fn=<DivBackward1>)\n",
      "tensor(2.7121, grad_fn=<DivBackward1>)\n",
      "tensor(2.7101, grad_fn=<DivBackward1>)\n",
      "tensor(2.7084, grad_fn=<DivBackward1>)\n",
      "tensor(2.7071, grad_fn=<DivBackward1>)\n",
      "tensor(2.7059, grad_fn=<DivBackward1>)\n",
      "tensor(2.7050, grad_fn=<DivBackward1>)\n",
      "tensor(2.7041, grad_fn=<DivBackward1>)\n",
      "tensor(2.7034, grad_fn=<DivBackward1>)\n",
      "tensor(2.7027, grad_fn=<DivBackward1>)\n",
      "tensor(2.7021, grad_fn=<DivBackward1>)\n",
      "tensor(2.7015, grad_fn=<DivBackward1>)\n",
      "tensor(2.7010, grad_fn=<DivBackward1>)\n",
      "tensor(2.7004, grad_fn=<DivBackward1>)\n",
      "tensor(2.6999, grad_fn=<DivBackward1>)\n",
      "tensor(2.6994, grad_fn=<DivBackward1>)\n",
      "tensor(2.6990, grad_fn=<DivBackward1>)\n",
      "tensor(2.6985, grad_fn=<DivBackward1>)\n",
      "tensor(2.6981, grad_fn=<DivBackward1>)\n",
      "tensor(2.6976, grad_fn=<DivBackward1>)\n",
      "tensor(2.6972, grad_fn=<DivBackward1>)\n",
      "tensor(2.6967, grad_fn=<DivBackward1>)\n",
      "tensor(2.6965, grad_fn=<DivBackward1>)\n",
      "tensor(2.6961, grad_fn=<DivBackward1>)\n",
      "tensor(2.6962, grad_fn=<DivBackward1>)\n",
      "tensor(2.6964, grad_fn=<DivBackward1>)\n",
      "tensor(2.6986, grad_fn=<DivBackward1>)\n",
      "tensor(2.7017, grad_fn=<DivBackward1>)\n",
      "tensor(2.7132, grad_fn=<DivBackward1>)\n",
      "tensor(2.7232, grad_fn=<DivBackward1>)\n",
      "tensor(2.7536, grad_fn=<DivBackward1>)\n",
      "tensor(2.7388, grad_fn=<DivBackward1>)\n",
      "tensor(2.7424, grad_fn=<DivBackward1>)\n",
      "tensor(2.7180, grad_fn=<DivBackward1>)\n",
      "tensor(2.7164, grad_fn=<DivBackward1>)\n",
      "tensor(2.7082, grad_fn=<DivBackward1>)\n",
      "tensor(2.7103, grad_fn=<DivBackward1>)\n",
      "tensor(2.7050, grad_fn=<DivBackward1>)\n",
      "tensor(2.7085, grad_fn=<DivBackward1>)\n",
      "tensor(2.7041, grad_fn=<DivBackward1>)\n",
      "tensor(2.7087, grad_fn=<DivBackward1>)\n",
      "tensor(2.7043, grad_fn=<DivBackward1>)\n",
      "tensor(2.7099, grad_fn=<DivBackward1>)\n",
      "tensor(2.7048, grad_fn=<DivBackward1>)\n",
      "tensor(2.7108, grad_fn=<DivBackward1>)\n",
      "tensor(2.7049, grad_fn=<DivBackward1>)\n",
      "tensor(2.7110, grad_fn=<DivBackward1>)\n",
      "tensor(2.7044, grad_fn=<DivBackward1>)\n",
      "tensor(2.7104, grad_fn=<DivBackward1>)\n",
      "tensor(2.7035, grad_fn=<DivBackward1>)\n",
      "tensor(2.7095, grad_fn=<DivBackward1>)\n",
      "tensor(2.7027, grad_fn=<DivBackward1>)\n",
      "tensor(2.7087, grad_fn=<DivBackward1>)\n",
      "tensor(2.7018, grad_fn=<DivBackward1>)\n",
      "tensor(2.7080, grad_fn=<DivBackward1>)\n",
      "tensor(2.7013, grad_fn=<DivBackward1>)\n",
      "tensor(2.7078, grad_fn=<DivBackward1>)\n",
      "tensor(2.7007, grad_fn=<DivBackward1>)\n",
      "tensor(2.7075, grad_fn=<DivBackward1>)\n",
      "tensor(2.7003, grad_fn=<DivBackward1>)\n",
      "tensor(2.7071, grad_fn=<DivBackward1>)\n",
      "tensor(2.6997, grad_fn=<DivBackward1>)\n",
      "tensor(2.7066, grad_fn=<DivBackward1>)\n",
      "tensor(2.6992, grad_fn=<DivBackward1>)\n",
      "tensor(2.7063, grad_fn=<DivBackward1>)\n",
      "tensor(2.6984, grad_fn=<DivBackward1>)\n",
      "tensor(2.7056, grad_fn=<DivBackward1>)\n",
      "tensor(2.6980, grad_fn=<DivBackward1>)\n",
      "tensor(2.7054, grad_fn=<DivBackward1>)\n",
      "tensor(2.6974, grad_fn=<DivBackward1>)\n",
      "tensor(2.7050, grad_fn=<DivBackward1>)\n",
      "tensor(2.6969, grad_fn=<DivBackward1>)\n",
      "tensor(2.7045, grad_fn=<DivBackward1>)\n",
      "tensor(2.6963, grad_fn=<DivBackward1>)\n",
      "tensor(2.7039, grad_fn=<DivBackward1>)\n",
      "tensor(2.6958, grad_fn=<DivBackward1>)\n",
      "tensor(2.7035, grad_fn=<DivBackward1>)\n",
      "tensor(2.6951, grad_fn=<DivBackward1>)\n",
      "tensor(2.7028, grad_fn=<DivBackward1>)\n",
      "tensor(2.6946, grad_fn=<DivBackward1>)\n",
      "tensor(2.7026, grad_fn=<DivBackward1>)\n",
      "tensor(2.6941, grad_fn=<DivBackward1>)\n",
      "tensor(2.7019, grad_fn=<DivBackward1>)\n",
      "tensor(2.6936, grad_fn=<DivBackward1>)\n",
      "tensor(2.7015, grad_fn=<DivBackward1>)\n",
      "tensor(2.6930, grad_fn=<DivBackward1>)\n",
      "tensor(2.7010, grad_fn=<DivBackward1>)\n",
      "tensor(2.6925, grad_fn=<DivBackward1>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[437], line 32\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m     31\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m---> 32\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m        \u001b[49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mone_hot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint64\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_pairs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m        \u001b[49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mone_hot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint64\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredictable_chars\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28mprint\u001b[39m(loss)\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;66;03m# for batch in dls.train:\u001b[39;00m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;66;03m#     xb, yb = batch\u001b[39;00m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;66;03m#     xb = to_bw_flattened(xb)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     42\u001b[0m \n\u001b[1;32m     43\u001b[0m     \u001b[38;5;66;03m# print(f\"Avg loss: {tot_loss / valid_batches}, Avg acc: {tot_acc / valid_batches}\")\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[437], line 17\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(xb, yb)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_step\u001b[39m(xb, yb):\n\u001b[0;32m---> 17\u001b[0m     preds \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_fn(preds, yb)\n\u001b[1;32m     20\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/mambaforge-pypy3/envs/ai-base/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge-pypy3/envs/ai-base/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge-pypy3/envs/ai-base/lib/python3.11/site-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/mambaforge-pypy3/envs/ai-base/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge-pypy3/envs/ai-base/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge-pypy3/envs/ai-base/lib/python3.11/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hidden_neurons = 100\n",
    "predictable_chars = len(letters) + 1\n",
    "total_pairs = len(ptoi)\n",
    "\n",
    "epochs = 100\n",
    "lr = 3\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(total_pairs, hidden_neurons),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(hidden_neurons, predictable_chars),\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr, weight_decay=0.01)\n",
    "\n",
    "def train_step(xb, yb):\n",
    "    preds = model(xb)\n",
    "    loss = loss_fn(preds, yb)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    return loss\n",
    "\n",
    "# def valid_step(xb):\n",
    "#     preds = model(xb)\n",
    "#     loss = loss_fn(preds)\n",
    "#     return loss\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    loss = train_step(\n",
    "        F.one_hot(xs.to(torch.int64), total_pairs).float(), \n",
    "        F.one_hot(ys.to(torch.int64), predictable_chars).float()\n",
    "    )\n",
    "    print(loss)\n",
    "    # for batch in dls.train:\n",
    "    #     xb, yb = batch\n",
    "    #     xb = to_bw_flattened(xb)\n",
    "    #     xb, yb = to_torch_tensor(xb, yb)\n",
    "    #     loss, acc = train_step(xb, yb)\n",
    "\n",
    "    # print(f\"Avg loss: {tot_loss / valid_batches}, Avg acc: {tot_acc / valid_batches}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.a',\n",
       " '.b',\n",
       " '.c',\n",
       " '.d',\n",
       " '.e',\n",
       " '.f',\n",
       " '.g',\n",
       " '.h',\n",
       " '.i',\n",
       " '.j',\n",
       " '.k',\n",
       " '.l',\n",
       " '.m',\n",
       " '.n',\n",
       " '.o',\n",
       " '.p',\n",
       " '.q',\n",
       " '.r',\n",
       " '.s',\n",
       " '.t',\n",
       " '.u',\n",
       " '.v',\n",
       " '.w',\n",
       " '.x',\n",
       " '.y',\n",
       " '.z']"
      ]
     },
     "execution_count": 421,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "starters = [p for p in pairs if p[0] == '.' and p != '..']\n",
    "starters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xeeoimoavnbask\n",
      "kl\n",
      "bwa\n",
      "hahihgalf\n",
      "badjapnilen\n",
      "gaaitnaue\n",
      "vuhyiarngya\n",
      "wdainadiabturs\n",
      "kslmyianeauhla\n",
      "msnvoinennerga\n",
      "keahmceavaptay\n",
      "cyxoeseiarasan\n",
      "fcllua\n",
      "kreeanneqneaaa\n",
      "ee\n",
      "heai\n",
      "llayzaf\n",
      "mrylaxlnonlt\n",
      "eeeayro\n",
      "keaelnnha\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "words_to_gen = 20\n",
    "for wi in range(words_to_gen):\n",
    "    # word_len = torch.randint(min(word_lens), max(word_lens), (1,)).item()\n",
    "    # for i in range(int(word_len)):\n",
    "    li = 0\n",
    "    next_letter = ''\n",
    "    # e.g. \".a\"\n",
    "    genned_word = starters[random.randint(0, len(starters) - 1)]\n",
    "    while next_letter != '.' and len(genned_word) < max(word_lens):\n",
    "        prior_pair = genned_word[li] + genned_word[li+1]\n",
    "        prior_pair_i = ptoi[prior_pair]\n",
    "        prior_pair_i_one_hot = F.one_hot(\n",
    "            torch.tensor(prior_pair_i), total_pairs\n",
    "        ).float()\n",
    "\n",
    "        probs = torch.nn.Softmax()(model(prior_pair_i_one_hot))\n",
    "        pred = torch.multinomial(probs, 1, replacement=True).item()\n",
    "        next_letter = itos[int(pred)]\n",
    "\n",
    "        # next_sample_p = P[prior_letter_i]\n",
    "        # next_letter_idx = torch.multinomial(\n",
    "        #         next_sample_p, 1, replacement=True, generator=g\n",
    "        #     ).item()\n",
    "        # next_letter = itos[next_letter_idx]\n",
    "        genned_word += next_letter\n",
    "        li += 1\n",
    "    print(genned_word.strip('.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Test Split\n",
    "\n",
    "E02: split up the dataset randomly into 80% train set, 10% dev set, 10% test set. Train the bigram and trigram models only on the training set. Evaluate them on dev and test splits. What can you see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordsDataset(Dataset):\n",
    "    def __init__(self, words_file_dir):\n",
    "        self.words_file_dir = words_file_dir\n",
    "        self.words = open(words_file_dir).read().splitlines()\n",
    "        \n",
    "        xs = []\n",
    "        ys = []\n",
    "        for w in self.words:\n",
    "            wdot = f'.{w}.'\n",
    "            for i in range(len(wdot) - 2):\n",
    "                pair = wdot[i] + wdot[i+1]\n",
    "                xs.append(ptoi[pair])\n",
    "                ys.append(stoi[wdot[i+2]])\n",
    "\n",
    "        self.xs = torch.tensor(xs)\n",
    "        self.ys = torch.tensor(ys)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.xs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # return F.one_hot(self.xs[idx], total_pairs).float(), F.one_hot(self.ys[idx], predictable_chars).float()\n",
    "        return F.one_hot(self.xs[idx], total_pairs).float(), self.ys[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(156891, 19611, 19611)"
      ]
     },
     "execution_count": 531,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_data = WordsDataset('names.txt')\n",
    "train_data, dev_data, test_data = torch.utils.data.random_split(\n",
    "    full_data, [0.8, 0.1, 0.1]\n",
    ")\n",
    "len(train_data), len(dev_data), len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_data, batch_size=256, shuffle=True)\n",
    "dev_dl = DataLoader(dev_data, batch_size=256, shuffle=True)\n",
    "test_dl = DataLoader(test_data, batch_size=256, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1; Train loss: 2.478865760380059; Valid loss: 2.3297139112051433\n",
      "Epoch: 2; Train loss: 2.240069600147291; Valid loss: 2.2234733507230686\n",
      "Epoch: 3; Train loss: 2.1854187545729693; Valid loss: 2.195639046755704\n",
      "Epoch: 4; Train loss: 2.1607985934075487; Valid loss: 2.176535850995547\n",
      "Epoch: 5; Train loss: 2.146448504088559; Valid loss: 2.1770438095191857\n",
      "Epoch: 6; Train loss: 2.1334736592228998; Valid loss: 2.1644874176421722\n",
      "Epoch: 7; Train loss: 2.126377756599501; Valid loss: 2.158960958579918\n",
      "Epoch: 8; Train loss: 2.11990780931507; Valid loss: 2.152849623135158\n",
      "Epoch: 9; Train loss: 2.115403890609741; Valid loss: 2.1530039852315728\n",
      "Epoch: 10; Train loss: 2.112162614335438; Valid loss: 2.163072694431652\n",
      "Epoch: 11; Train loss: 2.108507923745409; Valid loss: 2.1507596567079617\n",
      "Epoch: 12; Train loss: 2.1063981013430273; Valid loss: 2.164969112965968\n",
      "Epoch: 13; Train loss: 2.1039171353055446; Valid loss: 2.1443202077568353\n",
      "Epoch: 14; Train loss: 2.102816231682483; Valid loss: 2.143884626301852\n",
      "Epoch: 15; Train loss: 2.0999715205701297; Valid loss: 2.1414537878779623\n",
      "Epoch: 16; Train loss: 2.0992738691084147; Valid loss: 2.138731772249395\n",
      "Epoch: 17; Train loss: 2.097322661374949; Valid loss: 2.1471016190268775\n",
      "Epoch: 18; Train loss: 2.096865316977509; Valid loss: 2.1535281159661035\n",
      "Epoch: 19; Train loss: 2.0952138278457118; Valid loss: 2.1389693207555003\n",
      "Epoch: 20; Train loss: 2.09471922928124; Valid loss: 2.1490364059225304\n",
      "Test loss: 2.1206584549569465\n"
     ]
    }
   ],
   "source": [
    "hidden_neurons = 100\n",
    "predictable_chars = len(letters) + 1\n",
    "total_pairs = len(ptoi)\n",
    "\n",
    "epochs = 20\n",
    "lr = 0.9\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(total_pairs, hidden_neurons),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(hidden_neurons, predictable_chars),\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr, weight_decay=0.00001)\n",
    "\n",
    "def train_step(xb, yb):\n",
    "    preds = model(xb)\n",
    "    loss = loss_fn(preds, yb)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    return loss\n",
    "\n",
    "def valid_step(xb, yb):\n",
    "    preds = model(xb)\n",
    "    loss = loss_fn(preds, yb)\n",
    "    return loss\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    tot_train_loss = 0\n",
    "    for batch in train_dl:\n",
    "        xb, yb = batch\n",
    "        loss = train_step(xb, yb)\n",
    "        tot_train_loss += loss.item()\n",
    "\n",
    "    model.eval()\n",
    "    tot_dev_loss = 0\n",
    "    with torch.inference_mode():\n",
    "        for batch in dev_dl:\n",
    "            xb, yb = batch\n",
    "            loss = valid_step(xb, yb)\n",
    "            tot_dev_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch: {epoch+1}; Train loss: {tot_train_loss / len(train_dl)}; Valid loss: {tot_dev_loss / len(dev_dl)}\")\n",
    "\n",
    "model.eval()\n",
    "tot_test_loss = 0\n",
    "with torch.inference_mode():\n",
    "    for batch in test_dl:\n",
    "        xb, yb = batch\n",
    "        loss = valid_step(xb, yb)\n",
    "        tot_test_loss += loss.item()\n",
    "\n",
    "print(f\"Test loss: {tot_test_loss / len(test_dl)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test loss with 10 epochs:\n",
    "* lr 0.5 and weight_decay 0.01 = 2.58\n",
    "* lr 0.5 and weight_decay 0.05 = 2.76 (basically didn't train)\n",
    "* lr 0.5 and weight_decay 0.005 = 2.44 (way better)\n",
    "* lr 0.5 and weight_decay 0.001 = 2.16 (still improving)\n",
    "* lr 0.5 and weight_decay 0.0001 = 2.15 (marginal gain)\n",
    "* lr 0.5 and weight_decay 0.00001 = 2.13\n",
    "* lr 0.5 and weight_decay 0.000001 = 2.13 (sticking with prior)\n",
    "* lr 0.9 and weight_decay 0.00001 = 2.12 **best**\n",
    "* lr 1.5 and weight_decay 0.00001 = 2.135 (starting to overfit? train was still going down)\n",
    "* lr 1.5 and weight_decay 0.0001 = 2.130 (lower lr was better)\n",
    "\n",
    "Test loss with 20 epochs at best settings:\n",
    "2.12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ter\n",
      "maxona\n",
      "riazah\n",
      "gus\n",
      "ka\n",
      "jaxlesh\n",
      "arucrohermi\n",
      "wa\n",
      "bra\n",
      "farielyn\n",
      "dee\n",
      "lukand\n",
      "eli\n",
      "kyn\n",
      "nasi\n",
      "jabenn\n",
      "bramarsyra\n",
      "araemai\n",
      "kia\n",
      "wilayzriyah\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "words_to_gen = 20\n",
    "for wi in range(words_to_gen):\n",
    "    li = 0\n",
    "    next_letter = ''\n",
    "    # e.g. \".a\"\n",
    "    genned_word = starters[random.randint(0, len(starters) - 1)]\n",
    "    while next_letter != '.' and len(genned_word) < max(word_lens):\n",
    "        prior_pair = genned_word[li] + genned_word[li+1]\n",
    "        prior_pair_i = ptoi[prior_pair]\n",
    "        prior_pair_i_one_hot = F.one_hot(\n",
    "            torch.tensor(prior_pair_i), total_pairs\n",
    "        ).float()\n",
    "\n",
    "        probs = torch.nn.Softmax()(model(prior_pair_i_one_hot))\n",
    "        pred = torch.multinomial(probs, 1, replacement=True).item()\n",
    "        next_letter = itos[int(pred)]\n",
    "\n",
    "        genned_word += next_letter\n",
    "        li += 1\n",
    "    print(genned_word.strip('.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2\n",
    "\n",
    "- Rebuild training set to take 3 letter inputs and predict next letter\n",
    "    - Build using raw pytorch without their model (i.e. do own matrix multiplies, initialize own weights, etc make sure to call requires_grad=True)\n",
    "    - Note, he includes \"...\", \"..a\", etc., so \".\" can occur more than once\n",
    "    - Can I improve the model without these? Try after the rest is done.\n",
    "- Create an embedding lookup table (he started with 2 dimensional embedding, one for each of the 27 characters, so was able to plot them before increasing embedding size)\n",
    "- Create the full model based on the paper\n",
    "    - He used cross entropy loss\n",
    "    - Use mini batches (he started with batches of 32, and instead of partitioning them out evenly, just shuffled via torch.randint)\n",
    "- Determine best learning rate (create pool of lrs from 0-1000 with torch.linspace, run training from lowest to highest training via batches, plot learn rate vs loss, best LR around where loss stops decreasing, and definitely before it explodes/starts increasing or being unstable)\n",
    "- After training stops/slows down at optimal learning rate, try some learning rate decay to learn some more (decrease lr by factor of 10)\n",
    "- Split into train/dev/test since we'll be tuning hyperparameters\n",
    "- Once we've optimized our model on learning rate, we can try increasing the hidden layer size (he used 300) and train again, then try increasing embedding dimensions\n",
    "- Try to beat his dev loss of 2.17 (or my prior one of 2.12 with the prior model)\n",
    "- Read Paper Bengio et al. 2003 (MLP langauge model)\n",
    "\n",
    "### Exercises\n",
    "\n",
    "E01: Tune the hyperparameters of the training to beat my best validation loss of 2.2\n",
    "E02: I was not careful with the intialization of the network in this video. (1) What is the loss you'd get if the predicted probabilities at initialization were perfectly uniform? What loss do we achieve? (2) Can you tune the initialization to get a starting loss that is much more similar to (1)?\n",
    "E03: Read the Bengio et al 2003 paper (link above), implement and try any idea from the paper. Did it work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "letters + ['.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'...': 0,\n",
       " '..a': 1,\n",
       " '..b': 2,\n",
       " '..c': 3,\n",
       " '..d': 4,\n",
       " '..e': 5,\n",
       " '..f': 6,\n",
       " '..g': 7,\n",
       " '..h': 8,\n",
       " '..i': 9,\n",
       " '..j': 10,\n",
       " '..k': 11,\n",
       " '..l': 12,\n",
       " '..m': 13,\n",
       " '..n': 14,\n",
       " '..o': 15,\n",
       " '..p': 16,\n",
       " '..q': 17,\n",
       " '..r': 18,\n",
       " '..s': 19,\n",
       " '..t': 20,\n",
       " '..u': 21,\n",
       " '..v': 22,\n",
       " '..w': 23,\n",
       " '..x': 24,\n",
       " '..y': 25,\n",
       " '..z': 26,\n",
       " '.a.': 27,\n",
       " '.aa': 28,\n",
       " '.ab': 29,\n",
       " '.ac': 30,\n",
       " '.ad': 31,\n",
       " '.ae': 32,\n",
       " '.af': 33,\n",
       " '.ag': 34,\n",
       " '.ah': 35,\n",
       " '.ai': 36,\n",
       " '.aj': 37,\n",
       " '.ak': 38,\n",
       " '.al': 39,\n",
       " '.am': 40,\n",
       " '.an': 41,\n",
       " '.ao': 42,\n",
       " '.ap': 43,\n",
       " '.aq': 44,\n",
       " '.ar': 45,\n",
       " '.as': 46,\n",
       " '.at': 47,\n",
       " '.au': 48,\n",
       " '.av': 49,\n",
       " '.aw': 50,\n",
       " '.ax': 51,\n",
       " '.ay': 52,\n",
       " '.az': 53,\n",
       " '.b.': 54,\n",
       " '.ba': 55,\n",
       " '.bb': 56,\n",
       " '.bc': 57,\n",
       " '.bd': 58,\n",
       " '.be': 59,\n",
       " '.bf': 60,\n",
       " '.bg': 61,\n",
       " '.bh': 62,\n",
       " '.bi': 63,\n",
       " '.bj': 64,\n",
       " '.bk': 65,\n",
       " '.bl': 66,\n",
       " '.bm': 67,\n",
       " '.bn': 68,\n",
       " '.bo': 69,\n",
       " '.bp': 70,\n",
       " '.bq': 71,\n",
       " '.br': 72,\n",
       " '.bs': 73,\n",
       " '.bt': 74,\n",
       " '.bu': 75,\n",
       " '.bv': 76,\n",
       " '.bw': 77,\n",
       " '.bx': 78,\n",
       " '.by': 79,\n",
       " '.bz': 80,\n",
       " '.c.': 81,\n",
       " '.ca': 82,\n",
       " '.cb': 83,\n",
       " '.cc': 84,\n",
       " '.cd': 85,\n",
       " '.ce': 86,\n",
       " '.cf': 87,\n",
       " '.cg': 88,\n",
       " '.ch': 89,\n",
       " '.ci': 90,\n",
       " '.cj': 91,\n",
       " '.ck': 92,\n",
       " '.cl': 93,\n",
       " '.cm': 94,\n",
       " '.cn': 95,\n",
       " '.co': 96,\n",
       " '.cp': 97,\n",
       " '.cq': 98,\n",
       " '.cr': 99,\n",
       " '.cs': 100,\n",
       " '.ct': 101,\n",
       " '.cu': 102,\n",
       " '.cv': 103,\n",
       " '.cw': 104,\n",
       " '.cx': 105,\n",
       " '.cy': 106,\n",
       " '.cz': 107,\n",
       " '.d.': 108,\n",
       " '.da': 109,\n",
       " '.db': 110,\n",
       " '.dc': 111,\n",
       " '.dd': 112,\n",
       " '.de': 113,\n",
       " '.df': 114,\n",
       " '.dg': 115,\n",
       " '.dh': 116,\n",
       " '.di': 117,\n",
       " '.dj': 118,\n",
       " '.dk': 119,\n",
       " '.dl': 120,\n",
       " '.dm': 121,\n",
       " '.dn': 122,\n",
       " '.do': 123,\n",
       " '.dp': 124,\n",
       " '.dq': 125,\n",
       " '.dr': 126,\n",
       " '.ds': 127,\n",
       " '.dt': 128,\n",
       " '.du': 129,\n",
       " '.dv': 130,\n",
       " '.dw': 131,\n",
       " '.dx': 132,\n",
       " '.dy': 133,\n",
       " '.dz': 134,\n",
       " '.e.': 135,\n",
       " '.ea': 136,\n",
       " '.eb': 137,\n",
       " '.ec': 138,\n",
       " '.ed': 139,\n",
       " '.ee': 140,\n",
       " '.ef': 141,\n",
       " '.eg': 142,\n",
       " '.eh': 143,\n",
       " '.ei': 144,\n",
       " '.ej': 145,\n",
       " '.ek': 146,\n",
       " '.el': 147,\n",
       " '.em': 148,\n",
       " '.en': 149,\n",
       " '.eo': 150,\n",
       " '.ep': 151,\n",
       " '.eq': 152,\n",
       " '.er': 153,\n",
       " '.es': 154,\n",
       " '.et': 155,\n",
       " '.eu': 156,\n",
       " '.ev': 157,\n",
       " '.ew': 158,\n",
       " '.ex': 159,\n",
       " '.ey': 160,\n",
       " '.ez': 161,\n",
       " '.f.': 162,\n",
       " '.fa': 163,\n",
       " '.fb': 164,\n",
       " '.fc': 165,\n",
       " '.fd': 166,\n",
       " '.fe': 167,\n",
       " '.ff': 168,\n",
       " '.fg': 169,\n",
       " '.fh': 170,\n",
       " '.fi': 171,\n",
       " '.fj': 172,\n",
       " '.fk': 173,\n",
       " '.fl': 174,\n",
       " '.fm': 175,\n",
       " '.fn': 176,\n",
       " '.fo': 177,\n",
       " '.fp': 178,\n",
       " '.fq': 179,\n",
       " '.fr': 180,\n",
       " '.fs': 181,\n",
       " '.ft': 182,\n",
       " '.fu': 183,\n",
       " '.fv': 184,\n",
       " '.fw': 185,\n",
       " '.fx': 186,\n",
       " '.fy': 187,\n",
       " '.fz': 188,\n",
       " '.g.': 189,\n",
       " '.ga': 190,\n",
       " '.gb': 191,\n",
       " '.gc': 192,\n",
       " '.gd': 193,\n",
       " '.ge': 194,\n",
       " '.gf': 195,\n",
       " '.gg': 196,\n",
       " '.gh': 197,\n",
       " '.gi': 198,\n",
       " '.gj': 199,\n",
       " '.gk': 200,\n",
       " '.gl': 201,\n",
       " '.gm': 202,\n",
       " '.gn': 203,\n",
       " '.go': 204,\n",
       " '.gp': 205,\n",
       " '.gq': 206,\n",
       " '.gr': 207,\n",
       " '.gs': 208,\n",
       " '.gt': 209,\n",
       " '.gu': 210,\n",
       " '.gv': 211,\n",
       " '.gw': 212,\n",
       " '.gx': 213,\n",
       " '.gy': 214,\n",
       " '.gz': 215,\n",
       " '.h.': 216,\n",
       " '.ha': 217,\n",
       " '.hb': 218,\n",
       " '.hc': 219,\n",
       " '.hd': 220,\n",
       " '.he': 221,\n",
       " '.hf': 222,\n",
       " '.hg': 223,\n",
       " '.hh': 224,\n",
       " '.hi': 225,\n",
       " '.hj': 226,\n",
       " '.hk': 227,\n",
       " '.hl': 228,\n",
       " '.hm': 229,\n",
       " '.hn': 230,\n",
       " '.ho': 231,\n",
       " '.hp': 232,\n",
       " '.hq': 233,\n",
       " '.hr': 234,\n",
       " '.hs': 235,\n",
       " '.ht': 236,\n",
       " '.hu': 237,\n",
       " '.hv': 238,\n",
       " '.hw': 239,\n",
       " '.hx': 240,\n",
       " '.hy': 241,\n",
       " '.hz': 242,\n",
       " '.i.': 243,\n",
       " '.ia': 244,\n",
       " '.ib': 245,\n",
       " '.ic': 246,\n",
       " '.id': 247,\n",
       " '.ie': 248,\n",
       " '.if': 249,\n",
       " '.ig': 250,\n",
       " '.ih': 251,\n",
       " '.ii': 252,\n",
       " '.ij': 253,\n",
       " '.ik': 254,\n",
       " '.il': 255,\n",
       " '.im': 256,\n",
       " '.in': 257,\n",
       " '.io': 258,\n",
       " '.ip': 259,\n",
       " '.iq': 260,\n",
       " '.ir': 261,\n",
       " '.is': 262,\n",
       " '.it': 263,\n",
       " '.iu': 264,\n",
       " '.iv': 265,\n",
       " '.iw': 266,\n",
       " '.ix': 267,\n",
       " '.iy': 268,\n",
       " '.iz': 269,\n",
       " '.j.': 270,\n",
       " '.ja': 271,\n",
       " '.jb': 272,\n",
       " '.jc': 273,\n",
       " '.jd': 274,\n",
       " '.je': 275,\n",
       " '.jf': 276,\n",
       " '.jg': 277,\n",
       " '.jh': 278,\n",
       " '.ji': 279,\n",
       " '.jj': 280,\n",
       " '.jk': 281,\n",
       " '.jl': 282,\n",
       " '.jm': 283,\n",
       " '.jn': 284,\n",
       " '.jo': 285,\n",
       " '.jp': 286,\n",
       " '.jq': 287,\n",
       " '.jr': 288,\n",
       " '.js': 289,\n",
       " '.jt': 290,\n",
       " '.ju': 291,\n",
       " '.jv': 292,\n",
       " '.jw': 293,\n",
       " '.jx': 294,\n",
       " '.jy': 295,\n",
       " '.jz': 296,\n",
       " '.k.': 297,\n",
       " '.ka': 298,\n",
       " '.kb': 299,\n",
       " '.kc': 300,\n",
       " '.kd': 301,\n",
       " '.ke': 302,\n",
       " '.kf': 303,\n",
       " '.kg': 304,\n",
       " '.kh': 305,\n",
       " '.ki': 306,\n",
       " '.kj': 307,\n",
       " '.kk': 308,\n",
       " '.kl': 309,\n",
       " '.km': 310,\n",
       " '.kn': 311,\n",
       " '.ko': 312,\n",
       " '.kp': 313,\n",
       " '.kq': 314,\n",
       " '.kr': 315,\n",
       " '.ks': 316,\n",
       " '.kt': 317,\n",
       " '.ku': 318,\n",
       " '.kv': 319,\n",
       " '.kw': 320,\n",
       " '.kx': 321,\n",
       " '.ky': 322,\n",
       " '.kz': 323,\n",
       " '.l.': 324,\n",
       " '.la': 325,\n",
       " '.lb': 326,\n",
       " '.lc': 327,\n",
       " '.ld': 328,\n",
       " '.le': 329,\n",
       " '.lf': 330,\n",
       " '.lg': 331,\n",
       " '.lh': 332,\n",
       " '.li': 333,\n",
       " '.lj': 334,\n",
       " '.lk': 335,\n",
       " '.ll': 336,\n",
       " '.lm': 337,\n",
       " '.ln': 338,\n",
       " '.lo': 339,\n",
       " '.lp': 340,\n",
       " '.lq': 341,\n",
       " '.lr': 342,\n",
       " '.ls': 343,\n",
       " '.lt': 344,\n",
       " '.lu': 345,\n",
       " '.lv': 346,\n",
       " '.lw': 347,\n",
       " '.lx': 348,\n",
       " '.ly': 349,\n",
       " '.lz': 350,\n",
       " '.m.': 351,\n",
       " '.ma': 352,\n",
       " '.mb': 353,\n",
       " '.mc': 354,\n",
       " '.md': 355,\n",
       " '.me': 356,\n",
       " '.mf': 357,\n",
       " '.mg': 358,\n",
       " '.mh': 359,\n",
       " '.mi': 360,\n",
       " '.mj': 361,\n",
       " '.mk': 362,\n",
       " '.ml': 363,\n",
       " '.mm': 364,\n",
       " '.mn': 365,\n",
       " '.mo': 366,\n",
       " '.mp': 367,\n",
       " '.mq': 368,\n",
       " '.mr': 369,\n",
       " '.ms': 370,\n",
       " '.mt': 371,\n",
       " '.mu': 372,\n",
       " '.mv': 373,\n",
       " '.mw': 374,\n",
       " '.mx': 375,\n",
       " '.my': 376,\n",
       " '.mz': 377,\n",
       " '.n.': 378,\n",
       " '.na': 379,\n",
       " '.nb': 380,\n",
       " '.nc': 381,\n",
       " '.nd': 382,\n",
       " '.ne': 383,\n",
       " '.nf': 384,\n",
       " '.ng': 385,\n",
       " '.nh': 386,\n",
       " '.ni': 387,\n",
       " '.nj': 388,\n",
       " '.nk': 389,\n",
       " '.nl': 390,\n",
       " '.nm': 391,\n",
       " '.nn': 392,\n",
       " '.no': 393,\n",
       " '.np': 394,\n",
       " '.nq': 395,\n",
       " '.nr': 396,\n",
       " '.ns': 397,\n",
       " '.nt': 398,\n",
       " '.nu': 399,\n",
       " '.nv': 400,\n",
       " '.nw': 401,\n",
       " '.nx': 402,\n",
       " '.ny': 403,\n",
       " '.nz': 404,\n",
       " '.o.': 405,\n",
       " '.oa': 406,\n",
       " '.ob': 407,\n",
       " '.oc': 408,\n",
       " '.od': 409,\n",
       " '.oe': 410,\n",
       " '.of': 411,\n",
       " '.og': 412,\n",
       " '.oh': 413,\n",
       " '.oi': 414,\n",
       " '.oj': 415,\n",
       " '.ok': 416,\n",
       " '.ol': 417,\n",
       " '.om': 418,\n",
       " '.on': 419,\n",
       " '.oo': 420,\n",
       " '.op': 421,\n",
       " '.oq': 422,\n",
       " '.or': 423,\n",
       " '.os': 424,\n",
       " '.ot': 425,\n",
       " '.ou': 426,\n",
       " '.ov': 427,\n",
       " '.ow': 428,\n",
       " '.ox': 429,\n",
       " '.oy': 430,\n",
       " '.oz': 431,\n",
       " '.p.': 432,\n",
       " '.pa': 433,\n",
       " '.pb': 434,\n",
       " '.pc': 435,\n",
       " '.pd': 436,\n",
       " '.pe': 437,\n",
       " '.pf': 438,\n",
       " '.pg': 439,\n",
       " '.ph': 440,\n",
       " '.pi': 441,\n",
       " '.pj': 442,\n",
       " '.pk': 443,\n",
       " '.pl': 444,\n",
       " '.pm': 445,\n",
       " '.pn': 446,\n",
       " '.po': 447,\n",
       " '.pp': 448,\n",
       " '.pq': 449,\n",
       " '.pr': 450,\n",
       " '.ps': 451,\n",
       " '.pt': 452,\n",
       " '.pu': 453,\n",
       " '.pv': 454,\n",
       " '.pw': 455,\n",
       " '.px': 456,\n",
       " '.py': 457,\n",
       " '.pz': 458,\n",
       " '.q.': 459,\n",
       " '.qa': 460,\n",
       " '.qb': 461,\n",
       " '.qc': 462,\n",
       " '.qd': 463,\n",
       " '.qe': 464,\n",
       " '.qf': 465,\n",
       " '.qg': 466,\n",
       " '.qh': 467,\n",
       " '.qi': 468,\n",
       " '.qj': 469,\n",
       " '.qk': 470,\n",
       " '.ql': 471,\n",
       " '.qm': 472,\n",
       " '.qn': 473,\n",
       " '.qo': 474,\n",
       " '.qp': 475,\n",
       " '.qq': 476,\n",
       " '.qr': 477,\n",
       " '.qs': 478,\n",
       " '.qt': 479,\n",
       " '.qu': 480,\n",
       " '.qv': 481,\n",
       " '.qw': 482,\n",
       " '.qx': 483,\n",
       " '.qy': 484,\n",
       " '.qz': 485,\n",
       " '.r.': 486,\n",
       " '.ra': 487,\n",
       " '.rb': 488,\n",
       " '.rc': 489,\n",
       " '.rd': 490,\n",
       " '.re': 491,\n",
       " '.rf': 492,\n",
       " '.rg': 493,\n",
       " '.rh': 494,\n",
       " '.ri': 495,\n",
       " '.rj': 496,\n",
       " '.rk': 497,\n",
       " '.rl': 498,\n",
       " '.rm': 499,\n",
       " '.rn': 500,\n",
       " '.ro': 501,\n",
       " '.rp': 502,\n",
       " '.rq': 503,\n",
       " '.rr': 504,\n",
       " '.rs': 505,\n",
       " '.rt': 506,\n",
       " '.ru': 507,\n",
       " '.rv': 508,\n",
       " '.rw': 509,\n",
       " '.rx': 510,\n",
       " '.ry': 511,\n",
       " '.rz': 512,\n",
       " '.s.': 513,\n",
       " '.sa': 514,\n",
       " '.sb': 515,\n",
       " '.sc': 516,\n",
       " '.sd': 517,\n",
       " '.se': 518,\n",
       " '.sf': 519,\n",
       " '.sg': 520,\n",
       " '.sh': 521,\n",
       " '.si': 522,\n",
       " '.sj': 523,\n",
       " '.sk': 524,\n",
       " '.sl': 525,\n",
       " '.sm': 526,\n",
       " '.sn': 527,\n",
       " '.so': 528,\n",
       " '.sp': 529,\n",
       " '.sq': 530,\n",
       " '.sr': 531,\n",
       " '.ss': 532,\n",
       " '.st': 533,\n",
       " '.su': 534,\n",
       " '.sv': 535,\n",
       " '.sw': 536,\n",
       " '.sx': 537,\n",
       " '.sy': 538,\n",
       " '.sz': 539,\n",
       " '.t.': 540,\n",
       " '.ta': 541,\n",
       " '.tb': 542,\n",
       " '.tc': 543,\n",
       " '.td': 544,\n",
       " '.te': 545,\n",
       " '.tf': 546,\n",
       " '.tg': 547,\n",
       " '.th': 548,\n",
       " '.ti': 549,\n",
       " '.tj': 550,\n",
       " '.tk': 551,\n",
       " '.tl': 552,\n",
       " '.tm': 553,\n",
       " '.tn': 554,\n",
       " '.to': 555,\n",
       " '.tp': 556,\n",
       " '.tq': 557,\n",
       " '.tr': 558,\n",
       " '.ts': 559,\n",
       " '.tt': 560,\n",
       " '.tu': 561,\n",
       " '.tv': 562,\n",
       " '.tw': 563,\n",
       " '.tx': 564,\n",
       " '.ty': 565,\n",
       " '.tz': 566,\n",
       " '.u.': 567,\n",
       " '.ua': 568,\n",
       " '.ub': 569,\n",
       " '.uc': 570,\n",
       " '.ud': 571,\n",
       " '.ue': 572,\n",
       " '.uf': 573,\n",
       " '.ug': 574,\n",
       " '.uh': 575,\n",
       " '.ui': 576,\n",
       " '.uj': 577,\n",
       " '.uk': 578,\n",
       " '.ul': 579,\n",
       " '.um': 580,\n",
       " '.un': 581,\n",
       " '.uo': 582,\n",
       " '.up': 583,\n",
       " '.uq': 584,\n",
       " '.ur': 585,\n",
       " '.us': 586,\n",
       " '.ut': 587,\n",
       " '.uu': 588,\n",
       " '.uv': 589,\n",
       " '.uw': 590,\n",
       " '.ux': 591,\n",
       " '.uy': 592,\n",
       " '.uz': 593,\n",
       " '.v.': 594,\n",
       " '.va': 595,\n",
       " '.vb': 596,\n",
       " '.vc': 597,\n",
       " '.vd': 598,\n",
       " '.ve': 599,\n",
       " '.vf': 600,\n",
       " '.vg': 601,\n",
       " '.vh': 602,\n",
       " '.vi': 603,\n",
       " '.vj': 604,\n",
       " '.vk': 605,\n",
       " '.vl': 606,\n",
       " '.vm': 607,\n",
       " '.vn': 608,\n",
       " '.vo': 609,\n",
       " '.vp': 610,\n",
       " '.vq': 611,\n",
       " '.vr': 612,\n",
       " '.vs': 613,\n",
       " '.vt': 614,\n",
       " '.vu': 615,\n",
       " '.vv': 616,\n",
       " '.vw': 617,\n",
       " '.vx': 618,\n",
       " '.vy': 619,\n",
       " '.vz': 620,\n",
       " '.w.': 621,\n",
       " '.wa': 622,\n",
       " '.wb': 623,\n",
       " '.wc': 624,\n",
       " '.wd': 625,\n",
       " '.we': 626,\n",
       " '.wf': 627,\n",
       " '.wg': 628,\n",
       " '.wh': 629,\n",
       " '.wi': 630,\n",
       " '.wj': 631,\n",
       " '.wk': 632,\n",
       " '.wl': 633,\n",
       " '.wm': 634,\n",
       " '.wn': 635,\n",
       " '.wo': 636,\n",
       " '.wp': 637,\n",
       " '.wq': 638,\n",
       " '.wr': 639,\n",
       " '.ws': 640,\n",
       " '.wt': 641,\n",
       " '.wu': 642,\n",
       " '.wv': 643,\n",
       " '.ww': 644,\n",
       " '.wx': 645,\n",
       " '.wy': 646,\n",
       " '.wz': 647,\n",
       " '.x.': 648,\n",
       " '.xa': 649,\n",
       " '.xb': 650,\n",
       " '.xc': 651,\n",
       " '.xd': 652,\n",
       " '.xe': 653,\n",
       " '.xf': 654,\n",
       " '.xg': 655,\n",
       " '.xh': 656,\n",
       " '.xi': 657,\n",
       " '.xj': 658,\n",
       " '.xk': 659,\n",
       " '.xl': 660,\n",
       " '.xm': 661,\n",
       " '.xn': 662,\n",
       " '.xo': 663,\n",
       " '.xp': 664,\n",
       " '.xq': 665,\n",
       " '.xr': 666,\n",
       " '.xs': 667,\n",
       " '.xt': 668,\n",
       " '.xu': 669,\n",
       " '.xv': 670,\n",
       " '.xw': 671,\n",
       " '.xx': 672,\n",
       " '.xy': 673,\n",
       " '.xz': 674,\n",
       " '.y.': 675,\n",
       " '.ya': 676,\n",
       " '.yb': 677,\n",
       " '.yc': 678,\n",
       " '.yd': 679,\n",
       " '.ye': 680,\n",
       " '.yf': 681,\n",
       " '.yg': 682,\n",
       " '.yh': 683,\n",
       " '.yi': 684,\n",
       " '.yj': 685,\n",
       " '.yk': 686,\n",
       " '.yl': 687,\n",
       " '.ym': 688,\n",
       " '.yn': 689,\n",
       " '.yo': 690,\n",
       " '.yp': 691,\n",
       " '.yq': 692,\n",
       " '.yr': 693,\n",
       " '.ys': 694,\n",
       " '.yt': 695,\n",
       " '.yu': 696,\n",
       " '.yv': 697,\n",
       " '.yw': 698,\n",
       " '.yx': 699,\n",
       " '.yy': 700,\n",
       " '.yz': 701,\n",
       " '.z.': 702,\n",
       " '.za': 703,\n",
       " '.zb': 704,\n",
       " '.zc': 705,\n",
       " '.zd': 706,\n",
       " '.ze': 707,\n",
       " '.zf': 708,\n",
       " '.zg': 709,\n",
       " '.zh': 710,\n",
       " '.zi': 711,\n",
       " '.zj': 712,\n",
       " '.zk': 713,\n",
       " '.zl': 714,\n",
       " '.zm': 715,\n",
       " '.zn': 716,\n",
       " '.zo': 717,\n",
       " '.zp': 718,\n",
       " '.zq': 719,\n",
       " '.zr': 720,\n",
       " '.zs': 721,\n",
       " '.zt': 722,\n",
       " '.zu': 723,\n",
       " '.zv': 724,\n",
       " '.zw': 725,\n",
       " '.zx': 726,\n",
       " '.zy': 727,\n",
       " '.zz': 728,\n",
       " 'a..': 729,\n",
       " 'a.a': 730,\n",
       " 'a.b': 731,\n",
       " 'a.c': 732,\n",
       " 'a.d': 733,\n",
       " 'a.e': 734,\n",
       " 'a.f': 735,\n",
       " 'a.g': 736,\n",
       " 'a.h': 737,\n",
       " 'a.i': 738,\n",
       " 'a.j': 739,\n",
       " 'a.k': 740,\n",
       " 'a.l': 741,\n",
       " 'a.m': 742,\n",
       " 'a.n': 743,\n",
       " 'a.o': 744,\n",
       " 'a.p': 745,\n",
       " 'a.q': 746,\n",
       " 'a.r': 747,\n",
       " 'a.s': 748,\n",
       " 'a.t': 749,\n",
       " 'a.u': 750,\n",
       " 'a.v': 751,\n",
       " 'a.w': 752,\n",
       " 'a.x': 753,\n",
       " 'a.y': 754,\n",
       " 'a.z': 755,\n",
       " 'aa.': 756,\n",
       " 'aaa': 757,\n",
       " 'aab': 758,\n",
       " 'aac': 759,\n",
       " 'aad': 760,\n",
       " 'aae': 761,\n",
       " 'aaf': 762,\n",
       " 'aag': 763,\n",
       " 'aah': 764,\n",
       " 'aai': 765,\n",
       " 'aaj': 766,\n",
       " 'aak': 767,\n",
       " 'aal': 768,\n",
       " 'aam': 769,\n",
       " 'aan': 770,\n",
       " 'aao': 771,\n",
       " 'aap': 772,\n",
       " 'aaq': 773,\n",
       " 'aar': 774,\n",
       " 'aas': 775,\n",
       " 'aat': 776,\n",
       " 'aau': 777,\n",
       " 'aav': 778,\n",
       " 'aaw': 779,\n",
       " 'aax': 780,\n",
       " 'aay': 781,\n",
       " 'aaz': 782,\n",
       " 'ab.': 783,\n",
       " 'aba': 784,\n",
       " 'abb': 785,\n",
       " 'abc': 786,\n",
       " 'abd': 787,\n",
       " 'abe': 788,\n",
       " 'abf': 789,\n",
       " 'abg': 790,\n",
       " 'abh': 791,\n",
       " 'abi': 792,\n",
       " 'abj': 793,\n",
       " 'abk': 794,\n",
       " 'abl': 795,\n",
       " 'abm': 796,\n",
       " 'abn': 797,\n",
       " 'abo': 798,\n",
       " 'abp': 799,\n",
       " 'abq': 800,\n",
       " 'abr': 801,\n",
       " 'abs': 802,\n",
       " 'abt': 803,\n",
       " 'abu': 804,\n",
       " 'abv': 805,\n",
       " 'abw': 806,\n",
       " 'abx': 807,\n",
       " 'aby': 808,\n",
       " 'abz': 809,\n",
       " 'ac.': 810,\n",
       " 'aca': 811,\n",
       " 'acb': 812,\n",
       " 'acc': 813,\n",
       " 'acd': 814,\n",
       " 'ace': 815,\n",
       " 'acf': 816,\n",
       " 'acg': 817,\n",
       " 'ach': 818,\n",
       " 'aci': 819,\n",
       " 'acj': 820,\n",
       " 'ack': 821,\n",
       " 'acl': 822,\n",
       " 'acm': 823,\n",
       " 'acn': 824,\n",
       " 'aco': 825,\n",
       " 'acp': 826,\n",
       " 'acq': 827,\n",
       " 'acr': 828,\n",
       " 'acs': 829,\n",
       " 'act': 830,\n",
       " 'acu': 831,\n",
       " 'acv': 832,\n",
       " 'acw': 833,\n",
       " 'acx': 834,\n",
       " 'acy': 835,\n",
       " 'acz': 836,\n",
       " 'ad.': 837,\n",
       " 'ada': 838,\n",
       " 'adb': 839,\n",
       " 'adc': 840,\n",
       " 'add': 841,\n",
       " 'ade': 842,\n",
       " 'adf': 843,\n",
       " 'adg': 844,\n",
       " 'adh': 845,\n",
       " 'adi': 846,\n",
       " 'adj': 847,\n",
       " 'adk': 848,\n",
       " 'adl': 849,\n",
       " 'adm': 850,\n",
       " 'adn': 851,\n",
       " 'ado': 852,\n",
       " 'adp': 853,\n",
       " 'adq': 854,\n",
       " 'adr': 855,\n",
       " 'ads': 856,\n",
       " 'adt': 857,\n",
       " 'adu': 858,\n",
       " 'adv': 859,\n",
       " 'adw': 860,\n",
       " 'adx': 861,\n",
       " 'ady': 862,\n",
       " 'adz': 863,\n",
       " 'ae.': 864,\n",
       " 'aea': 865,\n",
       " 'aeb': 866,\n",
       " 'aec': 867,\n",
       " 'aed': 868,\n",
       " 'aee': 869,\n",
       " 'aef': 870,\n",
       " 'aeg': 871,\n",
       " 'aeh': 872,\n",
       " 'aei': 873,\n",
       " 'aej': 874,\n",
       " 'aek': 875,\n",
       " 'ael': 876,\n",
       " 'aem': 877,\n",
       " 'aen': 878,\n",
       " 'aeo': 879,\n",
       " 'aep': 880,\n",
       " 'aeq': 881,\n",
       " 'aer': 882,\n",
       " 'aes': 883,\n",
       " 'aet': 884,\n",
       " 'aeu': 885,\n",
       " 'aev': 886,\n",
       " 'aew': 887,\n",
       " 'aex': 888,\n",
       " 'aey': 889,\n",
       " 'aez': 890,\n",
       " 'af.': 891,\n",
       " 'afa': 892,\n",
       " 'afb': 893,\n",
       " 'afc': 894,\n",
       " 'afd': 895,\n",
       " 'afe': 896,\n",
       " 'aff': 897,\n",
       " 'afg': 898,\n",
       " 'afh': 899,\n",
       " 'afi': 900,\n",
       " 'afj': 901,\n",
       " 'afk': 902,\n",
       " 'afl': 903,\n",
       " 'afm': 904,\n",
       " 'afn': 905,\n",
       " 'afo': 906,\n",
       " 'afp': 907,\n",
       " 'afq': 908,\n",
       " 'afr': 909,\n",
       " 'afs': 910,\n",
       " 'aft': 911,\n",
       " 'afu': 912,\n",
       " 'afv': 913,\n",
       " 'afw': 914,\n",
       " 'afx': 915,\n",
       " 'afy': 916,\n",
       " 'afz': 917,\n",
       " 'ag.': 918,\n",
       " 'aga': 919,\n",
       " 'agb': 920,\n",
       " 'agc': 921,\n",
       " 'agd': 922,\n",
       " 'age': 923,\n",
       " 'agf': 924,\n",
       " 'agg': 925,\n",
       " 'agh': 926,\n",
       " 'agi': 927,\n",
       " 'agj': 928,\n",
       " 'agk': 929,\n",
       " 'agl': 930,\n",
       " 'agm': 931,\n",
       " 'agn': 932,\n",
       " 'ago': 933,\n",
       " 'agp': 934,\n",
       " 'agq': 935,\n",
       " 'agr': 936,\n",
       " 'ags': 937,\n",
       " 'agt': 938,\n",
       " 'agu': 939,\n",
       " 'agv': 940,\n",
       " 'agw': 941,\n",
       " 'agx': 942,\n",
       " 'agy': 943,\n",
       " 'agz': 944,\n",
       " 'ah.': 945,\n",
       " 'aha': 946,\n",
       " 'ahb': 947,\n",
       " 'ahc': 948,\n",
       " 'ahd': 949,\n",
       " 'ahe': 950,\n",
       " 'ahf': 951,\n",
       " 'ahg': 952,\n",
       " 'ahh': 953,\n",
       " 'ahi': 954,\n",
       " 'ahj': 955,\n",
       " 'ahk': 956,\n",
       " 'ahl': 957,\n",
       " 'ahm': 958,\n",
       " 'ahn': 959,\n",
       " 'aho': 960,\n",
       " 'ahp': 961,\n",
       " 'ahq': 962,\n",
       " 'ahr': 963,\n",
       " 'ahs': 964,\n",
       " 'aht': 965,\n",
       " 'ahu': 966,\n",
       " 'ahv': 967,\n",
       " 'ahw': 968,\n",
       " 'ahx': 969,\n",
       " 'ahy': 970,\n",
       " 'ahz': 971,\n",
       " 'ai.': 972,\n",
       " 'aia': 973,\n",
       " 'aib': 974,\n",
       " 'aic': 975,\n",
       " 'aid': 976,\n",
       " 'aie': 977,\n",
       " 'aif': 978,\n",
       " 'aig': 979,\n",
       " 'aih': 980,\n",
       " 'aii': 981,\n",
       " 'aij': 982,\n",
       " 'aik': 983,\n",
       " 'ail': 984,\n",
       " 'aim': 985,\n",
       " 'ain': 986,\n",
       " 'aio': 987,\n",
       " 'aip': 988,\n",
       " 'aiq': 989,\n",
       " 'air': 990,\n",
       " 'ais': 991,\n",
       " 'ait': 992,\n",
       " 'aiu': 993,\n",
       " 'aiv': 994,\n",
       " 'aiw': 995,\n",
       " 'aix': 996,\n",
       " 'aiy': 997,\n",
       " 'aiz': 998,\n",
       " 'aj.': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "letter_groups = []\n",
    "for l in letters + ['.']:\n",
    "    for l2 in letters + ['.']:\n",
    "        for l3 in letters + ['.']:\n",
    "            i = l + l2 + l3\n",
    "            letter_groups.append(i)\n",
    "\n",
    "letter_groups = sorted(letter_groups)\n",
    "lgtoi = {p:i for i, p in enumerate(letter_groups)}\n",
    "lgtoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0,  5, 13],\n",
       "         [ 5, 13, 13],\n",
       "         [13, 13,  1]]),\n",
       " tensor([13,  1,  0]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs = []\n",
    "ys = []\n",
    "for w in words:\n",
    "    wdot = f'.{w}.'\n",
    "    for i in range(len(wdot) - 3):\n",
    "        xs.append([stoi[wdot[i]], stoi[wdot[i+1]], stoi[wdot[i+2]]])\n",
    "        ys.append(stoi[wdot[i+3]])\n",
    "\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "\n",
    "xs[:3], ys[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0631, -0.2029],\n",
       "        [ 2.8766, -0.7994],\n",
       "        [-1.6427,  0.2011],\n",
       "        [-0.8726, -2.6461],\n",
       "        [-0.2279,  0.4190],\n",
       "        [-0.2730,  1.0746],\n",
       "        [ 0.0260,  1.4665],\n",
       "        [-0.5551, -1.1855],\n",
       "        [-0.6583, -0.2567],\n",
       "        [ 0.0440,  2.8808],\n",
       "        [-0.5162,  3.0270],\n",
       "        [-0.7207,  0.4204],\n",
       "        [-0.4111, -0.1048],\n",
       "        [ 0.0774, -0.1515],\n",
       "        [ 1.3203, -0.0547],\n",
       "        [-0.3013,  0.4203],\n",
       "        [-0.8684, -0.1510],\n",
       "        [-0.3079, -0.0278],\n",
       "        [-1.9435, -1.0159],\n",
       "        [ 0.9335, -1.6403],\n",
       "        [ 1.2955,  0.8649],\n",
       "        [-0.7816,  0.7840],\n",
       "        [-0.1503,  1.6820],\n",
       "        [ 1.0821, -2.4105],\n",
       "        [ 0.8483, -0.1123],\n",
       "        [-0.1654, -1.8554],\n",
       "        [-0.2905, -1.0411]], requires_grad=True)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Embedding matrix: 2 dimensions for each of our 27 letters and .\n",
    "C = torch.randn(len(letters) + 1, 2, requires_grad=True)\n",
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0631, -0.2029],\n",
       "         [-0.2730,  1.0746],\n",
       "         [ 0.0774, -0.1515]],\n",
       "\n",
       "        [[-0.2730,  1.0746],\n",
       "         [ 0.0774, -0.1515],\n",
       "         [ 0.0774, -0.1515]],\n",
       "\n",
       "        [[ 0.0774, -0.1515],\n",
       "         [ 0.0774, -0.1515],\n",
       "         [ 2.8766, -0.7994]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.2905, -1.0411],\n",
       "         [-0.2905, -1.0411],\n",
       "         [-0.1654, -1.8554]],\n",
       "\n",
       "        [[-0.2905, -1.0411],\n",
       "         [-0.1654, -1.8554],\n",
       "         [-0.2905, -1.0411]],\n",
       "\n",
       "        [[-0.1654, -1.8554],\n",
       "         [-0.2905, -1.0411],\n",
       "         [ 0.8483, -0.1123]]], grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can get the embedding of each sample simultaneously\n",
    "# Each of the 3 letters in the sample gets its embedding pulled in\n",
    "C[xs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([164080, 3])\n",
      "tensor([ 0,  5, 13])\n",
      "m\n",
      "tensor([ 0.0774, -0.1515], grad_fn=<SelectBackward0>)\n",
      "tensor([ 0.0774, -0.1515], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Here we prove out our method of getting all the embeddings of all the samples simultaneously\n",
    "# We have a tensor of shape datasetsize, 3 - 3 letters per sample\n",
    "print(xs.shape)\n",
    "print(xs[0])\n",
    "# Plucking an individual letter from the first sample (emm -> m) and getting its embedding\n",
    "print(itos[xs[0, 2].item()])\n",
    "print(C[xs[0, 2]])\n",
    "# We can prove this works and the embedding indexing is correct by manually pulling out the embedding\n",
    "print(C[stoi['m']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([164080, 3, 2])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now that we have our dataset and embeddings, we can create our model\n",
    "# What shape does the embedding matmul layer need to be?\n",
    "# For each sample, we have 3 letters, each of which has a 2 dimensional embedding\n",
    "C[xs].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([164080, 6])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C[xs].view(-1, 6).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.0631, -0.2029],\n",
       "         [-0.2730,  1.0746],\n",
       "         [ 0.0774, -0.1515]], grad_fn=<SelectBackward0>),\n",
       " tensor([ 0.0631, -0.2029, -0.2730,  1.0746,  0.0774, -0.1515],\n",
       "        grad_fn=<SelectBackward0>),\n",
       " torch.Size([164080, 6]))"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In order to matmul against a weights layer, we need to adjust the view to \n",
    "# squeeze together the weights for each sample into 1 array\n",
    "# .view is extremely efficient for this.\n",
    "# -1 indicates torch should infer the remainder - since we say 6, \n",
    "# it knows what's left is just the sample size\n",
    "C[xs][0], C[xs].view(-1, 6)[0], C[xs].view(-1, 6).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matmul rules = columns in mat1 must match rows of mat2\n",
    "W1 = torch.randn(3 * 2, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([164080, 6]), torch.Size([6, 100]))"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C[xs].view(-1, 6).shape, W1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.8130,  0.5567, -0.8454,  ...,  0.2220, -0.8572, -0.8659],\n",
       "        [-0.2813, -0.2873, -0.0758,  ...,  0.8181, -0.8381, -0.1628],\n",
       "        [ 0.4101, -0.9823,  0.9907,  ...,  0.9825, -0.9856, -0.6687],\n",
       "        ...,\n",
       "        [ 0.8378, -0.7639,  0.8282,  ...,  0.9997,  0.9962, -1.0000],\n",
       "        [ 0.7021, -0.7984,  0.9368,  ...,  0.9979,  0.9929, -0.9999],\n",
       "        [-0.1550, -0.8173,  0.9516,  ...,  0.7609,  0.6267, -0.9993]],\n",
       "       grad_fn=<TanhBackward0>)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now, instead of feeding in an index for every item in the vocab (each of the 3 letter combos)\n",
    "# we feed in the embeddings - one for each letter\n",
    "emb = C[xs].view(-1, 6)\n",
    "W1 = torch.randn(3*2, 100, requires_grad=True)\n",
    "B1 = torch.randn(1, 100, requires_grad=True)\n",
    "h = torch.tanh((emb @ W1) + B1)\n",
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[  0.1993,   7.8002, -17.5028,  ...,  -3.8611,  -8.4735,  -0.7769],\n",
       "         [  0.8250,  12.4768,  -8.3885,  ...,  12.0273,  12.3067,   3.7830],\n",
       "         [ -3.0201,  12.7769,  -6.0181,  ...,   8.3125,  11.0352, -12.1264],\n",
       "         ...,\n",
       "         [ -7.6937,   4.9222,   3.3658,  ..., -10.0868,  18.0513,  11.7578],\n",
       "         [-13.8144,   6.1876,  15.5580,  ...,  -5.4705,  15.6050,   6.1973],\n",
       "         [-11.3170,   3.0962,   4.9012,  ...,  -4.1695,   2.8165,  -3.4197]],\n",
       "        grad_fn=<AddBackward0>),\n",
       " torch.Size([164080, 27]))"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adding a hidden layer, inputs mach outputs of prior\n",
    "W2 = torch.randn(100, len(letters) + 1, requires_grad=True)\n",
    "B2 = torch.randn(1, len(letters) + 1, requires_grad=True)\n",
    "logits = (h @ W2) + B2\n",
    "logits, logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.6526e-11, 4.1417e-07, 7.0393e-15, 7.9912e-06, 8.4777e-18, 1.6341e-11,\n",
       "        7.4169e-18, 6.8404e-06, 7.7683e-09, 2.5394e-11, 1.3825e-18, 1.7038e-11,\n",
       "        2.7575e-07, 2.9313e-03, 1.2123e-05, 1.6497e-10, 4.0219e-06, 9.9704e-01,\n",
       "        6.5281e-19, 2.3154e-08, 7.1088e-09, 4.7216e-10, 7.6960e-14, 7.7146e-17,\n",
       "        2.6018e-17, 5.8048e-17, 1.0003e-09], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(logits, dim=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(17.1203, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loss\n",
    "F.cross_entropy(logits, ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  9.8757,   6.8974,  -0.6721,  ...,  17.8811,   8.6039,  12.5137],\n",
       "        [ 11.1974,  11.0013,  -4.2726,  ...,   9.7281,   5.2552,   7.4368],\n",
       "        [  7.9302,   3.0886,   4.0240,  ...,   7.1671,  -8.4007,   9.4945],\n",
       "        ...,\n",
       "        [ 11.4861,  -9.3848,  -9.5869,  ...,  -6.5863,  -1.4888,  -3.5241],\n",
       "        [ 10.8306,  -6.1538, -12.2600,  ...,  -0.6697,   0.4025,  -0.0759],\n",
       "        [ 14.7355,  -7.6178,  -4.6745,  ...,  12.1386,  -2.5125,   9.8109]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combining all of the above, this is our model\n",
    "# First layer is the embeddings, second is a hidden layer with 100 neurons, last is the output layer\n",
    "emb = C[xs].view(-1, 6)\n",
    "W1 = torch.randn(3*2, 100, requires_grad=True)\n",
    "B1 = torch.randn(1, 100, requires_grad=True)\n",
    "h = torch.tanh((emb @ W1) + B1)\n",
    "W2 = torch.randn(100, len(letters) + 1, requires_grad=True)\n",
    "B2 = torch.randn(1, len(letters) + 1, requires_grad=True)\n",
    "logits = (h @ W2) + B2\n",
    "logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Careful with broadcasting semantics.\n",
    "I realized I made a mistake in the shape of the bias, should be just 2, not 1,2.\n",
    "Will need to practice this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.3197, -1.8988],\n",
       "         [ 1.3503,  0.5139],\n",
       "         [ 1.4483,  0.7237],\n",
       "         [-0.5174, -0.3881],\n",
       "         [-0.2633,  1.2137],\n",
       "         [ 1.0323,  0.8259]]),\n",
       " tensor([[-0.2194, -0.8094]]),\n",
       " torch.Size([6, 2]),\n",
       " torch.Size([1, 2]),\n",
       " tensor([[ 0.1002, -2.7082],\n",
       "         [ 1.1309, -0.2955],\n",
       "         [ 1.2288, -0.0857],\n",
       "         [-0.7368, -1.1976],\n",
       "         [-0.4828,  0.4043],\n",
       "         [ 0.8128,  0.0165]]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_w1 = torch.randn(6, 2)\n",
    "test_b1 = torch.randn(1, 2)\n",
    "# 6, 2\n",
    "# 1, 2\n",
    "test_w1, test_b1, test_w1.shape, test_b1.shape, test_w1 + test_b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.9454, -2.0285],\n",
       "         [-0.8637, -1.6595],\n",
       "         [ 2.4227, -1.2493],\n",
       "         [ 0.0531, -1.6711],\n",
       "         [-1.0206,  0.6913],\n",
       "         [-2.7342, -2.6559]]),\n",
       " tensor([ 0.4342, -0.2416]),\n",
       " torch.Size([6, 2]),\n",
       " torch.Size([2]),\n",
       " tensor([[-0.5112, -2.2700],\n",
       "         [-0.4295, -1.9011],\n",
       "         [ 2.8569, -1.4908],\n",
       "         [ 0.4872, -1.9127],\n",
       "         [-0.5864,  0.4497],\n",
       "         [-2.3000, -2.8974]]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_w2 = torch.randn(6, 2)\n",
    "test_b2 = torch.randn(2)\n",
    "# 6, 2\n",
    "#    2\n",
    "test_w2, test_b2, test_w2.shape, test_b2.shape, test_w2 + test_b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate param initialization\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "C = torch.randn((len(letters) + 1, 2), generator=g)\n",
    "W1 = torch.randn(3*2, 100, generator=g)\n",
    "B1 = torch.randn(100, generator=g)\n",
    "W2 = torch.randn(100, len(letters) + 1, generator=g)\n",
    "B2 = torch.randn(len(letters) + 1, generator=g)\n",
    "parameters = [C, W1, B1, W2, B2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3481"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Total model size\n",
    "sum(p.nelement() for p in parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(15.0936, grad_fn=<NllLossBackward0>)\n",
      "tensor(14.3958, grad_fn=<NllLossBackward0>)\n",
      "tensor(13.7687, grad_fn=<NllLossBackward0>)\n",
      "tensor(13.2041, grad_fn=<NllLossBackward0>)\n",
      "tensor(12.6962, grad_fn=<NllLossBackward0>)\n",
      "tensor(12.2365, grad_fn=<NllLossBackward0>)\n",
      "tensor(11.8148, grad_fn=<NllLossBackward0>)\n",
      "tensor(11.4224, grad_fn=<NllLossBackward0>)\n",
      "tensor(11.0534, grad_fn=<NllLossBackward0>)\n",
      "tensor(10.7043, grad_fn=<NllLossBackward0>)\n",
      "tensor(10.3727, grad_fn=<NllLossBackward0>)\n",
      "tensor(10.0573, grad_fn=<NllLossBackward0>)\n",
      "tensor(9.7573, grad_fn=<NllLossBackward0>)\n",
      "tensor(9.4721, grad_fn=<NllLossBackward0>)\n",
      "tensor(9.2013, grad_fn=<NllLossBackward0>)\n",
      "tensor(8.9447, grad_fn=<NllLossBackward0>)\n",
      "tensor(8.7022, grad_fn=<NllLossBackward0>)\n",
      "tensor(8.4734, grad_fn=<NllLossBackward0>)\n",
      "tensor(8.2578, grad_fn=<NllLossBackward0>)\n",
      "tensor(8.0542, grad_fn=<NllLossBackward0>)\n",
      "tensor(7.8613, grad_fn=<NllLossBackward0>)\n",
      "tensor(7.6776, grad_fn=<NllLossBackward0>)\n",
      "tensor(7.5017, grad_fn=<NllLossBackward0>)\n",
      "tensor(7.3329, grad_fn=<NllLossBackward0>)\n",
      "tensor(7.1703, grad_fn=<NllLossBackward0>)\n",
      "tensor(7.0137, grad_fn=<NllLossBackward0>)\n",
      "tensor(6.8628, grad_fn=<NllLossBackward0>)\n",
      "tensor(6.7174, grad_fn=<NllLossBackward0>)\n",
      "tensor(6.5774, grad_fn=<NllLossBackward0>)\n",
      "tensor(6.4428, grad_fn=<NllLossBackward0>)\n",
      "tensor(6.3132, grad_fn=<NllLossBackward0>)\n",
      "tensor(6.1888, grad_fn=<NllLossBackward0>)\n",
      "tensor(6.0692, grad_fn=<NllLossBackward0>)\n",
      "tensor(5.9544, grad_fn=<NllLossBackward0>)\n",
      "tensor(5.8443, grad_fn=<NllLossBackward0>)\n",
      "tensor(5.7386, grad_fn=<NllLossBackward0>)\n",
      "tensor(5.6375, grad_fn=<NllLossBackward0>)\n",
      "tensor(5.5407, grad_fn=<NllLossBackward0>)\n",
      "tensor(5.4482, grad_fn=<NllLossBackward0>)\n",
      "tensor(5.3599, grad_fn=<NllLossBackward0>)\n",
      "tensor(5.2757, grad_fn=<NllLossBackward0>)\n",
      "tensor(5.1955, grad_fn=<NllLossBackward0>)\n",
      "tensor(5.1193, grad_fn=<NllLossBackward0>)\n",
      "tensor(5.0467, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.9776, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.9119, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.8491, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.7891, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.7316, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.6764, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.6233, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.5721, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.5228, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.4751, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.4291, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.3845, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.3413, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.2996, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.2591, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.2200, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.1820, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.1453, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.1097, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.0753, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.0419, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.0095, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.9781, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.9476, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.9181, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.8895, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.8617, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.8346, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.8084, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7829, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7582, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7341, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7108, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.6880, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.6660, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.6445, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.6236, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.6034, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.5837, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.5645, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.5459, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.5278, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.5103, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.4932, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.4766, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.4604, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.4448, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.4295, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.4147, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.4003, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.3863, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.3727, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.3595, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.3467, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.3342, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.3220, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    emb = C[xs].view(-1, 6)\n",
    "    h = torch.tanh((emb @ W1) + B1)\n",
    "    logits = (h @ W2) + B2\n",
    "\n",
    "    loss = F.cross_entropy(logits, ys)\n",
    "\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    lr = 0.1\n",
    "    for p in parameters:\n",
    "        p.data -= lr * p.grad\n",
    "\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordsDataset(Dataset):\n",
    "    def __init__(self, words_file_dir):\n",
    "        self.words_file_dir = words_file_dir\n",
    "        self.words = open(words_file_dir).read().splitlines()\n",
    "        \n",
    "        xs = []\n",
    "        ys = []\n",
    "        for w in self.words:\n",
    "            wdot = f'.{w}.'\n",
    "            for i in range(len(wdot) - 3):\n",
    "                xs.append([stoi[wdot[i]], stoi[wdot[i+1]], stoi[wdot[i+2]]])\n",
    "                ys.append(stoi[wdot[i+3]])\n",
    "\n",
    "        self.xs = torch.tensor(xs)\n",
    "        self.ys = torch.tensor(ys)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.xs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.xs[idx], self.ys[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131264 16408 16408\n"
     ]
    }
   ],
   "source": [
    "full_data = WordsDataset('names.txt')\n",
    "train_data, dev_data, test_data = torch.utils.data.random_split(\n",
    "    full_data, [0.8, 0.1, 0.1]\n",
    ")\n",
    "print(len(train_data), len(dev_data), len(test_data))\n",
    "train_dl = DataLoader(train_data, batch_size=16, shuffle=True)\n",
    "dev_dl = DataLoader(dev_data, batch_size=16, shuffle=True)\n",
    "test_dl = DataLoader(test_data, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(xb: torch.Tensor):\n",
    "    emb = C[xb].view(-1, 6)\n",
    "    h = torch.tanh((emb @ W1) + B1)\n",
    "    return (h @ W2) + B2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8204"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_b = []\n",
    "batches = []\n",
    "\n",
    "batch_i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2):\n",
    "    for batch in train_dl:\n",
    "        xb, yb = batch\n",
    "        preds = model(xb)\n",
    "        loss = F.cross_entropy(preds, yb)\n",
    "\n",
    "        for p in parameters:\n",
    "            p.grad = None\n",
    "        loss.backward()\n",
    "\n",
    "        # lr = 0.1\n",
    "        lr = 0.01\n",
    "        for p in parameters:\n",
    "            p.data -= lr * p.grad\n",
    "\n",
    "        losses_b.append(loss.log10().item())\n",
    "        batches.append(batch_i)\n",
    "        batch_i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fd97843e750>]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(batches, losses_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.4600, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg dev loss: 2.302677301286954\n"
     ]
    }
   ],
   "source": [
    "tot_loss = 0\n",
    "for batch in dev_dl:\n",
    "    xb, yb = batch\n",
    "    preds = model(xb)\n",
    "    loss = F.cross_entropy(preds, yb)\n",
    "    tot_loss += loss.item()\n",
    "\n",
    "print(f\"Avg dev loss: {tot_loss / len(dev_dl)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize dimensions 0 and 1 of the embedding matrix C for all characters\n",
    "# vowels are clustered, especially aeio\n",
    "# .is totally separate\n",
    "# Model has learned these patterns via embedding\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.scatter(C[:,0].data, C[:,1].data, s=200)\n",
    "for i in range(C.shape[0]):\n",
    "    plt.text(C[i,0].item(), C[i,1].item(), itos[i], ha=\"center\", va=\"center\", color='white')\n",
    "plt.grid('minor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expanding params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "embedding_size = 20\n",
    "hidden_neurons = 200\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "train_dl = DataLoader(train_data, batch_size=batch_size, shuffle=True, generator=g)\n",
    "dev_dl = DataLoader(dev_data, batch_size=batch_size, shuffle=True, generator=g)\n",
    "test_dl = DataLoader(test_data, batch_size=batch_size, shuffle=True, generator=g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate param initialization\n",
    "C = torch.randn((len(letters) + 1, embedding_size), generator=g)\n",
    "# is num chars we picked per group\n",
    "W1 = torch.randn(3*embedding_size, hidden_neurons, generator=g)\n",
    "B1 = torch.randn(hidden_neurons, generator=g)\n",
    "W2 = torch.randn(hidden_neurons, len(letters) + 1, generator=g)\n",
    "B2 = torch.randn(len(letters) + 1, generator=g)\n",
    "parameters = [C, W1, B1, W2, B2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18167"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Total model size\n",
    "sum(p.nelement() for p in parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(xb: torch.Tensor):\n",
    "    emb = C[xb].view(-1, 3*embedding_size)\n",
    "    h = torch.tanh((emb @ W1) + B1)\n",
    "    return (h @ W2) + B2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_b = []\n",
    "batches = []\n",
    "\n",
    "batch_i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    for batch in train_dl:\n",
    "        xb, yb = batch\n",
    "        preds = model(xb)\n",
    "        loss = F.cross_entropy(preds, yb)\n",
    "\n",
    "        for p in parameters:\n",
    "            p.grad = None\n",
    "        loss.backward()\n",
    "\n",
    "        lr = 0.1\n",
    "        if i > 5 and i < 10:\n",
    "            lr = 0.01\n",
    "        if i >= 10 and i < 15:\n",
    "            lr = 0.001\n",
    "        if i >= 15:\n",
    "            lr = 0.0005\n",
    "        for p in parameters:\n",
    "            p.data -= lr * p.grad\n",
    "\n",
    "        losses_b.append(loss.log10().item())\n",
    "        batches.append(batch_i)\n",
    "        batch_i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 2.4385437965393066\n",
      "Avg dev loss: 2.102236957926499\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fd973afe750>]"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(f\"Train loss: {loss}\")\n",
    "\n",
    "tot_loss = 0\n",
    "for batch in dev_dl:\n",
    "    xb, yb = batch\n",
    "    preds = model(xb)\n",
    "    loss = F.cross_entropy(preds, yb)\n",
    "    tot_loss += loss.item()\n",
    "\n",
    "print(f\"Avg dev loss: {tot_loss / len(dev_dl)}\")\n",
    "\n",
    "plt.plot(batches, losses_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best dev loss:\n",
    "\n",
    "2.200\n",
    "embedding_size = 10\n",
    "hidden_neurons = 200\n",
    "batch_size = 64\n",
    "\n",
    "2.11\n",
    "embedding_size = 10\n",
    "hidden_neurons = 200\n",
    "batch_size = 16\n",
    "\n",
    "2.1022\n",
    "embedding_size = 20\n",
    "hidden_neurons = 200\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fianne\n",
      "fxxelisson\n",
      "fzoretaj\n",
      "fianna\n",
      "fyse\n",
      "qqqeylond\n",
      "fxila\n",
      "fylenogjton\n",
      "qqqeysie\n",
      "ydane\n",
      "qqqewy\n",
      "fza\n",
      "fyfancie\n",
      "fynn\n",
      "fxansh\n",
      "cdaiya\n",
      "yden\n",
      "qqqeerayce\n",
      "fzyenae\n",
      "fylia\n"
     ]
    }
   ],
   "source": [
    "words_to_gen = 20\n",
    "for wi in range(words_to_gen):\n",
    "    li = 0\n",
    "    next_letter = ''\n",
    "    genned_word = '...'\n",
    "    while next_letter != '.' and len(genned_word) < 20:\n",
    "        prior_group = []\n",
    "        prior_group.append(stoi[genned_word[li]])\n",
    "        prior_group.append(stoi[genned_word[li+1]])\n",
    "        prior_group.append(stoi[genned_word[li+2]])\n",
    "\n",
    "        probs = F.softmax(model(torch.tensor(prior_group)), dim=1)\n",
    "        pred = torch.multinomial(probs, 1, replacement=True, generator=g).item()\n",
    "        next_letter = itos[int(pred)]\n",
    "\n",
    "        genned_word += next_letter\n",
    "        li += 1\n",
    "    print(genned_word.strip('.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0]"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[0] * 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ta'"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "itos[20] + itos[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mot.\n",
      "yriel.\n",
      "im.\n",
      "mia.\n",
      "iah.\n",
      "mandran.\n",
      "h.\n",
      "igula.\n",
      "yvi.\n",
      "yacoby.\n",
      "dijeylah.\n",
      "ylenn.\n",
      "tlius.\n",
      "chi.\n",
      "ysolardaw.\n",
      "v.\n",
      "daydyn.\n",
      "dyowecfanvan.\n",
      "rvin.\n",
      "irsh.\n"
     ]
    }
   ],
   "source": [
    "for _ in range(20):\n",
    "    \n",
    "    out = []\n",
    "    context = [0, 20, 1] # initialize\n",
    "    while True:\n",
    "      probs = F.softmax(model(torch.tensor([context])), dim=1)\n",
    "      ix = torch.multinomial(probs, 1, replacement=True, generator=g).item()  \n",
    "      context = context[1:] + [ix]\n",
    "      out.append(ix)\n",
    "      if ix == 0:\n",
    "        break\n",
    "    \n",
    "    print(''.join(itos[i] for i in out))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
