{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella', 'sophia']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = open('names.txt').read().splitlines()\n",
    "words[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e m\n",
      "m m\n",
      "m a\n"
     ]
    }
   ],
   "source": [
    "for word in words[:1]:\n",
    "    for char, char1 in zip(word, word[1:]):\n",
    "        print(char, char1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 1,\n",
       " 'b': 2,\n",
       " 'c': 3,\n",
       " 'd': 4,\n",
       " 'e': 5,\n",
       " 'f': 6,\n",
       " 'g': 7,\n",
       " 'h': 8,\n",
       " 'i': 9,\n",
       " 'j': 10,\n",
       " 'k': 11,\n",
       " 'l': 12,\n",
       " 'm': 13,\n",
       " 'n': 14,\n",
       " 'o': 15,\n",
       " 'p': 16,\n",
       " 'q': 17,\n",
       " 'r': 18,\n",
       " 's': 19,\n",
       " 't': 20,\n",
       " 'u': 21,\n",
       " 'v': 22,\n",
       " 'w': 23,\n",
       " 'x': 24,\n",
       " 'y': 25,\n",
       " 'z': 26,\n",
       " '.': 0}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "letters = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i, s in enumerate(letters)}\n",
    "stoi['.'] = 0\n",
    "stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'a',\n",
       " 2: 'b',\n",
       " 3: 'c',\n",
       " 4: 'd',\n",
       " 5: 'e',\n",
       " 6: 'f',\n",
       " 7: 'g',\n",
       " 8: 'h',\n",
       " 9: 'i',\n",
       " 10: 'j',\n",
       " 11: 'k',\n",
       " 12: 'l',\n",
       " 13: 'm',\n",
       " 14: 'n',\n",
       " 15: 'o',\n",
       " 16: 'p',\n",
       " 17: 'q',\n",
       " 18: 'r',\n",
       " 19: 's',\n",
       " 20: 't',\n",
       " 21: 'u',\n",
       " 22: 'v',\n",
       " 23: 'w',\n",
       " 24: 'x',\n",
       " 25: 'y',\n",
       " 26: 'z',\n",
       " 0: '.'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "itos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = torch.zeros((27,27), dtype=torch.int64)\n",
    "N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = {}\n",
    "for word in words:\n",
    "    w_start_end = '.' + word + '.'\n",
    "    for char, char1 in zip(w_start_end, w_start_end[1:]):\n",
    "        counts[(char, char1)] = counts.get((char, char1), 0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 556, 4410, 1306, 1542, 1690, 1531,  417,  669,  874,  591, 2422, 2963,\n",
       "         1572, 2538, 1146,  394,  515,   92, 1639, 2055, 1308,   78,  376,  307,\n",
       "          134,  535,  929],\n",
       "        [6640,  556,  541,  470, 1042,  692,  134,  168, 2332, 1650,  175,  568,\n",
       "         2528, 1634, 5438,   63,   82,   60, 3264, 1118,  687,  381,  834,  161,\n",
       "          182, 2050,  435],\n",
       "        [ 114,  321,   38,    1,   65,  655,    2,  664,   41,  217,    1,  116,\n",
       "          103,    0,    4,  105,   11,   76,  842,    8,    2,   45,    0,    3,\n",
       "          104,   83,    0],\n",
       "        [  97,  815,    3,   42,    1,  551,   25,    2,  664,  271,    3,  316,\n",
       "          116,   31,  378,  380,    1,   11,   76,    5,   35,   35,   23,    0,\n",
       "            3,  104,    4],\n",
       "        [ 516, 1303,    1,    3,  149, 1283,    5,   25,  118,  674,    9,    3,\n",
       "           60,   30,   31,  378,   14,    1,  424,   29,    4,   92,   17,   23,\n",
       "         1070,  317,    1],\n",
       "        [3983,  679,  121,  153,  384, 1271,   82,  125,  152,  818,   55,  178,\n",
       "         3248,  769, 2675,  269,   83,   14, 1958,  861,  580,   69,  463,   50,\n",
       "          132, 1070,  181],\n",
       "        [  80,  242,    0,   19,  334,  123,   44,    1,    1,  160,    0,    2,\n",
       "           20,   27,    4,   60,    0,  201,  114,    6,   18,   10,   26,    4,\n",
       "           31,   14,    2],\n",
       "        [ 108,  330,    3,   24,   19,  334,    1,   25,  360,  190,    3,  185,\n",
       "           32,    6,   27,   83,    1,  204,  201,   30,   31,   85,    1,   26,\n",
       "          213,   31,    1],\n",
       "        [2409, 2244,    8,    2,   24,  674,    2,    2,    1,  729,    9,   29,\n",
       "          185,  117,  138,  287,    1,    1,  204,   31,   71,  166,   39,   10,\n",
       "          779,  213,   20],\n",
       "        [2489, 2445,  110,  509,  440, 1653,  101,  428,   95,   82,   76,  445,\n",
       "         1345,  427, 2126,  588,   53,   52,  849, 1316,  541,  109,  269,    8,\n",
       "           89,  779,  277],\n",
       "        [  71, 1473,    1,    4,    4,  440,    0,  307,   45,  119,    2,    2,\n",
       "            9,    5,    2,  479,    1,  109,   11,    7,    2,  202,    5,    6,\n",
       "          379,   10,    0],\n",
       "        [ 363, 1731,    2,    2,    2,  895,    1,   19,  307,  509,    2,   20,\n",
       "          139,    9,   26,  344,    3,   18,  109,   95,   17,   50,    2,   34,\n",
       "         1588,  379,    2],\n",
       "        [1314, 2623,   52,   25,  138, 2921,   22,    6,   19, 2480,    6,   24,\n",
       "         1345,   60,   14,  692,   15,    3,   18,   94,   77,  324,   72,   16,\n",
       "          287, 1588,   10],\n",
       "        [ 516, 2590,  112,   51,   24,  818,    1,   26,    5, 1256,    7,    1,\n",
       "            5,  168,   20,  452,   38,   44,   97,   35,    4,  139,    3,    2,\n",
       "          465,  287,   11],\n",
       "        [6763, 2977,    8,  213,  704, 1359,   11,  273,   26, 1725,   44,   58,\n",
       "          195,   19, 1906,  496,    5,    2,   44,  278,  443,   96,   55,   11,\n",
       "            6,  465,  145],\n",
       "        [ 855,  149,  140,  114,  190,  132,   34,   44,  171,   69,   16,   68,\n",
       "          619,  261, 2411,  115,   95,    3, 1059,  504,  118,  275,  176,  114,\n",
       "           45,  103,   54],\n",
       "        [  33,  209,    2,    1,    1,  197,    1,    0,  204,   61,    1,    1,\n",
       "           16,    1,    1,   59,   39,    1,  151,   16,   17,    4,    3,    0,\n",
       "            0,   12,    0],\n",
       "        [  28,   13,   99,  187, 1697,    1,   76,  121, 3033,   13,   90,  413,\n",
       "            1,    2,  869,    2,   16,  425,    1,    2,  252,  206,   21,    3,\n",
       "          773,   23,    0],\n",
       "        [1377, 2356,   41,   99,  187, 1697,    9,   76,  121, 3033,   25,   90,\n",
       "          413,  162,  140,  869,   14,   16,  425,  190,  208,  252,   80,   21,\n",
       "            3,  773,   23],\n",
       "        [1169, 1201,   21,   60,    9,  884,    2,    2, 1285,  684,    2,   82,\n",
       "          279,   90,   24,  531,   51,    1,   55,  461,  765,  185,   14,   24,\n",
       "          341,  215,   10],\n",
       "        [ 483, 1027,    1,   17,  169,  716,    2,    2,  647,  532,    3,  301,\n",
       "          134,    4,   22,  667,   10,  414,  352,   35,  374,   78,   15,   11,\n",
       "            2,  341,  105],\n",
       "        [ 155,  163,  103,  103,  136,  169,   19,   47,   58,  121,   14,   93,\n",
       "          301,  154,  275,   10,   16,   10,  414,  474,   82,    3,   37,   86,\n",
       "           34,   13,   45],\n",
       "        [  88,  642,    1,    8,    1,  568,    1,   23,    1,  911,    6,    3,\n",
       "           14,   58,    8,  153,    0,   22,   48,    8,   25,    7,    7,    0,\n",
       "           73,  121,    0],\n",
       "        [  51,  280,    1,    5,    8,  149,    2,    1,   23,  148,    0,    6,\n",
       "           13,    2,   58,   36,    0,    0,   22,   20,    8,   25,    3,    2,\n",
       "           30,   73,    1],\n",
       "        [ 164,  103,    1,    4,    5,   36,    3,   22,    1,  102,   86, 1104,\n",
       "           39,    1,    1,   41,    6,  291,  401,   31,   70,    5,    4,    3,\n",
       "           38,   30,   19],\n",
       "        [2007, 2143,   27,  115,  272,  301,   12,   30,   22,  192,   23,   86,\n",
       "         1104,  148, 1826,  271,   15,    6,  291,  401,  104,  141,  106,    4,\n",
       "           28,   23,   78],\n",
       "        [ 160,  860,    4,    2,    2,  373,    0,    1,   43,  364,    2,    2,\n",
       "          123,   35,    4,  110,    2,    0,   32,    4,    4,   73,    2,    3,\n",
       "            1,  147,   45]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for pair, freq in counts.items():\n",
    "    i_one = stoi[pair[0]]\n",
    "    i_two = stoi[pair[1]]\n",
    "    N[i_one, i_two] = freq\n",
    "N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.2417e-03, 1.7781e-02, 5.2657e-03, 6.2172e-03, 6.8139e-03, 6.1728e-03,\n",
       "         1.6813e-03, 2.6973e-03, 3.5239e-03, 2.3829e-03, 9.7653e-03, 1.1947e-02,\n",
       "         6.3381e-03, 1.0233e-02, 4.6206e-03, 1.5886e-03, 2.0764e-03, 3.7093e-04,\n",
       "         6.6083e-03, 8.2856e-03, 5.2737e-03, 3.1449e-04, 1.5160e-03, 1.2378e-03,\n",
       "         5.4027e-04, 2.1571e-03, 3.7456e-03],\n",
       "        [2.6772e-02, 2.2417e-03, 2.1813e-03, 1.8950e-03, 4.2012e-03, 2.7901e-03,\n",
       "         5.4027e-04, 6.7736e-04, 9.4024e-03, 6.6526e-03, 7.0558e-04, 2.2901e-03,\n",
       "         1.0193e-02, 6.5881e-03, 2.1925e-02, 2.5401e-04, 3.3062e-04, 2.4191e-04,\n",
       "         1.3160e-02, 4.5077e-03, 2.7699e-03, 1.5362e-03, 3.3626e-03, 6.4914e-04,\n",
       "         7.3381e-04, 8.2654e-03, 1.7539e-03],\n",
       "        [4.5964e-04, 1.2942e-03, 1.5321e-04, 4.0319e-06, 2.6207e-04, 2.6409e-03,\n",
       "         8.0638e-06, 2.6772e-03, 1.6531e-04, 8.7492e-04, 4.0319e-06, 4.6770e-04,\n",
       "         4.1529e-04, 0.0000e+00, 1.6128e-05, 4.2335e-04, 4.4351e-05, 3.0642e-04,\n",
       "         3.3949e-03, 3.2255e-05, 8.0638e-06, 1.8144e-04, 0.0000e+00, 1.2096e-05,\n",
       "         4.1932e-04, 3.3465e-04, 0.0000e+00],\n",
       "        [3.9109e-04, 3.2860e-03, 1.2096e-05, 1.6934e-04, 4.0319e-06, 2.2216e-03,\n",
       "         1.0080e-04, 8.0638e-06, 2.6772e-03, 1.0926e-03, 1.2096e-05, 1.2741e-03,\n",
       "         4.6770e-04, 1.2499e-04, 1.5241e-03, 1.5321e-03, 4.0319e-06, 4.4351e-05,\n",
       "         3.0642e-04, 2.0160e-05, 1.4112e-04, 1.4112e-04, 9.2734e-05, 0.0000e+00,\n",
       "         1.2096e-05, 4.1932e-04, 1.6128e-05],\n",
       "        [2.0805e-03, 5.2536e-03, 4.0319e-06, 1.2096e-05, 6.0075e-04, 5.1729e-03,\n",
       "         2.0160e-05, 1.0080e-04, 4.7576e-04, 2.7175e-03, 3.6287e-05, 1.2096e-05,\n",
       "         2.4191e-04, 1.2096e-04, 1.2499e-04, 1.5241e-03, 5.6447e-05, 4.0319e-06,\n",
       "         1.7095e-03, 1.1693e-04, 1.6128e-05, 3.7093e-04, 6.8542e-05, 9.2734e-05,\n",
       "         4.3141e-03, 1.2781e-03, 4.0319e-06],\n",
       "        [1.6059e-02, 2.7377e-03, 4.8786e-04, 6.1688e-04, 1.5482e-03, 5.1245e-03,\n",
       "         3.3062e-04, 5.0399e-04, 6.1285e-04, 3.2981e-03, 2.2175e-04, 7.1768e-04,\n",
       "         1.3096e-02, 3.1005e-03, 1.0785e-02, 1.0846e-03, 3.3465e-04, 5.6447e-05,\n",
       "         7.8945e-03, 3.4715e-03, 2.3385e-03, 2.7820e-04, 1.8668e-03, 2.0160e-04,\n",
       "         5.3221e-04, 4.3141e-03, 7.2977e-04],\n",
       "        [3.2255e-04, 9.7572e-04, 0.0000e+00, 7.6606e-05, 1.3467e-03, 4.9592e-04,\n",
       "         1.7740e-04, 4.0319e-06, 4.0319e-06, 6.4510e-04, 0.0000e+00, 8.0638e-06,\n",
       "         8.0638e-05, 1.0886e-04, 1.6128e-05, 2.4191e-04, 0.0000e+00, 8.1041e-04,\n",
       "         4.5964e-04, 2.4191e-05, 7.2574e-05, 4.0319e-05, 1.0483e-04, 1.6128e-05,\n",
       "         1.2499e-04, 5.6447e-05, 8.0638e-06],\n",
       "        [4.3545e-04, 1.3305e-03, 1.2096e-05, 9.6766e-05, 7.6606e-05, 1.3467e-03,\n",
       "         4.0319e-06, 1.0080e-04, 1.4515e-03, 7.6606e-04, 1.2096e-05, 7.4590e-04,\n",
       "         1.2902e-04, 2.4191e-05, 1.0886e-04, 3.3465e-04, 4.0319e-06, 8.2251e-04,\n",
       "         8.1041e-04, 1.2096e-04, 1.2499e-04, 3.4271e-04, 4.0319e-06, 1.0483e-04,\n",
       "         8.5879e-04, 1.2499e-04, 4.0319e-06],\n",
       "        [9.7128e-03, 9.0476e-03, 3.2255e-05, 8.0638e-06, 9.6766e-05, 2.7175e-03,\n",
       "         8.0638e-06, 8.0638e-06, 4.0319e-06, 2.9393e-03, 3.6287e-05, 1.1693e-04,\n",
       "         7.4590e-04, 4.7173e-04, 5.5640e-04, 1.1572e-03, 4.0319e-06, 4.0319e-06,\n",
       "         8.2251e-04, 1.2499e-04, 2.8626e-04, 6.6930e-04, 1.5724e-04, 4.0319e-05,\n",
       "         3.1409e-03, 8.5879e-04, 8.0638e-05],\n",
       "        [1.0035e-02, 9.8580e-03, 4.4351e-04, 2.0522e-03, 1.7740e-03, 6.6647e-03,\n",
       "         4.0722e-04, 1.7257e-03, 3.8303e-04, 3.3062e-04, 3.0642e-04, 1.7942e-03,\n",
       "         5.4229e-03, 1.7216e-03, 8.5718e-03, 2.3708e-03, 2.1369e-04, 2.0966e-04,\n",
       "         3.4231e-03, 5.3060e-03, 2.1813e-03, 4.3948e-04, 1.0846e-03, 3.2255e-05,\n",
       "         3.5884e-04, 3.1409e-03, 1.1168e-03],\n",
       "        [2.8626e-04, 5.9390e-03, 4.0319e-06, 1.6128e-05, 1.6128e-05, 1.7740e-03,\n",
       "         0.0000e+00, 1.2378e-03, 1.8144e-04, 4.7980e-04, 8.0638e-06, 8.0638e-06,\n",
       "         3.6287e-05, 2.0160e-05, 8.0638e-06, 1.9313e-03, 4.0319e-06, 4.3948e-04,\n",
       "         4.4351e-05, 2.8223e-05, 8.0638e-06, 8.1444e-04, 2.0160e-05, 2.4191e-05,\n",
       "         1.5281e-03, 4.0319e-05, 0.0000e+00],\n",
       "        [1.4636e-03, 6.9792e-03, 8.0638e-06, 8.0638e-06, 8.0638e-06, 3.6086e-03,\n",
       "         4.0319e-06, 7.6606e-05, 1.2378e-03, 2.0522e-03, 8.0638e-06, 8.0638e-05,\n",
       "         5.6043e-04, 3.6287e-05, 1.0483e-04, 1.3870e-03, 1.2096e-05, 7.2574e-05,\n",
       "         4.3948e-04, 3.8303e-04, 6.8542e-05, 2.0160e-04, 8.0638e-06, 1.3708e-04,\n",
       "         6.4027e-03, 1.5281e-03, 8.0638e-06],\n",
       "        [5.2979e-03, 1.0576e-02, 2.0966e-04, 1.0080e-04, 5.5640e-04, 1.1777e-02,\n",
       "         8.8702e-05, 2.4191e-05, 7.6606e-05, 9.9991e-03, 2.4191e-05, 9.6766e-05,\n",
       "         5.4229e-03, 2.4191e-04, 5.6447e-05, 2.7901e-03, 6.0479e-05, 1.2096e-05,\n",
       "         7.2574e-05, 3.7900e-04, 3.1046e-04, 1.3063e-03, 2.9030e-04, 6.4510e-05,\n",
       "         1.1572e-03, 6.4027e-03, 4.0319e-05],\n",
       "        [2.0805e-03, 1.0443e-02, 4.5157e-04, 2.0563e-04, 9.6766e-05, 3.2981e-03,\n",
       "         4.0319e-06, 1.0483e-04, 2.0160e-05, 5.0641e-03, 2.8223e-05, 4.0319e-06,\n",
       "         2.0160e-05, 6.7736e-04, 8.0638e-05, 1.8224e-03, 1.5321e-04, 1.7740e-04,\n",
       "         3.9109e-04, 1.4112e-04, 1.6128e-05, 5.6043e-04, 1.2096e-05, 8.0638e-06,\n",
       "         1.8748e-03, 1.1572e-03, 4.4351e-05],\n",
       "        [2.7268e-02, 1.2003e-02, 3.2255e-05, 8.5879e-04, 2.8385e-03, 5.4794e-03,\n",
       "         4.4351e-05, 1.1007e-03, 1.0483e-04, 6.9550e-03, 1.7740e-04, 2.3385e-04,\n",
       "         7.8622e-04, 7.6606e-05, 7.6848e-03, 1.9998e-03, 2.0160e-05, 8.0638e-06,\n",
       "         1.7740e-04, 1.1209e-03, 1.7861e-03, 3.8706e-04, 2.2175e-04, 4.4351e-05,\n",
       "         2.4191e-05, 1.8748e-03, 5.8463e-04],\n",
       "        [3.4473e-03, 6.0075e-04, 5.6447e-04, 4.5964e-04, 7.6606e-04, 5.3221e-04,\n",
       "         1.3708e-04, 1.7740e-04, 6.8945e-04, 2.7820e-04, 6.4510e-05, 2.7417e-04,\n",
       "         2.4957e-03, 1.0523e-03, 9.7209e-03, 4.6367e-04, 3.8303e-04, 1.2096e-05,\n",
       "         4.2698e-03, 2.0321e-03, 4.7576e-04, 1.1088e-03, 7.0961e-04, 4.5964e-04,\n",
       "         1.8144e-04, 4.1529e-04, 2.1772e-04],\n",
       "        [1.3305e-04, 8.4267e-04, 8.0638e-06, 4.0319e-06, 4.0319e-06, 7.9428e-04,\n",
       "         4.0319e-06, 0.0000e+00, 8.2251e-04, 2.4595e-04, 4.0319e-06, 4.0319e-06,\n",
       "         6.4510e-05, 4.0319e-06, 4.0319e-06, 2.3788e-04, 1.5724e-04, 4.0319e-06,\n",
       "         6.0882e-04, 6.4510e-05, 6.8542e-05, 1.6128e-05, 1.2096e-05, 0.0000e+00,\n",
       "         0.0000e+00, 4.8383e-05, 0.0000e+00],\n",
       "        [1.1289e-04, 5.2415e-05, 3.9916e-04, 7.5397e-04, 6.8421e-03, 4.0319e-06,\n",
       "         3.0642e-04, 4.8786e-04, 1.2229e-02, 5.2415e-05, 3.6287e-04, 1.6652e-03,\n",
       "         4.0319e-06, 8.0638e-06, 3.5037e-03, 8.0638e-06, 6.4510e-05, 1.7136e-03,\n",
       "         4.0319e-06, 8.0638e-06, 1.0160e-03, 8.3057e-04, 8.4670e-05, 1.2096e-05,\n",
       "         3.1167e-03, 9.2734e-05, 0.0000e+00],\n",
       "        [5.5519e-03, 9.4992e-03, 1.6531e-04, 3.9916e-04, 7.5397e-04, 6.8421e-03,\n",
       "         3.6287e-05, 3.0642e-04, 4.8786e-04, 1.2229e-02, 1.0080e-04, 3.6287e-04,\n",
       "         1.6652e-03, 6.5317e-04, 5.6447e-04, 3.5037e-03, 5.6447e-05, 6.4510e-05,\n",
       "         1.7136e-03, 7.6606e-04, 8.3864e-04, 1.0160e-03, 3.2255e-04, 8.4670e-05,\n",
       "         1.2096e-05, 3.1167e-03, 9.2734e-05],\n",
       "        [4.7133e-03, 4.8423e-03, 8.4670e-05, 2.4191e-04, 3.6287e-05, 3.5642e-03,\n",
       "         8.0638e-06, 8.0638e-06, 5.1810e-03, 2.7578e-03, 8.0638e-06, 3.3062e-04,\n",
       "         1.1249e-03, 3.6287e-04, 9.6766e-05, 2.1409e-03, 2.0563e-04, 4.0319e-06,\n",
       "         2.2175e-04, 1.8587e-03, 3.0844e-03, 7.4590e-04, 5.6447e-05, 9.6766e-05,\n",
       "         1.3749e-03, 8.6686e-04, 4.0319e-05],\n",
       "        [1.9474e-03, 4.1408e-03, 4.0319e-06, 6.8542e-05, 6.8139e-04, 2.8868e-03,\n",
       "         8.0638e-06, 8.0638e-06, 2.6086e-03, 2.1450e-03, 1.2096e-05, 1.2136e-03,\n",
       "         5.4027e-04, 1.6128e-05, 8.8702e-05, 2.6893e-03, 4.0319e-05, 1.6692e-03,\n",
       "         1.4192e-03, 1.4112e-04, 1.5079e-03, 3.1449e-04, 6.0479e-05, 4.4351e-05,\n",
       "         8.0638e-06, 1.3749e-03, 4.2335e-04],\n",
       "        [6.2494e-04, 6.5720e-04, 4.1529e-04, 4.1529e-04, 5.4834e-04, 6.8139e-04,\n",
       "         7.6606e-05, 1.8950e-04, 2.3385e-04, 4.8786e-04, 5.6447e-05, 3.7497e-04,\n",
       "         1.2136e-03, 6.2091e-04, 1.1088e-03, 4.0319e-05, 6.4510e-05, 4.0319e-05,\n",
       "         1.6692e-03, 1.9111e-03, 3.3062e-04, 1.2096e-05, 1.4918e-04, 3.4674e-04,\n",
       "         1.3708e-04, 5.2415e-05, 1.8144e-04],\n",
       "        [3.5481e-04, 2.5885e-03, 4.0319e-06, 3.2255e-05, 4.0319e-06, 2.2901e-03,\n",
       "         4.0319e-06, 9.2734e-05, 4.0319e-06, 3.6731e-03, 2.4191e-05, 1.2096e-05,\n",
       "         5.6447e-05, 2.3385e-04, 3.2255e-05, 6.1688e-04, 0.0000e+00, 8.8702e-05,\n",
       "         1.9353e-04, 3.2255e-05, 1.0080e-04, 2.8223e-05, 2.8223e-05, 0.0000e+00,\n",
       "         2.9433e-04, 4.8786e-04, 0.0000e+00],\n",
       "        [2.0563e-04, 1.1289e-03, 4.0319e-06, 2.0160e-05, 3.2255e-05, 6.0075e-04,\n",
       "         8.0638e-06, 4.0319e-06, 9.2734e-05, 5.9672e-04, 0.0000e+00, 2.4191e-05,\n",
       "         5.2415e-05, 8.0638e-06, 2.3385e-04, 1.4515e-04, 0.0000e+00, 0.0000e+00,\n",
       "         8.8702e-05, 8.0638e-05, 3.2255e-05, 1.0080e-04, 1.2096e-05, 8.0638e-06,\n",
       "         1.2096e-04, 2.9433e-04, 4.0319e-06],\n",
       "        [6.6123e-04, 4.1529e-04, 4.0319e-06, 1.6128e-05, 2.0160e-05, 1.4515e-04,\n",
       "         1.2096e-05, 8.8702e-05, 4.0319e-06, 4.1125e-04, 3.4674e-04, 4.4512e-03,\n",
       "         1.5724e-04, 4.0319e-06, 4.0319e-06, 1.6531e-04, 2.4191e-05, 1.1733e-03,\n",
       "         1.6168e-03, 1.2499e-04, 2.8223e-04, 2.0160e-05, 1.6128e-05, 1.2096e-05,\n",
       "         1.5321e-04, 1.2096e-04, 7.6606e-05],\n",
       "        [8.0920e-03, 8.6404e-03, 1.0886e-04, 4.6367e-04, 1.0967e-03, 1.2136e-03,\n",
       "         4.8383e-05, 1.2096e-04, 8.8702e-05, 7.7412e-04, 9.2734e-05, 3.4674e-04,\n",
       "         4.4512e-03, 5.9672e-04, 7.3623e-03, 1.0926e-03, 6.0479e-05, 2.4191e-05,\n",
       "         1.1733e-03, 1.6168e-03, 4.1932e-04, 5.6850e-04, 4.2738e-04, 1.6128e-05,\n",
       "         1.1289e-04, 9.2734e-05, 3.1449e-04],\n",
       "        [6.4510e-04, 3.4674e-03, 1.6128e-05, 8.0638e-06, 8.0638e-06, 1.5039e-03,\n",
       "         0.0000e+00, 4.0319e-06, 1.7337e-04, 1.4676e-03, 8.0638e-06, 8.0638e-06,\n",
       "         4.9592e-04, 1.4112e-04, 1.6128e-05, 4.4351e-04, 8.0638e-06, 0.0000e+00,\n",
       "         1.2902e-04, 1.6128e-05, 1.6128e-05, 2.9433e-04, 8.0638e-06, 1.2096e-05,\n",
       "         4.0319e-06, 5.9269e-04, 1.8144e-04]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = N / N.sum()\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[20,  1, 14, 11, 20, 11,  1,  4, 13, 10],\n",
       "        [ 0, 14, 14, 18, 18,  8, 25,  1, 18,  0],\n",
       "        [18, 18, 18, 17,  7,  4, 18,  0,  5,  7],\n",
       "        [ 1,  1,  9,  5, 12,  5,  5, 15,  8,  1],\n",
       "        [ 5,  0, 18, 12,  8,  9,  5,  5, 24,  1],\n",
       "        [12,  0,  5, 12,  0, 14,  1, 12, 12, 18],\n",
       "        [19,  6,  9,  9,  1,  4,  4,  9,  5, 18],\n",
       "        [ 5, 11, 24,  8, 24, 20, 12, 15,  1, 17],\n",
       "        [ 1,  1,  0,  9,  0, 11,  5,  1,  1, 24],\n",
       "        [12, 14,  1, 13, 12, 12, 13, 14,  0, 14],\n",
       "        [ 1,  1,  7,  5,  5, 15,  1,  0,  7,  1],\n",
       "        [ 1, 21, 15,  1,  1,  1,  1, 24, 24,  0],\n",
       "        [ 1,  9,  5,  1,  5,  1, 25, 25,  9,  9],\n",
       "        [ 1,  1,  1, 24,  2,  1, 15,  5,  1,  1],\n",
       "        [ 1,  9,  0,  0,  0,  1,  0, 26,  0,  0],\n",
       "        [13, 14, 14, 19, 22, 14,  0, 12, 22, 14],\n",
       "        [ 8,  8,  0, 16,  1,  1,  8,  1,  1,  8],\n",
       "        [ 8, 14, 21, 14,  4, 24,  8,  8, 14,  8],\n",
       "        [ 9,  0,  9, 18, 15,  0,  9, 18,  3,  9],\n",
       "        [ 0,  1, 15,  0,  5,  5,  1, 21,  8,  1],\n",
       "        [ 9,  8,  1,  9, 15, 17,  1, 15,  1, 17],\n",
       "        [12,  4,  1, 19,  0, 12, 18, 19, 14, 20],\n",
       "        [ 5,  5,  5,  5,  5,  9,  1,  5,  9, 20],\n",
       "        [ 1,  9,  5,  1, 14, 20,  8, 14,  5,  1],\n",
       "        [18, 10, 18,  1, 11,  0,  0, 11, 11, 11],\n",
       "        [14,  0, 12, 12,  3,  1, 18,  1,  1, 14],\n",
       "        [ 5, 25, 13,  1,  9, 18,  0,  5,  1,  1]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2)\n",
    "samples = torch.multinomial(p, 10, replacement=True, generator=g)\n",
    "samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([27, 1])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_lens = [len(w) for w in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 556, 4410, 1306, 1542, 1690, 1531,  417,  669,  874,  591, 2422, 2963,\n",
       "         1572, 2538, 1146,  394,  515,   92, 1639, 2055, 1308,   78,  376,  307,\n",
       "          134,  535,  929],\n",
       "        [6640,  556,  541,  470, 1042,  692,  134,  168, 2332, 1650,  175,  568,\n",
       "         2528, 1634, 5438,   63,   82,   60, 3264, 1118,  687,  381,  834,  161,\n",
       "          182, 2050,  435],\n",
       "        [ 114,  321,   38,    1,   65,  655,    2,  664,   41,  217,    1,  116,\n",
       "          103,    0,    4,  105,   11,   76,  842,    8,    2,   45,    0,    3,\n",
       "          104,   83,    0],\n",
       "        [  97,  815,    3,   42,    1,  551,   25,    2,  664,  271,    3,  316,\n",
       "          116,   31,  378,  380,    1,   11,   76,    5,   35,   35,   23,    0,\n",
       "            3,  104,    4],\n",
       "        [ 516, 1303,    1,    3,  149, 1283,    5,   25,  118,  674,    9,    3,\n",
       "           60,   30,   31,  378,   14,    1,  424,   29,    4,   92,   17,   23,\n",
       "         1070,  317,    1],\n",
       "        [3983,  679,  121,  153,  384, 1271,   82,  125,  152,  818,   55,  178,\n",
       "         3248,  769, 2675,  269,   83,   14, 1958,  861,  580,   69,  463,   50,\n",
       "          132, 1070,  181],\n",
       "        [  80,  242,    0,   19,  334,  123,   44,    1,    1,  160,    0,    2,\n",
       "           20,   27,    4,   60,    0,  201,  114,    6,   18,   10,   26,    4,\n",
       "           31,   14,    2],\n",
       "        [ 108,  330,    3,   24,   19,  334,    1,   25,  360,  190,    3,  185,\n",
       "           32,    6,   27,   83,    1,  204,  201,   30,   31,   85,    1,   26,\n",
       "          213,   31,    1],\n",
       "        [2409, 2244,    8,    2,   24,  674,    2,    2,    1,  729,    9,   29,\n",
       "          185,  117,  138,  287,    1,    1,  204,   31,   71,  166,   39,   10,\n",
       "          779,  213,   20],\n",
       "        [2489, 2445,  110,  509,  440, 1653,  101,  428,   95,   82,   76,  445,\n",
       "         1345,  427, 2126,  588,   53,   52,  849, 1316,  541,  109,  269,    8,\n",
       "           89,  779,  277],\n",
       "        [  71, 1473,    1,    4,    4,  440,    0,  307,   45,  119,    2,    2,\n",
       "            9,    5,    2,  479,    1,  109,   11,    7,    2,  202,    5,    6,\n",
       "          379,   10,    0],\n",
       "        [ 363, 1731,    2,    2,    2,  895,    1,   19,  307,  509,    2,   20,\n",
       "          139,    9,   26,  344,    3,   18,  109,   95,   17,   50,    2,   34,\n",
       "         1588,  379,    2],\n",
       "        [1314, 2623,   52,   25,  138, 2921,   22,    6,   19, 2480,    6,   24,\n",
       "         1345,   60,   14,  692,   15,    3,   18,   94,   77,  324,   72,   16,\n",
       "          287, 1588,   10],\n",
       "        [ 516, 2590,  112,   51,   24,  818,    1,   26,    5, 1256,    7,    1,\n",
       "            5,  168,   20,  452,   38,   44,   97,   35,    4,  139,    3,    2,\n",
       "          465,  287,   11],\n",
       "        [6763, 2977,    8,  213,  704, 1359,   11,  273,   26, 1725,   44,   58,\n",
       "          195,   19, 1906,  496,    5,    2,   44,  278,  443,   96,   55,   11,\n",
       "            6,  465,  145],\n",
       "        [ 855,  149,  140,  114,  190,  132,   34,   44,  171,   69,   16,   68,\n",
       "          619,  261, 2411,  115,   95,    3, 1059,  504,  118,  275,  176,  114,\n",
       "           45,  103,   54],\n",
       "        [  33,  209,    2,    1,    1,  197,    1,    0,  204,   61,    1,    1,\n",
       "           16,    1,    1,   59,   39,    1,  151,   16,   17,    4,    3,    0,\n",
       "            0,   12,    0],\n",
       "        [  28,   13,   99,  187, 1697,    1,   76,  121, 3033,   13,   90,  413,\n",
       "            1,    2,  869,    2,   16,  425,    1,    2,  252,  206,   21,    3,\n",
       "          773,   23,    0],\n",
       "        [1377, 2356,   41,   99,  187, 1697,    9,   76,  121, 3033,   25,   90,\n",
       "          413,  162,  140,  869,   14,   16,  425,  190,  208,  252,   80,   21,\n",
       "            3,  773,   23],\n",
       "        [1169, 1201,   21,   60,    9,  884,    2,    2, 1285,  684,    2,   82,\n",
       "          279,   90,   24,  531,   51,    1,   55,  461,  765,  185,   14,   24,\n",
       "          341,  215,   10],\n",
       "        [ 483, 1027,    1,   17,  169,  716,    2,    2,  647,  532,    3,  301,\n",
       "          134,    4,   22,  667,   10,  414,  352,   35,  374,   78,   15,   11,\n",
       "            2,  341,  105],\n",
       "        [ 155,  163,  103,  103,  136,  169,   19,   47,   58,  121,   14,   93,\n",
       "          301,  154,  275,   10,   16,   10,  414,  474,   82,    3,   37,   86,\n",
       "           34,   13,   45],\n",
       "        [  88,  642,    1,    8,    1,  568,    1,   23,    1,  911,    6,    3,\n",
       "           14,   58,    8,  153,    0,   22,   48,    8,   25,    7,    7,    0,\n",
       "           73,  121,    0],\n",
       "        [  51,  280,    1,    5,    8,  149,    2,    1,   23,  148,    0,    6,\n",
       "           13,    2,   58,   36,    0,    0,   22,   20,    8,   25,    3,    2,\n",
       "           30,   73,    1],\n",
       "        [ 164,  103,    1,    4,    5,   36,    3,   22,    1,  102,   86, 1104,\n",
       "           39,    1,    1,   41,    6,  291,  401,   31,   70,    5,    4,    3,\n",
       "           38,   30,   19],\n",
       "        [2007, 2143,   27,  115,  272,  301,   12,   30,   22,  192,   23,   86,\n",
       "         1104,  148, 1826,  271,   15,    6,  291,  401,  104,  141,  106,    4,\n",
       "           28,   23,   78],\n",
       "        [ 160,  860,    4,    2,    2,  373,    0,    1,   43,  364,    2,    2,\n",
       "          123,   35,    4,  110,    2,    0,   32,    4,    4,   73,    2,    3,\n",
       "            1,  147,   45]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 1., 1., 1.],\n",
       "         [2., 2., 2., 2.]]),\n",
       " tensor([[4.],\n",
       "         [8.]]))"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ones = torch.ones(4)\n",
    "twos = torch.ones(4) * 2\n",
    "test = torch.stack([ones, twos])\n",
    "test, test.sum(dim=1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P = N.float()\n",
    "P /= P.sum(dim=1, keepdim=True)\n",
    "P[0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "junide\n",
      "janasah\n",
      "p\n",
      "cfqh\n",
      "a\n",
      "nn\n",
      "kxi\n",
      "ritolian\n",
      "jgee\n",
      "kxqhnaauranilevias\n",
      "dedainrwieta\n",
      "ssonielylarte\n",
      "faveumerifontume\n",
      "phynslenaruani\n",
      "core\n",
      "yaenon\n",
      "ka\n",
      "jabdinerimikimaynin\n",
      "anaasn\n",
      "ssorionsushxdxossmitqn\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "words_to_gen = 20\n",
    "for wi in range(words_to_gen):\n",
    "    # word_len = torch.randint(min(word_lens), max(word_lens), (1,)).item()\n",
    "    # for i in range(int(word_len)):\n",
    "    li = 0\n",
    "    next_letter = ''\n",
    "    genned_word = '.'\n",
    "    while next_letter != '.':\n",
    "        prior_letter = genned_word[li]\n",
    "        prior_letter_i = stoi[prior_letter]\n",
    "        next_sample_p = P[prior_letter_i]\n",
    "        next_letter_idx = torch.multinomial(\n",
    "                next_sample_p, 1, replacement=True, generator=g\n",
    "            ).item()\n",
    "        next_letter = itos[next_letter_idx]\n",
    "        genned_word += next_letter\n",
    "        li += 1\n",
    "    print(genned_word.strip('.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neg log likelihood: 570686.625\n",
      "avg neg log likelhood: 2.5014097690582275\n"
     ]
    }
   ],
   "source": [
    "n = 0\n",
    "log_likelihood = 0\n",
    "for word in words:\n",
    "    w_start_end = '.' + word + '.'\n",
    "    for char, char1 in zip(w_start_end, w_start_end[1:]):\n",
    "        prob = P[stoi[char], stoi[char1]]\n",
    "        log_likelihood += torch.log(prob)\n",
    "        n += 1\n",
    "        # print(f'{char}{char1}: {prob:.4f}')\n",
    "\n",
    "print(f\"neg log likelihood: {-log_likelihood}\")\n",
    "print(f\"avg neg log likelhood: {-log_likelihood / n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.e',\n",
       " 'em',\n",
       " 'mm',\n",
       " 'ma',\n",
       " 'a.',\n",
       " '.o',\n",
       " 'ol',\n",
       " 'li',\n",
       " 'iv',\n",
       " 'vi',\n",
       " 'ia',\n",
       " 'a.',\n",
       " '.a',\n",
       " 'av',\n",
       " 'va',\n",
       " 'a.',\n",
       " '.i',\n",
       " 'is',\n",
       " 'sa',\n",
       " 'ab',\n",
       " 'be',\n",
       " 'el',\n",
       " 'll',\n",
       " 'la',\n",
       " 'a.',\n",
       " '.s',\n",
       " 'so',\n",
       " 'op',\n",
       " 'ph',\n",
       " 'hi',\n",
       " 'ia',\n",
       " 'a.',\n",
       " '.c',\n",
       " 'ch',\n",
       " 'ha',\n",
       " 'ar',\n",
       " 'rl',\n",
       " 'lo',\n",
       " 'ot',\n",
       " 'tt',\n",
       " 'te',\n",
       " 'e.',\n",
       " '.m',\n",
       " 'mi',\n",
       " 'ia',\n",
       " 'a.',\n",
       " '.a',\n",
       " 'am',\n",
       " 'me',\n",
       " 'el',\n",
       " 'li',\n",
       " 'ia',\n",
       " 'a.',\n",
       " '.h',\n",
       " 'ha',\n",
       " 'ar',\n",
       " 'rp',\n",
       " 'pe',\n",
       " 'er',\n",
       " 'r.',\n",
       " '.e',\n",
       " 'ev',\n",
       " 've',\n",
       " 'el',\n",
       " 'ly',\n",
       " 'yn',\n",
       " 'n.',\n",
       " '.a',\n",
       " 'ab',\n",
       " 'bi',\n",
       " 'ig',\n",
       " 'ga',\n",
       " 'ai',\n",
       " 'il',\n",
       " 'l.',\n",
       " '.e',\n",
       " 'em',\n",
       " 'mi',\n",
       " 'il',\n",
       " 'ly',\n",
       " 'y.',\n",
       " '.e',\n",
       " 'el',\n",
       " 'li',\n",
       " 'iz',\n",
       " 'za',\n",
       " 'ab',\n",
       " 'be',\n",
       " 'et',\n",
       " 'th',\n",
       " 'h.',\n",
       " '.m',\n",
       " 'mi',\n",
       " 'il',\n",
       " 'la',\n",
       " 'a.',\n",
       " '.e',\n",
       " 'el',\n",
       " 'll',\n",
       " 'la',\n",
       " 'a.',\n",
       " '.a',\n",
       " 'av',\n",
       " 've',\n",
       " 'er',\n",
       " 'ry',\n",
       " 'y.',\n",
       " '.s',\n",
       " 'so',\n",
       " 'of',\n",
       " 'fi',\n",
       " 'ia',\n",
       " 'a.',\n",
       " '.c',\n",
       " 'ca',\n",
       " 'am',\n",
       " 'mi',\n",
       " 'il',\n",
       " 'la',\n",
       " 'a.',\n",
       " '.a',\n",
       " 'ar',\n",
       " 'ri',\n",
       " 'ia',\n",
       " 'a.',\n",
       " '.s',\n",
       " 'sc',\n",
       " 'ca',\n",
       " 'ar',\n",
       " 'rl',\n",
       " 'le',\n",
       " 'et',\n",
       " 'tt',\n",
       " 't.',\n",
       " '.v',\n",
       " 'vi',\n",
       " 'ic',\n",
       " 'ct',\n",
       " 'to',\n",
       " 'or',\n",
       " 'ri',\n",
       " 'ia',\n",
       " 'a.',\n",
       " '.m',\n",
       " 'ma',\n",
       " 'ad',\n",
       " 'di',\n",
       " 'is',\n",
       " 'so',\n",
       " 'on',\n",
       " 'n.',\n",
       " '.l',\n",
       " 'lu',\n",
       " 'un',\n",
       " 'na',\n",
       " 'a.',\n",
       " '.g',\n",
       " 'gr',\n",
       " 'ra',\n",
       " 'ac',\n",
       " 'ce',\n",
       " 'e.',\n",
       " '.c',\n",
       " 'ch',\n",
       " 'hl',\n",
       " 'lo',\n",
       " 'oe',\n",
       " 'e.',\n",
       " '.p',\n",
       " 'pe',\n",
       " 'en',\n",
       " 'ne',\n",
       " 'el',\n",
       " 'lo',\n",
       " 'op',\n",
       " 'pe',\n",
       " 'e.',\n",
       " '.l',\n",
       " 'la',\n",
       " 'ay',\n",
       " 'yl',\n",
       " 'la',\n",
       " 'a.',\n",
       " '.r',\n",
       " 'ri',\n",
       " 'il',\n",
       " 'le',\n",
       " 'ey',\n",
       " 'y.',\n",
       " '.z',\n",
       " 'zo',\n",
       " 'oe',\n",
       " 'ey',\n",
       " 'y.',\n",
       " '.n',\n",
       " 'no',\n",
       " 'or',\n",
       " 'ra',\n",
       " 'a.',\n",
       " '.l',\n",
       " 'li',\n",
       " 'il',\n",
       " 'ly',\n",
       " 'y.',\n",
       " '.e',\n",
       " 'el',\n",
       " 'le',\n",
       " 'ea',\n",
       " 'an',\n",
       " 'no',\n",
       " 'or',\n",
       " 'r.',\n",
       " '.h',\n",
       " 'ha',\n",
       " 'an',\n",
       " 'nn',\n",
       " 'na',\n",
       " 'ah',\n",
       " 'h.',\n",
       " '.l',\n",
       " 'li',\n",
       " 'il',\n",
       " 'll',\n",
       " 'li',\n",
       " 'ia',\n",
       " 'an',\n",
       " 'n.',\n",
       " '.a',\n",
       " 'ad',\n",
       " 'dd',\n",
       " 'di',\n",
       " 'is',\n",
       " 'so',\n",
       " 'on',\n",
       " 'n.',\n",
       " '.a',\n",
       " 'au',\n",
       " 'ub',\n",
       " 'br',\n",
       " 're',\n",
       " 'ey',\n",
       " 'y.',\n",
       " '.e',\n",
       " 'el',\n",
       " 'll',\n",
       " 'li',\n",
       " 'ie',\n",
       " 'e.',\n",
       " '.s',\n",
       " 'st',\n",
       " 'te',\n",
       " 'el',\n",
       " 'll',\n",
       " 'la',\n",
       " 'a.',\n",
       " '.n',\n",
       " 'na',\n",
       " 'at',\n",
       " 'ta',\n",
       " 'al',\n",
       " 'li',\n",
       " 'ie',\n",
       " 'e.',\n",
       " '.z',\n",
       " 'zo',\n",
       " 'oe',\n",
       " 'e.',\n",
       " '.l',\n",
       " 'le',\n",
       " 'ea',\n",
       " 'ah',\n",
       " 'h.',\n",
       " '.h',\n",
       " 'ha',\n",
       " 'az',\n",
       " 'ze',\n",
       " 'el',\n",
       " 'l.',\n",
       " '.v',\n",
       " 'vi',\n",
       " 'io',\n",
       " 'ol',\n",
       " 'le',\n",
       " 'et',\n",
       " 't.',\n",
       " '.a',\n",
       " 'au',\n",
       " 'ur',\n",
       " 'ro',\n",
       " 'or',\n",
       " 'ra',\n",
       " 'a.',\n",
       " '.s',\n",
       " 'sa',\n",
       " 'av',\n",
       " 'va',\n",
       " 'an',\n",
       " 'nn',\n",
       " 'na',\n",
       " 'ah',\n",
       " 'h.',\n",
       " '.a',\n",
       " 'au',\n",
       " 'ud',\n",
       " 'dr',\n",
       " 're',\n",
       " 'ey',\n",
       " 'y.',\n",
       " '.b',\n",
       " 'br',\n",
       " 'ro',\n",
       " 'oo',\n",
       " 'ok',\n",
       " 'kl',\n",
       " 'ly',\n",
       " 'yn',\n",
       " 'n.',\n",
       " '.b',\n",
       " 'be',\n",
       " 'el',\n",
       " 'll',\n",
       " 'la',\n",
       " 'a.',\n",
       " '.c',\n",
       " 'cl',\n",
       " 'la',\n",
       " 'ai',\n",
       " 'ir',\n",
       " 're',\n",
       " 'e.',\n",
       " '.s',\n",
       " 'sk',\n",
       " 'ky',\n",
       " 'yl',\n",
       " 'la',\n",
       " 'ar',\n",
       " 'r.',\n",
       " '.l',\n",
       " 'lu',\n",
       " 'uc',\n",
       " 'cy',\n",
       " 'y.',\n",
       " '.p',\n",
       " 'pa',\n",
       " 'ai',\n",
       " 'is',\n",
       " 'sl',\n",
       " 'le',\n",
       " 'ey',\n",
       " 'y.',\n",
       " '.e',\n",
       " 'ev',\n",
       " 've',\n",
       " 'er',\n",
       " 'rl',\n",
       " 'ly',\n",
       " 'y.',\n",
       " '.a',\n",
       " 'an',\n",
       " 'nn',\n",
       " 'na',\n",
       " 'a.',\n",
       " '.c',\n",
       " 'ca',\n",
       " 'ar',\n",
       " 'ro',\n",
       " 'ol',\n",
       " 'li',\n",
       " 'in',\n",
       " 'ne',\n",
       " 'e.',\n",
       " '.n',\n",
       " 'no',\n",
       " 'ov',\n",
       " 'va',\n",
       " 'a.',\n",
       " '.g',\n",
       " 'ge',\n",
       " 'en',\n",
       " 'ne',\n",
       " 'es',\n",
       " 'si',\n",
       " 'is',\n",
       " 's.',\n",
       " '.e',\n",
       " 'em',\n",
       " 'mi',\n",
       " 'il',\n",
       " 'li',\n",
       " 'ia',\n",
       " 'a.',\n",
       " '.k',\n",
       " 'ke',\n",
       " 'en',\n",
       " 'nn',\n",
       " 'ne',\n",
       " 'ed',\n",
       " 'dy',\n",
       " 'y.',\n",
       " '.s',\n",
       " 'sa',\n",
       " 'am',\n",
       " 'ma',\n",
       " 'an',\n",
       " 'nt',\n",
       " 'th',\n",
       " 'ha',\n",
       " 'a.',\n",
       " '.m',\n",
       " 'ma',\n",
       " 'ay',\n",
       " 'ya',\n",
       " 'a.',\n",
       " '.w',\n",
       " 'wi',\n",
       " 'il',\n",
       " 'll',\n",
       " 'lo',\n",
       " 'ow',\n",
       " 'w.',\n",
       " '.k',\n",
       " 'ki',\n",
       " 'in',\n",
       " 'ns',\n",
       " 'sl',\n",
       " 'le',\n",
       " 'ey',\n",
       " 'y.',\n",
       " '.n',\n",
       " 'na',\n",
       " 'ao',\n",
       " 'om',\n",
       " 'mi',\n",
       " 'i.',\n",
       " '.a',\n",
       " 'aa',\n",
       " 'al',\n",
       " 'li',\n",
       " 'iy',\n",
       " 'ya',\n",
       " 'ah',\n",
       " 'h.',\n",
       " '.e',\n",
       " 'el',\n",
       " 'le',\n",
       " 'en',\n",
       " 'na',\n",
       " 'a.',\n",
       " '.s',\n",
       " 'sa',\n",
       " 'ar',\n",
       " 'ra',\n",
       " 'ah',\n",
       " 'h.',\n",
       " '.a',\n",
       " 'ar',\n",
       " 'ri',\n",
       " 'ia',\n",
       " 'an',\n",
       " 'na',\n",
       " 'a.',\n",
       " '.a',\n",
       " 'al',\n",
       " 'll',\n",
       " 'li',\n",
       " 'is',\n",
       " 'so',\n",
       " 'on',\n",
       " 'n.',\n",
       " '.g',\n",
       " 'ga',\n",
       " 'ab',\n",
       " 'br',\n",
       " 'ri',\n",
       " 'ie',\n",
       " 'el',\n",
       " 'll',\n",
       " 'la',\n",
       " 'a.',\n",
       " '.a',\n",
       " 'al',\n",
       " 'li',\n",
       " 'ic',\n",
       " 'ce',\n",
       " 'e.',\n",
       " '.m',\n",
       " 'ma',\n",
       " 'ad',\n",
       " 'de',\n",
       " 'el',\n",
       " 'ly',\n",
       " 'yn',\n",
       " 'n.',\n",
       " '.c',\n",
       " 'co',\n",
       " 'or',\n",
       " 'ra',\n",
       " 'a.',\n",
       " '.r',\n",
       " 'ru',\n",
       " 'ub',\n",
       " 'by',\n",
       " 'y.',\n",
       " '.e',\n",
       " 'ev',\n",
       " 'va',\n",
       " 'a.',\n",
       " '.s',\n",
       " 'se',\n",
       " 'er',\n",
       " 're',\n",
       " 'en',\n",
       " 'ni',\n",
       " 'it',\n",
       " 'ty',\n",
       " 'y.',\n",
       " '.a',\n",
       " 'au',\n",
       " 'ut',\n",
       " 'tu',\n",
       " 'um',\n",
       " 'mn',\n",
       " 'n.',\n",
       " '.a',\n",
       " 'ad',\n",
       " 'de',\n",
       " 'el',\n",
       " 'li',\n",
       " 'in',\n",
       " 'ne',\n",
       " 'e.',\n",
       " '.h',\n",
       " 'ha',\n",
       " 'ai',\n",
       " 'il',\n",
       " 'le',\n",
       " 'ey',\n",
       " 'y.',\n",
       " '.g',\n",
       " 'gi',\n",
       " 'ia',\n",
       " 'an',\n",
       " 'nn',\n",
       " 'na',\n",
       " 'a.',\n",
       " '.v',\n",
       " 'va',\n",
       " 'al',\n",
       " 'le',\n",
       " 'en',\n",
       " 'nt',\n",
       " 'ti',\n",
       " 'in',\n",
       " 'na',\n",
       " 'a.',\n",
       " '.i',\n",
       " 'is',\n",
       " 'sl',\n",
       " 'la',\n",
       " 'a.',\n",
       " '.e',\n",
       " 'el',\n",
       " 'li',\n",
       " 'ia',\n",
       " 'an',\n",
       " 'na',\n",
       " 'a.',\n",
       " '.q',\n",
       " 'qu',\n",
       " 'ui',\n",
       " 'in',\n",
       " 'nn',\n",
       " 'n.',\n",
       " '.n',\n",
       " 'ne',\n",
       " 'ev',\n",
       " 'va',\n",
       " 'ae',\n",
       " 'eh',\n",
       " 'h.',\n",
       " '.i',\n",
       " 'iv',\n",
       " 'vy',\n",
       " 'y.',\n",
       " '.s',\n",
       " 'sa',\n",
       " 'ad',\n",
       " 'di',\n",
       " 'ie',\n",
       " 'e.',\n",
       " '.p',\n",
       " 'pi',\n",
       " 'ip',\n",
       " 'pe',\n",
       " 'er',\n",
       " 'r.',\n",
       " '.l',\n",
       " 'ly',\n",
       " 'yd',\n",
       " 'di',\n",
       " 'ia',\n",
       " 'a.',\n",
       " '.a',\n",
       " 'al',\n",
       " 'le',\n",
       " 'ex',\n",
       " 'xa',\n",
       " 'a.',\n",
       " '.j',\n",
       " 'jo',\n",
       " 'os',\n",
       " 'se',\n",
       " 'ep',\n",
       " 'ph',\n",
       " 'hi',\n",
       " 'in',\n",
       " 'ne',\n",
       " 'e.',\n",
       " '.e',\n",
       " 'em',\n",
       " 'me',\n",
       " 'er',\n",
       " 'ry',\n",
       " 'y.',\n",
       " '.j',\n",
       " 'ju',\n",
       " 'ul',\n",
       " 'li',\n",
       " 'ia',\n",
       " 'a.',\n",
       " '.d',\n",
       " 'de',\n",
       " 'el',\n",
       " 'li',\n",
       " 'il',\n",
       " 'la',\n",
       " 'ah',\n",
       " 'h.',\n",
       " '.a',\n",
       " 'ar',\n",
       " 'ri',\n",
       " 'ia',\n",
       " 'an',\n",
       " 'nn',\n",
       " 'na',\n",
       " 'a.',\n",
       " '.v',\n",
       " 'vi',\n",
       " 'iv',\n",
       " 'vi',\n",
       " 'ia',\n",
       " 'an',\n",
       " 'n.',\n",
       " '.k',\n",
       " 'ka',\n",
       " 'ay',\n",
       " 'yl',\n",
       " 'le',\n",
       " 'ee',\n",
       " 'e.',\n",
       " '.s',\n",
       " 'so',\n",
       " 'op',\n",
       " 'ph',\n",
       " 'hi',\n",
       " 'ie',\n",
       " 'e.',\n",
       " '.b',\n",
       " 'br',\n",
       " 'ri',\n",
       " 'ie',\n",
       " 'el',\n",
       " 'll',\n",
       " 'le',\n",
       " 'e.',\n",
       " '.m',\n",
       " 'ma',\n",
       " 'ad',\n",
       " 'de',\n",
       " 'el',\n",
       " 'li',\n",
       " 'in',\n",
       " 'ne',\n",
       " 'e.',\n",
       " '.p',\n",
       " 'pe',\n",
       " 'ey',\n",
       " 'yt',\n",
       " 'to',\n",
       " 'on',\n",
       " 'n.',\n",
       " '.r',\n",
       " 'ry',\n",
       " 'yl',\n",
       " 'le',\n",
       " 'ee',\n",
       " 'e.',\n",
       " '.c',\n",
       " 'cl',\n",
       " 'la',\n",
       " 'ar',\n",
       " 'ra',\n",
       " 'a.',\n",
       " '.h',\n",
       " 'ha',\n",
       " 'ad',\n",
       " 'dl',\n",
       " 'le',\n",
       " 'ey',\n",
       " 'y.',\n",
       " '.m',\n",
       " 'me',\n",
       " 'el',\n",
       " 'la',\n",
       " 'an',\n",
       " 'ni',\n",
       " 'ie',\n",
       " 'e.',\n",
       " '.m',\n",
       " 'ma',\n",
       " 'ac',\n",
       " 'ck',\n",
       " 'ke',\n",
       " 'en',\n",
       " 'nz',\n",
       " 'zi',\n",
       " 'ie',\n",
       " 'e.',\n",
       " '.r',\n",
       " 're',\n",
       " 'ea',\n",
       " 'ag',\n",
       " 'ga',\n",
       " 'an',\n",
       " 'n.',\n",
       " '.a',\n",
       " 'ad',\n",
       " 'da',\n",
       " 'al',\n",
       " 'ly',\n",
       " 'yn',\n",
       " 'nn',\n",
       " 'n.',\n",
       " '.l',\n",
       " 'li',\n",
       " 'il',\n",
       " 'li',\n",
       " 'ia',\n",
       " 'an',\n",
       " 'na',\n",
       " 'a.',\n",
       " '.a',\n",
       " 'au',\n",
       " 'ub',\n",
       " 'br',\n",
       " 're',\n",
       " 'ee',\n",
       " 'e.',\n",
       " '.j',\n",
       " 'ja',\n",
       " 'ad',\n",
       " 'de',\n",
       " 'e.',\n",
       " '.k',\n",
       " 'ka',\n",
       " 'at',\n",
       " 'th',\n",
       " 'he',\n",
       " 'er',\n",
       " 'ri',\n",
       " 'in',\n",
       " 'ne',\n",
       " 'e.',\n",
       " '.i',\n",
       " 'is',\n",
       " 'sa',\n",
       " 'ab',\n",
       " 'be',\n",
       " 'el',\n",
       " 'll',\n",
       " 'le',\n",
       " 'e.',\n",
       " '.n',\n",
       " 'na',\n",
       " 'at',\n",
       " 'ta',\n",
       " 'al',\n",
       " 'li',\n",
       " 'ia',\n",
       " 'a.',\n",
       " '.r',\n",
       " 'ra',\n",
       " 'ae',\n",
       " 'el',\n",
       " 'ly',\n",
       " 'yn',\n",
       " 'nn',\n",
       " 'n.',\n",
       " '.m',\n",
       " 'ma',\n",
       " 'ar',\n",
       " 'ri',\n",
       " 'ia',\n",
       " 'a.',\n",
       " '.a',\n",
       " 'at',\n",
       " 'th',\n",
       " 'he',\n",
       " 'en',\n",
       " 'na',\n",
       " 'a.',\n",
       " '.x',\n",
       " 'xi',\n",
       " 'im',\n",
       " 'me',\n",
       " 'en',\n",
       " 'na',\n",
       " 'a.',\n",
       " '.a',\n",
       " 'ar',\n",
       " 'ry',\n",
       " 'ya',\n",
       " 'a.',\n",
       " '.l',\n",
       " 'le',\n",
       " 'ei',\n",
       " 'il',\n",
       " 'la',\n",
       " 'an',\n",
       " 'ni',\n",
       " 'i.',\n",
       " '.t',\n",
       " 'ta',\n",
       " 'ay',\n",
       " 'yl',\n",
       " 'lo',\n",
       " 'or',\n",
       " 'r.',\n",
       " '.f',\n",
       " 'fa',\n",
       " 'ai',\n",
       " 'it',\n",
       " 'th',\n",
       " 'h.',\n",
       " '.r',\n",
       " 'ro',\n",
       " 'os',\n",
       " 'se',\n",
       " 'e.',\n",
       " '.k',\n",
       " 'ky',\n",
       " 'yl',\n",
       " 'li',\n",
       " 'ie',\n",
       " 'e.',\n",
       " '.a',\n",
       " 'al',\n",
       " 'le',\n",
       " 'ex',\n",
       " 'xa',\n",
       " 'an',\n",
       " 'nd',\n",
       " 'dr',\n",
       " 'ra',\n",
       " 'a.',\n",
       " '.m',\n",
       " 'ma',\n",
       " 'ar',\n",
       " 'ry',\n",
       " 'y.',\n",
       " '.m',\n",
       " 'ma',\n",
       " 'ar',\n",
       " 'rg',\n",
       " 'ga',\n",
       " 'ar',\n",
       " 're',\n",
       " 'et',\n",
       " 't.',\n",
       " '.l',\n",
       " 'ly',\n",
       " 'yl',\n",
       " 'la',\n",
       " 'a.',\n",
       " '.a',\n",
       " 'as',\n",
       " 'sh',\n",
       " 'hl',\n",
       " 'le',\n",
       " 'ey',\n",
       " 'y.',\n",
       " '.a',\n",
       " 'am',\n",
       " 'ma',\n",
       " 'ay',\n",
       " 'ya',\n",
       " 'a.',\n",
       " '.e',\n",
       " 'el',\n",
       " 'li',\n",
       " 'iz',\n",
       " 'za',\n",
       " 'a.',\n",
       " '.b',\n",
       " 'br',\n",
       " 'ri',\n",
       " 'ia',\n",
       " 'an',\n",
       " 'nn',\n",
       " 'na',\n",
       " 'a.',\n",
       " '.b',\n",
       " 'ba',\n",
       " 'ai',\n",
       " 'il',\n",
       " 'le',\n",
       " 'ey',\n",
       " 'y.',\n",
       " '.a',\n",
       " 'an',\n",
       " 'nd',\n",
       " 'dr',\n",
       " 're',\n",
       " 'ea',\n",
       " 'a.',\n",
       " '.k',\n",
       " 'kh',\n",
       " 'hl',\n",
       " 'lo',\n",
       " 'oe',\n",
       " 'e.',\n",
       " '.j',\n",
       " 'ja',\n",
       " 'as',\n",
       " 'sm',\n",
       " 'mi',\n",
       " 'in',\n",
       " 'ne',\n",
       " 'e.',\n",
       " '.m',\n",
       " 'me',\n",
       " 'el',\n",
       " 'lo',\n",
       " 'od',\n",
       " 'dy',\n",
       " 'y.',\n",
       " '.i',\n",
       " 'ir',\n",
       " 'ri',\n",
       " 'is',\n",
       " 's.',\n",
       " '.i',\n",
       " 'is',\n",
       " 'sa',\n",
       " 'ab',\n",
       " 'be',\n",
       " 'el',\n",
       " 'l.',\n",
       " '.n',\n",
       " 'no',\n",
       " 'or',\n",
       " 'ra',\n",
       " 'ah',\n",
       " 'h.',\n",
       " '.a',\n",
       " 'an',\n",
       " 'nn',\n",
       " 'na',\n",
       " 'ab',\n",
       " 'be',\n",
       " 'el',\n",
       " 'll',\n",
       " 'le',\n",
       " 'e.',\n",
       " '.v',\n",
       " 'va',\n",
       " 'al',\n",
       " 'le',\n",
       " 'er',\n",
       " 'ri',\n",
       " 'ia',\n",
       " 'a.',\n",
       " '.e',\n",
       " 'em',\n",
       " 'me',\n",
       " 'er',\n",
       " 'rs',\n",
       " 'so',\n",
       " 'on',\n",
       " 'n.',\n",
       " '.a',\n",
       " 'ad',\n",
       " 'da',\n",
       " 'al',\n",
       " 'ly',\n",
       " 'yn',\n",
       " 'n.',\n",
       " '.r',\n",
       " 'ry',\n",
       " 'yl',\n",
       " ...]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#[char for char in w for w in words]\n",
    "[f'{c}{c1}' for w in words for c, c1 in zip(f'.{w}.', f'.{w}.'[1:])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  5.],\n",
       "        [ 5., 13.],\n",
       "        [13., 13.],\n",
       "        ...,\n",
       "        [25., 26.],\n",
       "        [26., 24.],\n",
       "        [24.,  0.]])"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_pairs = torch.tensor(\n",
    "    [[stoi[c], stoi[c1]] for w in words for c, c1 in zip(f'.{w}.', f'.{w}.'[1:])]\n",
    ").float()\n",
    "word_pairs\n",
    "# xs = torch.tensor(xs).float()\n",
    "# xs[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.],\n",
       "         [ 5.],\n",
       "         [13.],\n",
       "         ...,\n",
       "         [25.],\n",
       "         [26.],\n",
       "         [24.]]),\n",
       " tensor([[ 5.],\n",
       "         [13.],\n",
       "         [13.],\n",
       "         ...,\n",
       "         [26.],\n",
       "         [24.],\n",
       "         [ 0.]]))"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs = word_pairs[:,:1]\n",
    "ys = word_pairs[:,1:]\n",
    "xs,ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_neurons = 100\n",
    "predictable_chars = len(letters) + 1\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(1, hidden_neurons),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(hidden_neurons, predictable_chars),\n",
    "    torch.nn.Softmax()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daynil/mambaforge-pypy3/envs/ai-base/lib/python3.11/site-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([228146, 27]),\n",
       " tensor([[3.6331e-02, 4.2283e-02, 2.6865e-02,  ..., 6.9518e-02, 4.0645e-02,\n",
       "          4.6356e-02],\n",
       "         [1.2353e-02, 2.9820e-02, 1.5131e-02,  ..., 5.5166e-03, 5.5027e-02,\n",
       "          1.0606e-01],\n",
       "         [3.1652e-04, 4.7240e-03, 6.8943e-04,  ..., 1.7975e-05, 7.4913e-03,\n",
       "          1.3982e-01],\n",
       "         ...,\n",
       "         [5.3601e-07, 1.3013e-04, 2.6604e-06,  ..., 1.3688e-09, 1.5724e-04,\n",
       "          9.4873e-02],\n",
       "         [3.1181e-07, 9.5391e-05, 1.6579e-06,  ..., 6.1497e-10, 1.1272e-04,\n",
       "          9.0825e-02],\n",
       "         [9.2065e-07, 1.7737e-04, 4.2655e-06,  ..., 3.0443e-09, 2.1915e-04,\n",
       "          9.9017e-02]], grad_fn=<SoftmaxBackward0>))"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preds = model(xs)\n",
    "test_preds.shape, test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1., grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preds[0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0695, grad_fn=<MaxBackward1>),\n",
       " tensor([0.0363, 0.0423, 0.0269, 0.0517, 0.0351, 0.0348, 0.0326, 0.0354, 0.0337,\n",
       "         0.0281, 0.0306, 0.0683, 0.0330, 0.0371, 0.0187, 0.0365, 0.0233, 0.0478,\n",
       "         0.0226, 0.0310, 0.0381, 0.0333, 0.0393, 0.0270, 0.0695, 0.0406, 0.0464],\n",
       "        grad_fn=<SelectBackward0>))"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preds[0].max(), test_preds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(24)"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preds[0].argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0695, grad_fn=<MaxBackward1>)"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preds[0].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([0.0695, 0.2577, 0.5700,  ..., 0.7608, 0.7715, 0.7496],\n",
       "       grad_fn=<MaxBackward0>),\n",
       "indices=tensor([24, 11, 11,  ..., 11, 11, 11]))"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preds.max(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "228146"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preds.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(preds: torch.Tensor):\n",
    "    return (-torch.log(preds.max(dim=1).values).sum()) / test_preds.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.2578, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn(test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0219, 0.0348, 0.0533, 0.0416, 0.0252, 0.0760, 0.0333, 0.0720, 0.0300,\n",
       "        0.0654, 0.0357, 0.0850, 0.0332, 0.0318, 0.0059, 0.0129, 0.0217, 0.0211,\n",
       "        0.0780, 0.0352, 0.0114, 0.0077, 0.0165, 0.0219, 0.0156, 0.0724, 0.0403],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.2794, grad_fn=<DivBackward0>)\n",
      "tensor(0.0179, grad_fn=<DivBackward0>)\n",
      "tensor(0.0165, grad_fn=<DivBackward0>)\n",
      "tensor(0.0154, grad_fn=<DivBackward0>)\n",
      "tensor(0.0143, grad_fn=<DivBackward0>)\n",
      "tensor(0.0134, grad_fn=<DivBackward0>)\n",
      "tensor(0.0126, grad_fn=<DivBackward0>)\n",
      "tensor(0.0119, grad_fn=<DivBackward0>)\n",
      "tensor(0.0113, grad_fn=<DivBackward0>)\n",
      "tensor(0.0107, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "hidden_neurons = 100\n",
    "predictable_chars = len(letters) + 1\n",
    "\n",
    "epochs = 10\n",
    "lr = 0.2\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(1, hidden_neurons),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(hidden_neurons, predictable_chars),\n",
    "    torch.nn.Softmax()\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr)\n",
    "\n",
    "def train_step(xb):\n",
    "    preds = model(xb)\n",
    "    loss = loss_fn(preds)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    return loss\n",
    "\n",
    "def valid_step(xb):\n",
    "    preds = model(xb)\n",
    "    loss = loss_fn(preds)\n",
    "    return loss\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    loss = train_step(xs)\n",
    "    print(loss)\n",
    "    # for batch in dls.train:\n",
    "    #     xb, yb = batch\n",
    "    #     xb = to_bw_flattened(xb)\n",
    "    #     xb, yb = to_torch_tensor(xb, yb)\n",
    "    #     loss, acc = train_step(xb, yb)\n",
    "\n",
    "    # print(f\"Avg loss: {tot_loss / valid_batches}, Avg acc: {tot_acc / valid_batches}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thinking about it, the above is actually wrong because what I'm calculating loss on doesn't make sense. I'm basically giving it the first letter, then calculating loss on the confidence of whatever it predicts as the second.\n",
    "\n",
    "What I need to do is cross entropy loss - give it the xs and the labels, and calculate cross entropy loss on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.],\n",
       "         [ 5.],\n",
       "         [13.],\n",
       "         ...,\n",
       "         [25.],\n",
       "         [26.],\n",
       "         [24.]]),\n",
       " tensor([[ 5.],\n",
       "         [13.],\n",
       "         [13.],\n",
       "         ...,\n",
       "         [26.],\n",
       "         [24.],\n",
       "         [ 0.]]))"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs, ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.],\n",
       "         [ 5.],\n",
       "         [13.],\n",
       "         ...,\n",
       "         [25.],\n",
       "         [26.],\n",
       "         [24.]]),\n",
       " tensor([ 5., 13., 13.,  ..., 26., 24.,  0.]))"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs, ys.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0],\n",
       "        [ 5],\n",
       "        [13],\n",
       "        ...,\n",
       "        [25],\n",
       "        [26],\n",
       "        [24]], dtype=torch.int32)"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs.int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  5, 13,  ..., 25, 26, 24], dtype=torch.int32)"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs.to(torch.int64).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5, 13, 13,  ..., 26, 24,  0])"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys.to(torch.int64).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 0, 1, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 1],\n",
       "        [0, 0, 0,  ..., 1, 0, 0]])"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.one_hot(xs.squeeze().to(torch.int64), 27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 0, 0, 1],\n",
       "        [0, 0, 0,  ..., 1, 0, 0],\n",
       "        [1, 0, 0,  ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.one_hot(ys.squeeze().to(torch.int64), 27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.3066, grad_fn=<DivBackward1>)\n",
      "tensor(3.0894, grad_fn=<DivBackward1>)\n",
      "tensor(2.9425, grad_fn=<DivBackward1>)\n",
      "tensor(2.8668, grad_fn=<DivBackward1>)\n",
      "tensor(2.8288, grad_fn=<DivBackward1>)\n",
      "tensor(2.8046, grad_fn=<DivBackward1>)\n",
      "tensor(2.7865, grad_fn=<DivBackward1>)\n",
      "tensor(2.7711, grad_fn=<DivBackward1>)\n",
      "tensor(2.7569, grad_fn=<DivBackward1>)\n",
      "tensor(2.7435, grad_fn=<DivBackward1>)\n",
      "tensor(2.7306, grad_fn=<DivBackward1>)\n",
      "tensor(2.7179, grad_fn=<DivBackward1>)\n",
      "tensor(2.7058, grad_fn=<DivBackward1>)\n",
      "tensor(2.6938, grad_fn=<DivBackward1>)\n",
      "tensor(2.6828, grad_fn=<DivBackward1>)\n",
      "tensor(2.6719, grad_fn=<DivBackward1>)\n",
      "tensor(2.6621, grad_fn=<DivBackward1>)\n",
      "tensor(2.6541, grad_fn=<DivBackward1>)\n",
      "tensor(2.6500, grad_fn=<DivBackward1>)\n",
      "tensor(2.6603, grad_fn=<DivBackward1>)\n",
      "tensor(2.7022, grad_fn=<DivBackward1>)\n",
      "tensor(2.7783, grad_fn=<DivBackward1>)\n",
      "tensor(2.7740, grad_fn=<DivBackward1>)\n",
      "tensor(2.6420, grad_fn=<DivBackward1>)\n",
      "tensor(2.6152, grad_fn=<DivBackward1>)\n",
      "tensor(2.6088, grad_fn=<DivBackward1>)\n",
      "tensor(2.6025, grad_fn=<DivBackward1>)\n",
      "tensor(2.6039, grad_fn=<DivBackward1>)\n",
      "tensor(2.6071, grad_fn=<DivBackward1>)\n",
      "tensor(2.6207, grad_fn=<DivBackward1>)\n",
      "tensor(2.6328, grad_fn=<DivBackward1>)\n",
      "tensor(2.6316, grad_fn=<DivBackward1>)\n",
      "tensor(2.6217, grad_fn=<DivBackward1>)\n",
      "tensor(2.6018, grad_fn=<DivBackward1>)\n",
      "tensor(2.5935, grad_fn=<DivBackward1>)\n",
      "tensor(2.5859, grad_fn=<DivBackward1>)\n",
      "tensor(2.5828, grad_fn=<DivBackward1>)\n",
      "tensor(2.5794, grad_fn=<DivBackward1>)\n",
      "tensor(2.5773, grad_fn=<DivBackward1>)\n",
      "tensor(2.5745, grad_fn=<DivBackward1>)\n",
      "tensor(2.5742, grad_fn=<DivBackward1>)\n",
      "tensor(2.5707, grad_fn=<DivBackward1>)\n",
      "tensor(2.5678, grad_fn=<DivBackward1>)\n",
      "tensor(2.5651, grad_fn=<DivBackward1>)\n",
      "tensor(2.5623, grad_fn=<DivBackward1>)\n",
      "tensor(2.5606, grad_fn=<DivBackward1>)\n",
      "tensor(2.5587, grad_fn=<DivBackward1>)\n",
      "tensor(2.5559, grad_fn=<DivBackward1>)\n",
      "tensor(2.5549, grad_fn=<DivBackward1>)\n",
      "tensor(2.5520, grad_fn=<DivBackward1>)\n"
     ]
    }
   ],
   "source": [
    "hidden_neurons = 100\n",
    "predictable_chars = len(letters) + 1\n",
    "\n",
    "epochs = 50\n",
    "lr = 3\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(27, hidden_neurons),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(hidden_neurons, predictable_chars),\n",
    "    # cross entropy loss expects raw unnormalized logits\n",
    "    # torch.nn.Softmax()\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr)\n",
    "\n",
    "def train_step(xb, yb):\n",
    "    preds = model(xb)\n",
    "    loss = loss_fn(preds, yb)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    return loss\n",
    "\n",
    "# def valid_step(xb):\n",
    "#     preds = model(xb)\n",
    "#     loss = loss_fn(preds)\n",
    "#     return loss\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    loss = train_step(\n",
    "        F.one_hot(xs.squeeze().to(torch.int64), 27).float(), \n",
    "        F.one_hot(ys.squeeze().to(torch.int64), 27).float()\n",
    "    )\n",
    "    print(loss)\n",
    "    # for batch in dls.train:\n",
    "    #     xb, yb = batch\n",
    "    #     xb = to_bw_flattened(xb)\n",
    "    #     xb, yb = to_torch_tensor(xb, yb)\n",
    "    #     loss, acc = train_step(xb, yb)\n",
    "\n",
    "    # print(f\"Avg loss: {tot_loss / valid_batches}, Avg acc: {tot_acc / valid_batches}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2.6328, -0.0816, -0.4266, -0.3451,  0.4298,  0.1665, -1.5470, -1.0255,\n",
       "         1.2468,  0.7366, -1.3115, -0.2626,  1.4293,  0.8547,  2.3013, -0.5743,\n",
       "        -1.5736, -2.0400,  1.6903,  0.7525,  0.2714, -0.3738,  0.0182, -1.4725,\n",
       "        -1.6215,  1.0940, -0.5529], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "letter_a = torch.zeros(27)\n",
    "letter_a[1] = 1\n",
    "single_pred_logits = model(letter_a)\n",
    "single_pred_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daynil/mambaforge-pypy3/envs/ai-base/lib/python3.11/site-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_pred = torch.nn.Softmax()(single_pred_logits)\n",
    "single_pred.argmax(dim=0)\n",
    "# itos[single_pred.argmax().item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keen\n",
      "aely\n",
      "droma\n",
      "lien\n",
      "\n",
      "b\n",
      "miacun\n",
      "ble\n",
      "crtya\n",
      "kin\n",
      "lhrinnyledel\n",
      "wesadalauhaon\n",
      "gevesch\n",
      "pmilipnren\n",
      "anlealtisn\n",
      "com\n",
      "dllslefsi\n",
      "tant\n",
      "ryi\n",
      "san\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "words_to_gen = 20\n",
    "for wi in range(words_to_gen):\n",
    "    # word_len = torch.randint(min(word_lens), max(word_lens), (1,)).item()\n",
    "    # for i in range(int(word_len)):\n",
    "    li = 0\n",
    "    next_letter = ''\n",
    "    genned_word = '.'\n",
    "    while next_letter != '.' and len(genned_word) < max(word_lens):\n",
    "        prior_letter = genned_word[li]\n",
    "        prior_letter_i = stoi[prior_letter]\n",
    "        prior_letter_i_one_hot = F.one_hot(\n",
    "            torch.tensor(prior_letter_i), 27\n",
    "        ).float()\n",
    "\n",
    "        probs = torch.nn.Softmax()(model(prior_letter_i_one_hot))\n",
    "        pred = torch.multinomial(probs, 1, replacement=True).item()\n",
    "        next_letter = itos[int(pred)]\n",
    "\n",
    "        # next_sample_p = P[prior_letter_i]\n",
    "        # next_letter_idx = torch.multinomial(\n",
    "        #         next_sample_p, 1, replacement=True, generator=g\n",
    "        #     ).item()\n",
    "        # next_letter = itos[next_letter_idx]\n",
    "        genned_word += next_letter\n",
    "        li += 1\n",
    "    print(genned_word.strip('.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Karpathy:\n",
    "Update: I added some suggested exercises to the description of the video. imo learning requires in-person tinkering and work, watching a video is not enough. If you complete the exercises please feel free to link your work here. (+Feel free to suggest other good exercises!)\n",
    "\n",
    "E01: train a trigram language model, i.e. take two characters as an input to predict the 3rd one. Feel free to use either counting or a neural net. Evaluate the loss; Did it improve over a bigram model?\n",
    "\n",
    "E02: split up the dataset randomly into 80% train set, 10% dev set, 10% test set. Train the bigram and trigram models only on the training set. Evaluate them on dev and test splits. What can you see?\n",
    "\n",
    "E03: use the dev set to tune the strength of smoothing (or regularization) for the trigram model - i.e. try many possibilities and see which one works best based on the dev set loss. What patterns can you see in the train and dev set loss as you tune this strength? Take the best setting of the smoothing and evaluate on the test set once and at the end. How good of a loss do you achieve?\n",
    "\n",
    "E04: we saw that our 1-hot vectors merely select a row of W, so producing these vectors explicitly feels wasteful. Can you delete our use of F.one_hot in favor of simply indexing into rows of W?\n",
    "\n",
    "E05: look up and use F.cross_entropy instead. You should achieve the same result. Can you think of why we'd prefer to use F.cross_entropy instead?\n",
    "\n",
    "E06: meta-exercise! Think of a fun/interesting exercise and complete it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train a trigram language model\n",
    "E01: train a trigram language model, i.e. take two characters as an input to predict the 3rd one. Feel free to use either counting or a neural net. Evaluate the loss; Did it improve over a bigram model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to create a new vocab. The neural net wouldn't know what to do with 2 letters passed individually - we can't one hot encode 2 different answers. So to create a trigram, we create a vocab of all possible 2 letter combos with indices.\n",
    "\n",
    "Note that we have to generate this from the alphabet rather than the vocab since we may come up with novel word pairs that didn't exist in the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'q',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'w',\n",
       " 'x',\n",
       " 'y',\n",
       " 'z',\n",
       " '.']"
      ]
     },
     "execution_count": 411,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "letters + ['.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'..': 0,\n",
       " '.a': 1,\n",
       " '.b': 2,\n",
       " '.c': 3,\n",
       " '.d': 4,\n",
       " '.e': 5,\n",
       " '.f': 6,\n",
       " '.g': 7,\n",
       " '.h': 8,\n",
       " '.i': 9,\n",
       " '.j': 10,\n",
       " '.k': 11,\n",
       " '.l': 12,\n",
       " '.m': 13,\n",
       " '.n': 14,\n",
       " '.o': 15,\n",
       " '.p': 16,\n",
       " '.q': 17,\n",
       " '.r': 18,\n",
       " '.s': 19,\n",
       " '.t': 20,\n",
       " '.u': 21,\n",
       " '.v': 22,\n",
       " '.w': 23,\n",
       " '.x': 24,\n",
       " '.y': 25,\n",
       " '.z': 26,\n",
       " 'a.': 27,\n",
       " 'aa': 28,\n",
       " 'ab': 29,\n",
       " 'ac': 30,\n",
       " 'ad': 31,\n",
       " 'ae': 32,\n",
       " 'af': 33,\n",
       " 'ag': 34,\n",
       " 'ah': 35,\n",
       " 'ai': 36,\n",
       " 'aj': 37,\n",
       " 'ak': 38,\n",
       " 'al': 39,\n",
       " 'am': 40,\n",
       " 'an': 41,\n",
       " 'ao': 42,\n",
       " 'ap': 43,\n",
       " 'aq': 44,\n",
       " 'ar': 45,\n",
       " 'as': 46,\n",
       " 'at': 47,\n",
       " 'au': 48,\n",
       " 'av': 49,\n",
       " 'aw': 50,\n",
       " 'ax': 51,\n",
       " 'ay': 52,\n",
       " 'az': 53,\n",
       " 'b.': 54,\n",
       " 'ba': 55,\n",
       " 'bb': 56,\n",
       " 'bc': 57,\n",
       " 'bd': 58,\n",
       " 'be': 59,\n",
       " 'bf': 60,\n",
       " 'bg': 61,\n",
       " 'bh': 62,\n",
       " 'bi': 63,\n",
       " 'bj': 64,\n",
       " 'bk': 65,\n",
       " 'bl': 66,\n",
       " 'bm': 67,\n",
       " 'bn': 68,\n",
       " 'bo': 69,\n",
       " 'bp': 70,\n",
       " 'bq': 71,\n",
       " 'br': 72,\n",
       " 'bs': 73,\n",
       " 'bt': 74,\n",
       " 'bu': 75,\n",
       " 'bv': 76,\n",
       " 'bw': 77,\n",
       " 'bx': 78,\n",
       " 'by': 79,\n",
       " 'bz': 80,\n",
       " 'c.': 81,\n",
       " 'ca': 82,\n",
       " 'cb': 83,\n",
       " 'cc': 84,\n",
       " 'cd': 85,\n",
       " 'ce': 86,\n",
       " 'cf': 87,\n",
       " 'cg': 88,\n",
       " 'ch': 89,\n",
       " 'ci': 90,\n",
       " 'cj': 91,\n",
       " 'ck': 92,\n",
       " 'cl': 93,\n",
       " 'cm': 94,\n",
       " 'cn': 95,\n",
       " 'co': 96,\n",
       " 'cp': 97,\n",
       " 'cq': 98,\n",
       " 'cr': 99,\n",
       " 'cs': 100,\n",
       " 'ct': 101,\n",
       " 'cu': 102,\n",
       " 'cv': 103,\n",
       " 'cw': 104,\n",
       " 'cx': 105,\n",
       " 'cy': 106,\n",
       " 'cz': 107,\n",
       " 'd.': 108,\n",
       " 'da': 109,\n",
       " 'db': 110,\n",
       " 'dc': 111,\n",
       " 'dd': 112,\n",
       " 'de': 113,\n",
       " 'df': 114,\n",
       " 'dg': 115,\n",
       " 'dh': 116,\n",
       " 'di': 117,\n",
       " 'dj': 118,\n",
       " 'dk': 119,\n",
       " 'dl': 120,\n",
       " 'dm': 121,\n",
       " 'dn': 122,\n",
       " 'do': 123,\n",
       " 'dp': 124,\n",
       " 'dq': 125,\n",
       " 'dr': 126,\n",
       " 'ds': 127,\n",
       " 'dt': 128,\n",
       " 'du': 129,\n",
       " 'dv': 130,\n",
       " 'dw': 131,\n",
       " 'dx': 132,\n",
       " 'dy': 133,\n",
       " 'dz': 134,\n",
       " 'e.': 135,\n",
       " 'ea': 136,\n",
       " 'eb': 137,\n",
       " 'ec': 138,\n",
       " 'ed': 139,\n",
       " 'ee': 140,\n",
       " 'ef': 141,\n",
       " 'eg': 142,\n",
       " 'eh': 143,\n",
       " 'ei': 144,\n",
       " 'ej': 145,\n",
       " 'ek': 146,\n",
       " 'el': 147,\n",
       " 'em': 148,\n",
       " 'en': 149,\n",
       " 'eo': 150,\n",
       " 'ep': 151,\n",
       " 'eq': 152,\n",
       " 'er': 153,\n",
       " 'es': 154,\n",
       " 'et': 155,\n",
       " 'eu': 156,\n",
       " 'ev': 157,\n",
       " 'ew': 158,\n",
       " 'ex': 159,\n",
       " 'ey': 160,\n",
       " 'ez': 161,\n",
       " 'f.': 162,\n",
       " 'fa': 163,\n",
       " 'fb': 164,\n",
       " 'fc': 165,\n",
       " 'fd': 166,\n",
       " 'fe': 167,\n",
       " 'ff': 168,\n",
       " 'fg': 169,\n",
       " 'fh': 170,\n",
       " 'fi': 171,\n",
       " 'fj': 172,\n",
       " 'fk': 173,\n",
       " 'fl': 174,\n",
       " 'fm': 175,\n",
       " 'fn': 176,\n",
       " 'fo': 177,\n",
       " 'fp': 178,\n",
       " 'fq': 179,\n",
       " 'fr': 180,\n",
       " 'fs': 181,\n",
       " 'ft': 182,\n",
       " 'fu': 183,\n",
       " 'fv': 184,\n",
       " 'fw': 185,\n",
       " 'fx': 186,\n",
       " 'fy': 187,\n",
       " 'fz': 188,\n",
       " 'g.': 189,\n",
       " 'ga': 190,\n",
       " 'gb': 191,\n",
       " 'gc': 192,\n",
       " 'gd': 193,\n",
       " 'ge': 194,\n",
       " 'gf': 195,\n",
       " 'gg': 196,\n",
       " 'gh': 197,\n",
       " 'gi': 198,\n",
       " 'gj': 199,\n",
       " 'gk': 200,\n",
       " 'gl': 201,\n",
       " 'gm': 202,\n",
       " 'gn': 203,\n",
       " 'go': 204,\n",
       " 'gp': 205,\n",
       " 'gq': 206,\n",
       " 'gr': 207,\n",
       " 'gs': 208,\n",
       " 'gt': 209,\n",
       " 'gu': 210,\n",
       " 'gv': 211,\n",
       " 'gw': 212,\n",
       " 'gx': 213,\n",
       " 'gy': 214,\n",
       " 'gz': 215,\n",
       " 'h.': 216,\n",
       " 'ha': 217,\n",
       " 'hb': 218,\n",
       " 'hc': 219,\n",
       " 'hd': 220,\n",
       " 'he': 221,\n",
       " 'hf': 222,\n",
       " 'hg': 223,\n",
       " 'hh': 224,\n",
       " 'hi': 225,\n",
       " 'hj': 226,\n",
       " 'hk': 227,\n",
       " 'hl': 228,\n",
       " 'hm': 229,\n",
       " 'hn': 230,\n",
       " 'ho': 231,\n",
       " 'hp': 232,\n",
       " 'hq': 233,\n",
       " 'hr': 234,\n",
       " 'hs': 235,\n",
       " 'ht': 236,\n",
       " 'hu': 237,\n",
       " 'hv': 238,\n",
       " 'hw': 239,\n",
       " 'hx': 240,\n",
       " 'hy': 241,\n",
       " 'hz': 242,\n",
       " 'i.': 243,\n",
       " 'ia': 244,\n",
       " 'ib': 245,\n",
       " 'ic': 246,\n",
       " 'id': 247,\n",
       " 'ie': 248,\n",
       " 'if': 249,\n",
       " 'ig': 250,\n",
       " 'ih': 251,\n",
       " 'ii': 252,\n",
       " 'ij': 253,\n",
       " 'ik': 254,\n",
       " 'il': 255,\n",
       " 'im': 256,\n",
       " 'in': 257,\n",
       " 'io': 258,\n",
       " 'ip': 259,\n",
       " 'iq': 260,\n",
       " 'ir': 261,\n",
       " 'is': 262,\n",
       " 'it': 263,\n",
       " 'iu': 264,\n",
       " 'iv': 265,\n",
       " 'iw': 266,\n",
       " 'ix': 267,\n",
       " 'iy': 268,\n",
       " 'iz': 269,\n",
       " 'j.': 270,\n",
       " 'ja': 271,\n",
       " 'jb': 272,\n",
       " 'jc': 273,\n",
       " 'jd': 274,\n",
       " 'je': 275,\n",
       " 'jf': 276,\n",
       " 'jg': 277,\n",
       " 'jh': 278,\n",
       " 'ji': 279,\n",
       " 'jj': 280,\n",
       " 'jk': 281,\n",
       " 'jl': 282,\n",
       " 'jm': 283,\n",
       " 'jn': 284,\n",
       " 'jo': 285,\n",
       " 'jp': 286,\n",
       " 'jq': 287,\n",
       " 'jr': 288,\n",
       " 'js': 289,\n",
       " 'jt': 290,\n",
       " 'ju': 291,\n",
       " 'jv': 292,\n",
       " 'jw': 293,\n",
       " 'jx': 294,\n",
       " 'jy': 295,\n",
       " 'jz': 296,\n",
       " 'k.': 297,\n",
       " 'ka': 298,\n",
       " 'kb': 299,\n",
       " 'kc': 300,\n",
       " 'kd': 301,\n",
       " 'ke': 302,\n",
       " 'kf': 303,\n",
       " 'kg': 304,\n",
       " 'kh': 305,\n",
       " 'ki': 306,\n",
       " 'kj': 307,\n",
       " 'kk': 308,\n",
       " 'kl': 309,\n",
       " 'km': 310,\n",
       " 'kn': 311,\n",
       " 'ko': 312,\n",
       " 'kp': 313,\n",
       " 'kq': 314,\n",
       " 'kr': 315,\n",
       " 'ks': 316,\n",
       " 'kt': 317,\n",
       " 'ku': 318,\n",
       " 'kv': 319,\n",
       " 'kw': 320,\n",
       " 'kx': 321,\n",
       " 'ky': 322,\n",
       " 'kz': 323,\n",
       " 'l.': 324,\n",
       " 'la': 325,\n",
       " 'lb': 326,\n",
       " 'lc': 327,\n",
       " 'ld': 328,\n",
       " 'le': 329,\n",
       " 'lf': 330,\n",
       " 'lg': 331,\n",
       " 'lh': 332,\n",
       " 'li': 333,\n",
       " 'lj': 334,\n",
       " 'lk': 335,\n",
       " 'll': 336,\n",
       " 'lm': 337,\n",
       " 'ln': 338,\n",
       " 'lo': 339,\n",
       " 'lp': 340,\n",
       " 'lq': 341,\n",
       " 'lr': 342,\n",
       " 'ls': 343,\n",
       " 'lt': 344,\n",
       " 'lu': 345,\n",
       " 'lv': 346,\n",
       " 'lw': 347,\n",
       " 'lx': 348,\n",
       " 'ly': 349,\n",
       " 'lz': 350,\n",
       " 'm.': 351,\n",
       " 'ma': 352,\n",
       " 'mb': 353,\n",
       " 'mc': 354,\n",
       " 'md': 355,\n",
       " 'me': 356,\n",
       " 'mf': 357,\n",
       " 'mg': 358,\n",
       " 'mh': 359,\n",
       " 'mi': 360,\n",
       " 'mj': 361,\n",
       " 'mk': 362,\n",
       " 'ml': 363,\n",
       " 'mm': 364,\n",
       " 'mn': 365,\n",
       " 'mo': 366,\n",
       " 'mp': 367,\n",
       " 'mq': 368,\n",
       " 'mr': 369,\n",
       " 'ms': 370,\n",
       " 'mt': 371,\n",
       " 'mu': 372,\n",
       " 'mv': 373,\n",
       " 'mw': 374,\n",
       " 'mx': 375,\n",
       " 'my': 376,\n",
       " 'mz': 377,\n",
       " 'n.': 378,\n",
       " 'na': 379,\n",
       " 'nb': 380,\n",
       " 'nc': 381,\n",
       " 'nd': 382,\n",
       " 'ne': 383,\n",
       " 'nf': 384,\n",
       " 'ng': 385,\n",
       " 'nh': 386,\n",
       " 'ni': 387,\n",
       " 'nj': 388,\n",
       " 'nk': 389,\n",
       " 'nl': 390,\n",
       " 'nm': 391,\n",
       " 'nn': 392,\n",
       " 'no': 393,\n",
       " 'np': 394,\n",
       " 'nq': 395,\n",
       " 'nr': 396,\n",
       " 'ns': 397,\n",
       " 'nt': 398,\n",
       " 'nu': 399,\n",
       " 'nv': 400,\n",
       " 'nw': 401,\n",
       " 'nx': 402,\n",
       " 'ny': 403,\n",
       " 'nz': 404,\n",
       " 'o.': 405,\n",
       " 'oa': 406,\n",
       " 'ob': 407,\n",
       " 'oc': 408,\n",
       " 'od': 409,\n",
       " 'oe': 410,\n",
       " 'of': 411,\n",
       " 'og': 412,\n",
       " 'oh': 413,\n",
       " 'oi': 414,\n",
       " 'oj': 415,\n",
       " 'ok': 416,\n",
       " 'ol': 417,\n",
       " 'om': 418,\n",
       " 'on': 419,\n",
       " 'oo': 420,\n",
       " 'op': 421,\n",
       " 'oq': 422,\n",
       " 'or': 423,\n",
       " 'os': 424,\n",
       " 'ot': 425,\n",
       " 'ou': 426,\n",
       " 'ov': 427,\n",
       " 'ow': 428,\n",
       " 'ox': 429,\n",
       " 'oy': 430,\n",
       " 'oz': 431,\n",
       " 'p.': 432,\n",
       " 'pa': 433,\n",
       " 'pb': 434,\n",
       " 'pc': 435,\n",
       " 'pd': 436,\n",
       " 'pe': 437,\n",
       " 'pf': 438,\n",
       " 'pg': 439,\n",
       " 'ph': 440,\n",
       " 'pi': 441,\n",
       " 'pj': 442,\n",
       " 'pk': 443,\n",
       " 'pl': 444,\n",
       " 'pm': 445,\n",
       " 'pn': 446,\n",
       " 'po': 447,\n",
       " 'pp': 448,\n",
       " 'pq': 449,\n",
       " 'pr': 450,\n",
       " 'ps': 451,\n",
       " 'pt': 452,\n",
       " 'pu': 453,\n",
       " 'pv': 454,\n",
       " 'pw': 455,\n",
       " 'px': 456,\n",
       " 'py': 457,\n",
       " 'pz': 458,\n",
       " 'q.': 459,\n",
       " 'qa': 460,\n",
       " 'qb': 461,\n",
       " 'qc': 462,\n",
       " 'qd': 463,\n",
       " 'qe': 464,\n",
       " 'qf': 465,\n",
       " 'qg': 466,\n",
       " 'qh': 467,\n",
       " 'qi': 468,\n",
       " 'qj': 469,\n",
       " 'qk': 470,\n",
       " 'ql': 471,\n",
       " 'qm': 472,\n",
       " 'qn': 473,\n",
       " 'qo': 474,\n",
       " 'qp': 475,\n",
       " 'qq': 476,\n",
       " 'qr': 477,\n",
       " 'qs': 478,\n",
       " 'qt': 479,\n",
       " 'qu': 480,\n",
       " 'qv': 481,\n",
       " 'qw': 482,\n",
       " 'qx': 483,\n",
       " 'qy': 484,\n",
       " 'qz': 485,\n",
       " 'r.': 486,\n",
       " 'ra': 487,\n",
       " 'rb': 488,\n",
       " 'rc': 489,\n",
       " 'rd': 490,\n",
       " 're': 491,\n",
       " 'rf': 492,\n",
       " 'rg': 493,\n",
       " 'rh': 494,\n",
       " 'ri': 495,\n",
       " 'rj': 496,\n",
       " 'rk': 497,\n",
       " 'rl': 498,\n",
       " 'rm': 499,\n",
       " 'rn': 500,\n",
       " 'ro': 501,\n",
       " 'rp': 502,\n",
       " 'rq': 503,\n",
       " 'rr': 504,\n",
       " 'rs': 505,\n",
       " 'rt': 506,\n",
       " 'ru': 507,\n",
       " 'rv': 508,\n",
       " 'rw': 509,\n",
       " 'rx': 510,\n",
       " 'ry': 511,\n",
       " 'rz': 512,\n",
       " 's.': 513,\n",
       " 'sa': 514,\n",
       " 'sb': 515,\n",
       " 'sc': 516,\n",
       " 'sd': 517,\n",
       " 'se': 518,\n",
       " 'sf': 519,\n",
       " 'sg': 520,\n",
       " 'sh': 521,\n",
       " 'si': 522,\n",
       " 'sj': 523,\n",
       " 'sk': 524,\n",
       " 'sl': 525,\n",
       " 'sm': 526,\n",
       " 'sn': 527,\n",
       " 'so': 528,\n",
       " 'sp': 529,\n",
       " 'sq': 530,\n",
       " 'sr': 531,\n",
       " 'ss': 532,\n",
       " 'st': 533,\n",
       " 'su': 534,\n",
       " 'sv': 535,\n",
       " 'sw': 536,\n",
       " 'sx': 537,\n",
       " 'sy': 538,\n",
       " 'sz': 539,\n",
       " 't.': 540,\n",
       " 'ta': 541,\n",
       " 'tb': 542,\n",
       " 'tc': 543,\n",
       " 'td': 544,\n",
       " 'te': 545,\n",
       " 'tf': 546,\n",
       " 'tg': 547,\n",
       " 'th': 548,\n",
       " 'ti': 549,\n",
       " 'tj': 550,\n",
       " 'tk': 551,\n",
       " 'tl': 552,\n",
       " 'tm': 553,\n",
       " 'tn': 554,\n",
       " 'to': 555,\n",
       " 'tp': 556,\n",
       " 'tq': 557,\n",
       " 'tr': 558,\n",
       " 'ts': 559,\n",
       " 'tt': 560,\n",
       " 'tu': 561,\n",
       " 'tv': 562,\n",
       " 'tw': 563,\n",
       " 'tx': 564,\n",
       " 'ty': 565,\n",
       " 'tz': 566,\n",
       " 'u.': 567,\n",
       " 'ua': 568,\n",
       " 'ub': 569,\n",
       " 'uc': 570,\n",
       " 'ud': 571,\n",
       " 'ue': 572,\n",
       " 'uf': 573,\n",
       " 'ug': 574,\n",
       " 'uh': 575,\n",
       " 'ui': 576,\n",
       " 'uj': 577,\n",
       " 'uk': 578,\n",
       " 'ul': 579,\n",
       " 'um': 580,\n",
       " 'un': 581,\n",
       " 'uo': 582,\n",
       " 'up': 583,\n",
       " 'uq': 584,\n",
       " 'ur': 585,\n",
       " 'us': 586,\n",
       " 'ut': 587,\n",
       " 'uu': 588,\n",
       " 'uv': 589,\n",
       " 'uw': 590,\n",
       " 'ux': 591,\n",
       " 'uy': 592,\n",
       " 'uz': 593,\n",
       " 'v.': 594,\n",
       " 'va': 595,\n",
       " 'vb': 596,\n",
       " 'vc': 597,\n",
       " 'vd': 598,\n",
       " 've': 599,\n",
       " 'vf': 600,\n",
       " 'vg': 601,\n",
       " 'vh': 602,\n",
       " 'vi': 603,\n",
       " 'vj': 604,\n",
       " 'vk': 605,\n",
       " 'vl': 606,\n",
       " 'vm': 607,\n",
       " 'vn': 608,\n",
       " 'vo': 609,\n",
       " 'vp': 610,\n",
       " 'vq': 611,\n",
       " 'vr': 612,\n",
       " 'vs': 613,\n",
       " 'vt': 614,\n",
       " 'vu': 615,\n",
       " 'vv': 616,\n",
       " 'vw': 617,\n",
       " 'vx': 618,\n",
       " 'vy': 619,\n",
       " 'vz': 620,\n",
       " 'w.': 621,\n",
       " 'wa': 622,\n",
       " 'wb': 623,\n",
       " 'wc': 624,\n",
       " 'wd': 625,\n",
       " 'we': 626,\n",
       " 'wf': 627,\n",
       " 'wg': 628,\n",
       " 'wh': 629,\n",
       " 'wi': 630,\n",
       " 'wj': 631,\n",
       " 'wk': 632,\n",
       " 'wl': 633,\n",
       " 'wm': 634,\n",
       " 'wn': 635,\n",
       " 'wo': 636,\n",
       " 'wp': 637,\n",
       " 'wq': 638,\n",
       " 'wr': 639,\n",
       " 'ws': 640,\n",
       " 'wt': 641,\n",
       " 'wu': 642,\n",
       " 'wv': 643,\n",
       " 'ww': 644,\n",
       " 'wx': 645,\n",
       " 'wy': 646,\n",
       " 'wz': 647,\n",
       " 'x.': 648,\n",
       " 'xa': 649,\n",
       " 'xb': 650,\n",
       " 'xc': 651,\n",
       " 'xd': 652,\n",
       " 'xe': 653,\n",
       " 'xf': 654,\n",
       " 'xg': 655,\n",
       " 'xh': 656,\n",
       " 'xi': 657,\n",
       " 'xj': 658,\n",
       " 'xk': 659,\n",
       " 'xl': 660,\n",
       " 'xm': 661,\n",
       " 'xn': 662,\n",
       " 'xo': 663,\n",
       " 'xp': 664,\n",
       " 'xq': 665,\n",
       " 'xr': 666,\n",
       " 'xs': 667,\n",
       " 'xt': 668,\n",
       " 'xu': 669,\n",
       " 'xv': 670,\n",
       " 'xw': 671,\n",
       " 'xx': 672,\n",
       " 'xy': 673,\n",
       " 'xz': 674,\n",
       " 'y.': 675,\n",
       " 'ya': 676,\n",
       " 'yb': 677,\n",
       " 'yc': 678,\n",
       " 'yd': 679,\n",
       " 'ye': 680,\n",
       " 'yf': 681,\n",
       " 'yg': 682,\n",
       " 'yh': 683,\n",
       " 'yi': 684,\n",
       " 'yj': 685,\n",
       " 'yk': 686,\n",
       " 'yl': 687,\n",
       " 'ym': 688,\n",
       " 'yn': 689,\n",
       " 'yo': 690,\n",
       " 'yp': 691,\n",
       " 'yq': 692,\n",
       " 'yr': 693,\n",
       " 'ys': 694,\n",
       " 'yt': 695,\n",
       " 'yu': 696,\n",
       " 'yv': 697,\n",
       " 'yw': 698,\n",
       " 'yx': 699,\n",
       " 'yy': 700,\n",
       " 'yz': 701,\n",
       " 'z.': 702,\n",
       " 'za': 703,\n",
       " 'zb': 704,\n",
       " 'zc': 705,\n",
       " 'zd': 706,\n",
       " 'ze': 707,\n",
       " 'zf': 708,\n",
       " 'zg': 709,\n",
       " 'zh': 710,\n",
       " 'zi': 711,\n",
       " 'zj': 712,\n",
       " 'zk': 713,\n",
       " 'zl': 714,\n",
       " 'zm': 715,\n",
       " 'zn': 716,\n",
       " 'zo': 717,\n",
       " 'zp': 718,\n",
       " 'zq': 719,\n",
       " 'zr': 720,\n",
       " 'zs': 721,\n",
       " 'zt': 722,\n",
       " 'zu': 723,\n",
       " 'zv': 724,\n",
       " 'zw': 725,\n",
       " 'zx': 726,\n",
       " 'zy': 727,\n",
       " 'zz': 728}"
      ]
     },
     "execution_count": 414,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs = []\n",
    "for l in letters + ['.']:\n",
    "    for l2 in letters + ['.']:\n",
    "        pair = l + l2\n",
    "        pairs.append(pair)\n",
    "\n",
    "pairs = sorted(pairs)\n",
    "ptoi = {p:i for i, p in enumerate(pairs)}\n",
    "ptoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '..',\n",
       " 1: '.a',\n",
       " 2: '.b',\n",
       " 3: '.c',\n",
       " 4: '.d',\n",
       " 5: '.e',\n",
       " 6: '.f',\n",
       " 7: '.g',\n",
       " 8: '.h',\n",
       " 9: '.i',\n",
       " 10: '.j',\n",
       " 11: '.k',\n",
       " 12: '.l',\n",
       " 13: '.m',\n",
       " 14: '.n',\n",
       " 15: '.o',\n",
       " 16: '.p',\n",
       " 17: '.q',\n",
       " 18: '.r',\n",
       " 19: '.s',\n",
       " 20: '.t',\n",
       " 21: '.u',\n",
       " 22: '.v',\n",
       " 23: '.w',\n",
       " 24: '.x',\n",
       " 25: '.y',\n",
       " 26: '.z',\n",
       " 27: 'a.',\n",
       " 28: 'aa',\n",
       " 29: 'ab',\n",
       " 30: 'ac',\n",
       " 31: 'ad',\n",
       " 32: 'ae',\n",
       " 33: 'af',\n",
       " 34: 'ag',\n",
       " 35: 'ah',\n",
       " 36: 'ai',\n",
       " 37: 'aj',\n",
       " 38: 'ak',\n",
       " 39: 'al',\n",
       " 40: 'am',\n",
       " 41: 'an',\n",
       " 42: 'ao',\n",
       " 43: 'ap',\n",
       " 44: 'aq',\n",
       " 45: 'ar',\n",
       " 46: 'as',\n",
       " 47: 'at',\n",
       " 48: 'au',\n",
       " 49: 'av',\n",
       " 50: 'aw',\n",
       " 51: 'ax',\n",
       " 52: 'ay',\n",
       " 53: 'az',\n",
       " 54: 'b.',\n",
       " 55: 'ba',\n",
       " 56: 'bb',\n",
       " 57: 'bc',\n",
       " 58: 'bd',\n",
       " 59: 'be',\n",
       " 60: 'bf',\n",
       " 61: 'bg',\n",
       " 62: 'bh',\n",
       " 63: 'bi',\n",
       " 64: 'bj',\n",
       " 65: 'bk',\n",
       " 66: 'bl',\n",
       " 67: 'bm',\n",
       " 68: 'bn',\n",
       " 69: 'bo',\n",
       " 70: 'bp',\n",
       " 71: 'bq',\n",
       " 72: 'br',\n",
       " 73: 'bs',\n",
       " 74: 'bt',\n",
       " 75: 'bu',\n",
       " 76: 'bv',\n",
       " 77: 'bw',\n",
       " 78: 'bx',\n",
       " 79: 'by',\n",
       " 80: 'bz',\n",
       " 81: 'c.',\n",
       " 82: 'ca',\n",
       " 83: 'cb',\n",
       " 84: 'cc',\n",
       " 85: 'cd',\n",
       " 86: 'ce',\n",
       " 87: 'cf',\n",
       " 88: 'cg',\n",
       " 89: 'ch',\n",
       " 90: 'ci',\n",
       " 91: 'cj',\n",
       " 92: 'ck',\n",
       " 93: 'cl',\n",
       " 94: 'cm',\n",
       " 95: 'cn',\n",
       " 96: 'co',\n",
       " 97: 'cp',\n",
       " 98: 'cq',\n",
       " 99: 'cr',\n",
       " 100: 'cs',\n",
       " 101: 'ct',\n",
       " 102: 'cu',\n",
       " 103: 'cv',\n",
       " 104: 'cw',\n",
       " 105: 'cx',\n",
       " 106: 'cy',\n",
       " 107: 'cz',\n",
       " 108: 'd.',\n",
       " 109: 'da',\n",
       " 110: 'db',\n",
       " 111: 'dc',\n",
       " 112: 'dd',\n",
       " 113: 'de',\n",
       " 114: 'df',\n",
       " 115: 'dg',\n",
       " 116: 'dh',\n",
       " 117: 'di',\n",
       " 118: 'dj',\n",
       " 119: 'dk',\n",
       " 120: 'dl',\n",
       " 121: 'dm',\n",
       " 122: 'dn',\n",
       " 123: 'do',\n",
       " 124: 'dp',\n",
       " 125: 'dq',\n",
       " 126: 'dr',\n",
       " 127: 'ds',\n",
       " 128: 'dt',\n",
       " 129: 'du',\n",
       " 130: 'dv',\n",
       " 131: 'dw',\n",
       " 132: 'dx',\n",
       " 133: 'dy',\n",
       " 134: 'dz',\n",
       " 135: 'e.',\n",
       " 136: 'ea',\n",
       " 137: 'eb',\n",
       " 138: 'ec',\n",
       " 139: 'ed',\n",
       " 140: 'ee',\n",
       " 141: 'ef',\n",
       " 142: 'eg',\n",
       " 143: 'eh',\n",
       " 144: 'ei',\n",
       " 145: 'ej',\n",
       " 146: 'ek',\n",
       " 147: 'el',\n",
       " 148: 'em',\n",
       " 149: 'en',\n",
       " 150: 'eo',\n",
       " 151: 'ep',\n",
       " 152: 'eq',\n",
       " 153: 'er',\n",
       " 154: 'es',\n",
       " 155: 'et',\n",
       " 156: 'eu',\n",
       " 157: 'ev',\n",
       " 158: 'ew',\n",
       " 159: 'ex',\n",
       " 160: 'ey',\n",
       " 161: 'ez',\n",
       " 162: 'f.',\n",
       " 163: 'fa',\n",
       " 164: 'fb',\n",
       " 165: 'fc',\n",
       " 166: 'fd',\n",
       " 167: 'fe',\n",
       " 168: 'ff',\n",
       " 169: 'fg',\n",
       " 170: 'fh',\n",
       " 171: 'fi',\n",
       " 172: 'fj',\n",
       " 173: 'fk',\n",
       " 174: 'fl',\n",
       " 175: 'fm',\n",
       " 176: 'fn',\n",
       " 177: 'fo',\n",
       " 178: 'fp',\n",
       " 179: 'fq',\n",
       " 180: 'fr',\n",
       " 181: 'fs',\n",
       " 182: 'ft',\n",
       " 183: 'fu',\n",
       " 184: 'fv',\n",
       " 185: 'fw',\n",
       " 186: 'fx',\n",
       " 187: 'fy',\n",
       " 188: 'fz',\n",
       " 189: 'g.',\n",
       " 190: 'ga',\n",
       " 191: 'gb',\n",
       " 192: 'gc',\n",
       " 193: 'gd',\n",
       " 194: 'ge',\n",
       " 195: 'gf',\n",
       " 196: 'gg',\n",
       " 197: 'gh',\n",
       " 198: 'gi',\n",
       " 199: 'gj',\n",
       " 200: 'gk',\n",
       " 201: 'gl',\n",
       " 202: 'gm',\n",
       " 203: 'gn',\n",
       " 204: 'go',\n",
       " 205: 'gp',\n",
       " 206: 'gq',\n",
       " 207: 'gr',\n",
       " 208: 'gs',\n",
       " 209: 'gt',\n",
       " 210: 'gu',\n",
       " 211: 'gv',\n",
       " 212: 'gw',\n",
       " 213: 'gx',\n",
       " 214: 'gy',\n",
       " 215: 'gz',\n",
       " 216: 'h.',\n",
       " 217: 'ha',\n",
       " 218: 'hb',\n",
       " 219: 'hc',\n",
       " 220: 'hd',\n",
       " 221: 'he',\n",
       " 222: 'hf',\n",
       " 223: 'hg',\n",
       " 224: 'hh',\n",
       " 225: 'hi',\n",
       " 226: 'hj',\n",
       " 227: 'hk',\n",
       " 228: 'hl',\n",
       " 229: 'hm',\n",
       " 230: 'hn',\n",
       " 231: 'ho',\n",
       " 232: 'hp',\n",
       " 233: 'hq',\n",
       " 234: 'hr',\n",
       " 235: 'hs',\n",
       " 236: 'ht',\n",
       " 237: 'hu',\n",
       " 238: 'hv',\n",
       " 239: 'hw',\n",
       " 240: 'hx',\n",
       " 241: 'hy',\n",
       " 242: 'hz',\n",
       " 243: 'i.',\n",
       " 244: 'ia',\n",
       " 245: 'ib',\n",
       " 246: 'ic',\n",
       " 247: 'id',\n",
       " 248: 'ie',\n",
       " 249: 'if',\n",
       " 250: 'ig',\n",
       " 251: 'ih',\n",
       " 252: 'ii',\n",
       " 253: 'ij',\n",
       " 254: 'ik',\n",
       " 255: 'il',\n",
       " 256: 'im',\n",
       " 257: 'in',\n",
       " 258: 'io',\n",
       " 259: 'ip',\n",
       " 260: 'iq',\n",
       " 261: 'ir',\n",
       " 262: 'is',\n",
       " 263: 'it',\n",
       " 264: 'iu',\n",
       " 265: 'iv',\n",
       " 266: 'iw',\n",
       " 267: 'ix',\n",
       " 268: 'iy',\n",
       " 269: 'iz',\n",
       " 270: 'j.',\n",
       " 271: 'ja',\n",
       " 272: 'jb',\n",
       " 273: 'jc',\n",
       " 274: 'jd',\n",
       " 275: 'je',\n",
       " 276: 'jf',\n",
       " 277: 'jg',\n",
       " 278: 'jh',\n",
       " 279: 'ji',\n",
       " 280: 'jj',\n",
       " 281: 'jk',\n",
       " 282: 'jl',\n",
       " 283: 'jm',\n",
       " 284: 'jn',\n",
       " 285: 'jo',\n",
       " 286: 'jp',\n",
       " 287: 'jq',\n",
       " 288: 'jr',\n",
       " 289: 'js',\n",
       " 290: 'jt',\n",
       " 291: 'ju',\n",
       " 292: 'jv',\n",
       " 293: 'jw',\n",
       " 294: 'jx',\n",
       " 295: 'jy',\n",
       " 296: 'jz',\n",
       " 297: 'k.',\n",
       " 298: 'ka',\n",
       " 299: 'kb',\n",
       " 300: 'kc',\n",
       " 301: 'kd',\n",
       " 302: 'ke',\n",
       " 303: 'kf',\n",
       " 304: 'kg',\n",
       " 305: 'kh',\n",
       " 306: 'ki',\n",
       " 307: 'kj',\n",
       " 308: 'kk',\n",
       " 309: 'kl',\n",
       " 310: 'km',\n",
       " 311: 'kn',\n",
       " 312: 'ko',\n",
       " 313: 'kp',\n",
       " 314: 'kq',\n",
       " 315: 'kr',\n",
       " 316: 'ks',\n",
       " 317: 'kt',\n",
       " 318: 'ku',\n",
       " 319: 'kv',\n",
       " 320: 'kw',\n",
       " 321: 'kx',\n",
       " 322: 'ky',\n",
       " 323: 'kz',\n",
       " 324: 'l.',\n",
       " 325: 'la',\n",
       " 326: 'lb',\n",
       " 327: 'lc',\n",
       " 328: 'ld',\n",
       " 329: 'le',\n",
       " 330: 'lf',\n",
       " 331: 'lg',\n",
       " 332: 'lh',\n",
       " 333: 'li',\n",
       " 334: 'lj',\n",
       " 335: 'lk',\n",
       " 336: 'll',\n",
       " 337: 'lm',\n",
       " 338: 'ln',\n",
       " 339: 'lo',\n",
       " 340: 'lp',\n",
       " 341: 'lq',\n",
       " 342: 'lr',\n",
       " 343: 'ls',\n",
       " 344: 'lt',\n",
       " 345: 'lu',\n",
       " 346: 'lv',\n",
       " 347: 'lw',\n",
       " 348: 'lx',\n",
       " 349: 'ly',\n",
       " 350: 'lz',\n",
       " 351: 'm.',\n",
       " 352: 'ma',\n",
       " 353: 'mb',\n",
       " 354: 'mc',\n",
       " 355: 'md',\n",
       " 356: 'me',\n",
       " 357: 'mf',\n",
       " 358: 'mg',\n",
       " 359: 'mh',\n",
       " 360: 'mi',\n",
       " 361: 'mj',\n",
       " 362: 'mk',\n",
       " 363: 'ml',\n",
       " 364: 'mm',\n",
       " 365: 'mn',\n",
       " 366: 'mo',\n",
       " 367: 'mp',\n",
       " 368: 'mq',\n",
       " 369: 'mr',\n",
       " 370: 'ms',\n",
       " 371: 'mt',\n",
       " 372: 'mu',\n",
       " 373: 'mv',\n",
       " 374: 'mw',\n",
       " 375: 'mx',\n",
       " 376: 'my',\n",
       " 377: 'mz',\n",
       " 378: 'n.',\n",
       " 379: 'na',\n",
       " 380: 'nb',\n",
       " 381: 'nc',\n",
       " 382: 'nd',\n",
       " 383: 'ne',\n",
       " 384: 'nf',\n",
       " 385: 'ng',\n",
       " 386: 'nh',\n",
       " 387: 'ni',\n",
       " 388: 'nj',\n",
       " 389: 'nk',\n",
       " 390: 'nl',\n",
       " 391: 'nm',\n",
       " 392: 'nn',\n",
       " 393: 'no',\n",
       " 394: 'np',\n",
       " 395: 'nq',\n",
       " 396: 'nr',\n",
       " 397: 'ns',\n",
       " 398: 'nt',\n",
       " 399: 'nu',\n",
       " 400: 'nv',\n",
       " 401: 'nw',\n",
       " 402: 'nx',\n",
       " 403: 'ny',\n",
       " 404: 'nz',\n",
       " 405: 'o.',\n",
       " 406: 'oa',\n",
       " 407: 'ob',\n",
       " 408: 'oc',\n",
       " 409: 'od',\n",
       " 410: 'oe',\n",
       " 411: 'of',\n",
       " 412: 'og',\n",
       " 413: 'oh',\n",
       " 414: 'oi',\n",
       " 415: 'oj',\n",
       " 416: 'ok',\n",
       " 417: 'ol',\n",
       " 418: 'om',\n",
       " 419: 'on',\n",
       " 420: 'oo',\n",
       " 421: 'op',\n",
       " 422: 'oq',\n",
       " 423: 'or',\n",
       " 424: 'os',\n",
       " 425: 'ot',\n",
       " 426: 'ou',\n",
       " 427: 'ov',\n",
       " 428: 'ow',\n",
       " 429: 'ox',\n",
       " 430: 'oy',\n",
       " 431: 'oz',\n",
       " 432: 'p.',\n",
       " 433: 'pa',\n",
       " 434: 'pb',\n",
       " 435: 'pc',\n",
       " 436: 'pd',\n",
       " 437: 'pe',\n",
       " 438: 'pf',\n",
       " 439: 'pg',\n",
       " 440: 'ph',\n",
       " 441: 'pi',\n",
       " 442: 'pj',\n",
       " 443: 'pk',\n",
       " 444: 'pl',\n",
       " 445: 'pm',\n",
       " 446: 'pn',\n",
       " 447: 'po',\n",
       " 448: 'pp',\n",
       " 449: 'pq',\n",
       " 450: 'pr',\n",
       " 451: 'ps',\n",
       " 452: 'pt',\n",
       " 453: 'pu',\n",
       " 454: 'pv',\n",
       " 455: 'pw',\n",
       " 456: 'px',\n",
       " 457: 'py',\n",
       " 458: 'pz',\n",
       " 459: 'q.',\n",
       " 460: 'qa',\n",
       " 461: 'qb',\n",
       " 462: 'qc',\n",
       " 463: 'qd',\n",
       " 464: 'qe',\n",
       " 465: 'qf',\n",
       " 466: 'qg',\n",
       " 467: 'qh',\n",
       " 468: 'qi',\n",
       " 469: 'qj',\n",
       " 470: 'qk',\n",
       " 471: 'ql',\n",
       " 472: 'qm',\n",
       " 473: 'qn',\n",
       " 474: 'qo',\n",
       " 475: 'qp',\n",
       " 476: 'qq',\n",
       " 477: 'qr',\n",
       " 478: 'qs',\n",
       " 479: 'qt',\n",
       " 480: 'qu',\n",
       " 481: 'qv',\n",
       " 482: 'qw',\n",
       " 483: 'qx',\n",
       " 484: 'qy',\n",
       " 485: 'qz',\n",
       " 486: 'r.',\n",
       " 487: 'ra',\n",
       " 488: 'rb',\n",
       " 489: 'rc',\n",
       " 490: 'rd',\n",
       " 491: 're',\n",
       " 492: 'rf',\n",
       " 493: 'rg',\n",
       " 494: 'rh',\n",
       " 495: 'ri',\n",
       " 496: 'rj',\n",
       " 497: 'rk',\n",
       " 498: 'rl',\n",
       " 499: 'rm',\n",
       " 500: 'rn',\n",
       " 501: 'ro',\n",
       " 502: 'rp',\n",
       " 503: 'rq',\n",
       " 504: 'rr',\n",
       " 505: 'rs',\n",
       " 506: 'rt',\n",
       " 507: 'ru',\n",
       " 508: 'rv',\n",
       " 509: 'rw',\n",
       " 510: 'rx',\n",
       " 511: 'ry',\n",
       " 512: 'rz',\n",
       " 513: 's.',\n",
       " 514: 'sa',\n",
       " 515: 'sb',\n",
       " 516: 'sc',\n",
       " 517: 'sd',\n",
       " 518: 'se',\n",
       " 519: 'sf',\n",
       " 520: 'sg',\n",
       " 521: 'sh',\n",
       " 522: 'si',\n",
       " 523: 'sj',\n",
       " 524: 'sk',\n",
       " 525: 'sl',\n",
       " 526: 'sm',\n",
       " 527: 'sn',\n",
       " 528: 'so',\n",
       " 529: 'sp',\n",
       " 530: 'sq',\n",
       " 531: 'sr',\n",
       " 532: 'ss',\n",
       " 533: 'st',\n",
       " 534: 'su',\n",
       " 535: 'sv',\n",
       " 536: 'sw',\n",
       " 537: 'sx',\n",
       " 538: 'sy',\n",
       " 539: 'sz',\n",
       " 540: 't.',\n",
       " 541: 'ta',\n",
       " 542: 'tb',\n",
       " 543: 'tc',\n",
       " 544: 'td',\n",
       " 545: 'te',\n",
       " 546: 'tf',\n",
       " 547: 'tg',\n",
       " 548: 'th',\n",
       " 549: 'ti',\n",
       " 550: 'tj',\n",
       " 551: 'tk',\n",
       " 552: 'tl',\n",
       " 553: 'tm',\n",
       " 554: 'tn',\n",
       " 555: 'to',\n",
       " 556: 'tp',\n",
       " 557: 'tq',\n",
       " 558: 'tr',\n",
       " 559: 'ts',\n",
       " 560: 'tt',\n",
       " 561: 'tu',\n",
       " 562: 'tv',\n",
       " 563: 'tw',\n",
       " 564: 'tx',\n",
       " 565: 'ty',\n",
       " 566: 'tz',\n",
       " 567: 'u.',\n",
       " 568: 'ua',\n",
       " 569: 'ub',\n",
       " 570: 'uc',\n",
       " 571: 'ud',\n",
       " 572: 'ue',\n",
       " 573: 'uf',\n",
       " 574: 'ug',\n",
       " 575: 'uh',\n",
       " 576: 'ui',\n",
       " 577: 'uj',\n",
       " 578: 'uk',\n",
       " 579: 'ul',\n",
       " 580: 'um',\n",
       " 581: 'un',\n",
       " 582: 'uo',\n",
       " 583: 'up',\n",
       " 584: 'uq',\n",
       " 585: 'ur',\n",
       " 586: 'us',\n",
       " 587: 'ut',\n",
       " 588: 'uu',\n",
       " 589: 'uv',\n",
       " 590: 'uw',\n",
       " 591: 'ux',\n",
       " 592: 'uy',\n",
       " 593: 'uz',\n",
       " 594: 'v.',\n",
       " 595: 'va',\n",
       " 596: 'vb',\n",
       " 597: 'vc',\n",
       " 598: 'vd',\n",
       " 599: 've',\n",
       " 600: 'vf',\n",
       " 601: 'vg',\n",
       " 602: 'vh',\n",
       " 603: 'vi',\n",
       " 604: 'vj',\n",
       " 605: 'vk',\n",
       " 606: 'vl',\n",
       " 607: 'vm',\n",
       " 608: 'vn',\n",
       " 609: 'vo',\n",
       " 610: 'vp',\n",
       " 611: 'vq',\n",
       " 612: 'vr',\n",
       " 613: 'vs',\n",
       " 614: 'vt',\n",
       " 615: 'vu',\n",
       " 616: 'vv',\n",
       " 617: 'vw',\n",
       " 618: 'vx',\n",
       " 619: 'vy',\n",
       " 620: 'vz',\n",
       " 621: 'w.',\n",
       " 622: 'wa',\n",
       " 623: 'wb',\n",
       " 624: 'wc',\n",
       " 625: 'wd',\n",
       " 626: 'we',\n",
       " 627: 'wf',\n",
       " 628: 'wg',\n",
       " 629: 'wh',\n",
       " 630: 'wi',\n",
       " 631: 'wj',\n",
       " 632: 'wk',\n",
       " 633: 'wl',\n",
       " 634: 'wm',\n",
       " 635: 'wn',\n",
       " 636: 'wo',\n",
       " 637: 'wp',\n",
       " 638: 'wq',\n",
       " 639: 'wr',\n",
       " 640: 'ws',\n",
       " 641: 'wt',\n",
       " 642: 'wu',\n",
       " 643: 'wv',\n",
       " 644: 'ww',\n",
       " 645: 'wx',\n",
       " 646: 'wy',\n",
       " 647: 'wz',\n",
       " 648: 'x.',\n",
       " 649: 'xa',\n",
       " 650: 'xb',\n",
       " 651: 'xc',\n",
       " 652: 'xd',\n",
       " 653: 'xe',\n",
       " 654: 'xf',\n",
       " 655: 'xg',\n",
       " 656: 'xh',\n",
       " 657: 'xi',\n",
       " 658: 'xj',\n",
       " 659: 'xk',\n",
       " 660: 'xl',\n",
       " 661: 'xm',\n",
       " 662: 'xn',\n",
       " 663: 'xo',\n",
       " 664: 'xp',\n",
       " 665: 'xq',\n",
       " 666: 'xr',\n",
       " 667: 'xs',\n",
       " 668: 'xt',\n",
       " 669: 'xu',\n",
       " 670: 'xv',\n",
       " 671: 'xw',\n",
       " 672: 'xx',\n",
       " 673: 'xy',\n",
       " 674: 'xz',\n",
       " 675: 'y.',\n",
       " 676: 'ya',\n",
       " 677: 'yb',\n",
       " 678: 'yc',\n",
       " 679: 'yd',\n",
       " 680: 'ye',\n",
       " 681: 'yf',\n",
       " 682: 'yg',\n",
       " 683: 'yh',\n",
       " 684: 'yi',\n",
       " 685: 'yj',\n",
       " 686: 'yk',\n",
       " 687: 'yl',\n",
       " 688: 'ym',\n",
       " 689: 'yn',\n",
       " 690: 'yo',\n",
       " 691: 'yp',\n",
       " 692: 'yq',\n",
       " 693: 'yr',\n",
       " 694: 'ys',\n",
       " 695: 'yt',\n",
       " 696: 'yu',\n",
       " 697: 'yv',\n",
       " 698: 'yw',\n",
       " 699: 'yx',\n",
       " 700: 'yy',\n",
       " 701: 'yz',\n",
       " 702: 'z.',\n",
       " 703: 'za',\n",
       " 704: 'zb',\n",
       " 705: 'zc',\n",
       " 706: 'zd',\n",
       " 707: 'ze',\n",
       " 708: 'zf',\n",
       " 709: 'zg',\n",
       " 710: 'zh',\n",
       " 711: 'zi',\n",
       " 712: 'zj',\n",
       " 713: 'zk',\n",
       " 714: 'zl',\n",
       " 715: 'zm',\n",
       " 716: 'zn',\n",
       " 717: 'zo',\n",
       " 718: 'zp',\n",
       " 719: 'zq',\n",
       " 720: 'zr',\n",
       " 721: 'zs',\n",
       " 722: 'zt',\n",
       " 723: 'zu',\n",
       " 724: 'zv',\n",
       " 725: 'zw',\n",
       " 726: 'zx',\n",
       " 727: 'zy',\n",
       " 728: 'zz'}"
      ]
     },
     "execution_count": 415,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "itop = {i:p for p, i in ptoi.items()}\n",
    "itop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "729"
      ]
     },
     "execution_count": 416,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ptoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([5,\n",
       "  148,\n",
       "  364,\n",
       "  352,\n",
       "  15,\n",
       "  417,\n",
       "  333,\n",
       "  265,\n",
       "  603,\n",
       "  244,\n",
       "  1,\n",
       "  49,\n",
       "  595,\n",
       "  9,\n",
       "  262,\n",
       "  514,\n",
       "  29,\n",
       "  59,\n",
       "  147,\n",
       "  336,\n",
       "  325,\n",
       "  19,\n",
       "  528,\n",
       "  421,\n",
       "  440,\n",
       "  225,\n",
       "  244,\n",
       "  3,\n",
       "  89,\n",
       "  217,\n",
       "  45,\n",
       "  498,\n",
       "  339,\n",
       "  425,\n",
       "  560,\n",
       "  545,\n",
       "  13,\n",
       "  360,\n",
       "  244,\n",
       "  1,\n",
       "  40,\n",
       "  356,\n",
       "  147,\n",
       "  333,\n",
       "  244,\n",
       "  8,\n",
       "  217,\n",
       "  45,\n",
       "  502,\n",
       "  437,\n",
       "  153,\n",
       "  5,\n",
       "  157,\n",
       "  599,\n",
       "  147,\n",
       "  349,\n",
       "  689,\n",
       "  1,\n",
       "  29,\n",
       "  63,\n",
       "  250,\n",
       "  190,\n",
       "  36,\n",
       "  255,\n",
       "  5,\n",
       "  148,\n",
       "  360,\n",
       "  255,\n",
       "  349,\n",
       "  5,\n",
       "  147,\n",
       "  333,\n",
       "  269,\n",
       "  703,\n",
       "  29,\n",
       "  59,\n",
       "  155,\n",
       "  548,\n",
       "  13,\n",
       "  360,\n",
       "  255,\n",
       "  325,\n",
       "  5,\n",
       "  147,\n",
       "  336,\n",
       "  325,\n",
       "  1,\n",
       "  49,\n",
       "  599,\n",
       "  153,\n",
       "  511,\n",
       "  19,\n",
       "  528,\n",
       "  411,\n",
       "  171,\n",
       "  244,\n",
       "  3,\n",
       "  82,\n",
       "  40,\n",
       "  360,\n",
       "  255,\n",
       "  325,\n",
       "  1,\n",
       "  45,\n",
       "  495,\n",
       "  244,\n",
       "  19,\n",
       "  516,\n",
       "  82,\n",
       "  45,\n",
       "  498,\n",
       "  329,\n",
       "  155,\n",
       "  560,\n",
       "  22,\n",
       "  603,\n",
       "  246,\n",
       "  101,\n",
       "  555,\n",
       "  423,\n",
       "  495,\n",
       "  244,\n",
       "  13,\n",
       "  352,\n",
       "  31,\n",
       "  117,\n",
       "  262,\n",
       "  528,\n",
       "  419,\n",
       "  12,\n",
       "  345,\n",
       "  581,\n",
       "  379,\n",
       "  7,\n",
       "  207,\n",
       "  487,\n",
       "  30,\n",
       "  86,\n",
       "  3,\n",
       "  89,\n",
       "  228,\n",
       "  339,\n",
       "  410,\n",
       "  16,\n",
       "  437,\n",
       "  149,\n",
       "  383,\n",
       "  147,\n",
       "  339,\n",
       "  421,\n",
       "  437,\n",
       "  12,\n",
       "  325,\n",
       "  52,\n",
       "  687,\n",
       "  325,\n",
       "  18,\n",
       "  495,\n",
       "  255,\n",
       "  329,\n",
       "  160,\n",
       "  26,\n",
       "  717,\n",
       "  410,\n",
       "  160,\n",
       "  14,\n",
       "  393,\n",
       "  423,\n",
       "  487,\n",
       "  12,\n",
       "  333,\n",
       "  255,\n",
       "  349,\n",
       "  5,\n",
       "  147,\n",
       "  329,\n",
       "  136,\n",
       "  41,\n",
       "  393,\n",
       "  423,\n",
       "  8,\n",
       "  217,\n",
       "  41,\n",
       "  392,\n",
       "  379,\n",
       "  35,\n",
       "  12,\n",
       "  333,\n",
       "  255,\n",
       "  336,\n",
       "  333,\n",
       "  244,\n",
       "  41,\n",
       "  1,\n",
       "  31,\n",
       "  112,\n",
       "  117,\n",
       "  262,\n",
       "  528,\n",
       "  419,\n",
       "  1,\n",
       "  48,\n",
       "  569,\n",
       "  72,\n",
       "  491,\n",
       "  160,\n",
       "  5,\n",
       "  147,\n",
       "  336,\n",
       "  333,\n",
       "  248,\n",
       "  19,\n",
       "  533,\n",
       "  545,\n",
       "  147,\n",
       "  336,\n",
       "  325,\n",
       "  14,\n",
       "  379,\n",
       "  47,\n",
       "  541,\n",
       "  39,\n",
       "  333,\n",
       "  248,\n",
       "  26,\n",
       "  717,\n",
       "  410,\n",
       "  12,\n",
       "  329,\n",
       "  136,\n",
       "  35,\n",
       "  8,\n",
       "  217,\n",
       "  53,\n",
       "  707,\n",
       "  147,\n",
       "  22,\n",
       "  603,\n",
       "  258,\n",
       "  417,\n",
       "  329,\n",
       "  155,\n",
       "  1,\n",
       "  48,\n",
       "  585,\n",
       "  501,\n",
       "  423,\n",
       "  487,\n",
       "  19,\n",
       "  514,\n",
       "  49,\n",
       "  595,\n",
       "  41,\n",
       "  392,\n",
       "  379,\n",
       "  35,\n",
       "  1,\n",
       "  48,\n",
       "  571,\n",
       "  126,\n",
       "  491,\n",
       "  160,\n",
       "  2,\n",
       "  72,\n",
       "  501,\n",
       "  420,\n",
       "  416,\n",
       "  309,\n",
       "  349,\n",
       "  689,\n",
       "  2,\n",
       "  59,\n",
       "  147,\n",
       "  336,\n",
       "  325,\n",
       "  3,\n",
       "  93,\n",
       "  325,\n",
       "  36,\n",
       "  261,\n",
       "  491,\n",
       "  19,\n",
       "  524,\n",
       "  322,\n",
       "  687,\n",
       "  325,\n",
       "  45,\n",
       "  12,\n",
       "  345,\n",
       "  570,\n",
       "  106,\n",
       "  16,\n",
       "  433,\n",
       "  36,\n",
       "  262,\n",
       "  525,\n",
       "  329,\n",
       "  160,\n",
       "  5,\n",
       "  157,\n",
       "  599,\n",
       "  153,\n",
       "  498,\n",
       "  349,\n",
       "  1,\n",
       "  41,\n",
       "  392,\n",
       "  379,\n",
       "  3,\n",
       "  82,\n",
       "  45,\n",
       "  501,\n",
       "  417,\n",
       "  333,\n",
       "  257,\n",
       "  383,\n",
       "  14,\n",
       "  393,\n",
       "  427,\n",
       "  595,\n",
       "  7,\n",
       "  194,\n",
       "  149,\n",
       "  383,\n",
       "  154,\n",
       "  522,\n",
       "  262,\n",
       "  5,\n",
       "  148,\n",
       "  360,\n",
       "  255,\n",
       "  333,\n",
       "  244,\n",
       "  11,\n",
       "  302,\n",
       "  149,\n",
       "  392,\n",
       "  383,\n",
       "  139,\n",
       "  133,\n",
       "  19,\n",
       "  514,\n",
       "  40,\n",
       "  352,\n",
       "  41,\n",
       "  398,\n",
       "  548,\n",
       "  217,\n",
       "  13,\n",
       "  352,\n",
       "  52,\n",
       "  676,\n",
       "  23,\n",
       "  630,\n",
       "  255,\n",
       "  336,\n",
       "  339,\n",
       "  428,\n",
       "  11,\n",
       "  306,\n",
       "  257,\n",
       "  397,\n",
       "  525,\n",
       "  329,\n",
       "  160,\n",
       "  14,\n",
       "  379,\n",
       "  42,\n",
       "  418,\n",
       "  360,\n",
       "  1,\n",
       "  28,\n",
       "  39,\n",
       "  333,\n",
       "  268,\n",
       "  676,\n",
       "  35,\n",
       "  5,\n",
       "  147,\n",
       "  329,\n",
       "  149,\n",
       "  379,\n",
       "  19,\n",
       "  514,\n",
       "  45,\n",
       "  487,\n",
       "  35,\n",
       "  1,\n",
       "  45,\n",
       "  495,\n",
       "  244,\n",
       "  41,\n",
       "  379,\n",
       "  1,\n",
       "  39,\n",
       "  336,\n",
       "  333,\n",
       "  262,\n",
       "  528,\n",
       "  419,\n",
       "  7,\n",
       "  190,\n",
       "  29,\n",
       "  72,\n",
       "  495,\n",
       "  248,\n",
       "  147,\n",
       "  336,\n",
       "  325,\n",
       "  1,\n",
       "  39,\n",
       "  333,\n",
       "  246,\n",
       "  86,\n",
       "  13,\n",
       "  352,\n",
       "  31,\n",
       "  113,\n",
       "  147,\n",
       "  349,\n",
       "  689,\n",
       "  3,\n",
       "  96,\n",
       "  423,\n",
       "  487,\n",
       "  18,\n",
       "  507,\n",
       "  569,\n",
       "  79,\n",
       "  5,\n",
       "  157,\n",
       "  595,\n",
       "  19,\n",
       "  518,\n",
       "  153,\n",
       "  491,\n",
       "  149,\n",
       "  387,\n",
       "  263,\n",
       "  565,\n",
       "  1,\n",
       "  48,\n",
       "  587,\n",
       "  561,\n",
       "  580,\n",
       "  365,\n",
       "  1,\n",
       "  31,\n",
       "  113,\n",
       "  147,\n",
       "  333,\n",
       "  257,\n",
       "  383,\n",
       "  8,\n",
       "  217,\n",
       "  36,\n",
       "  255,\n",
       "  329,\n",
       "  160,\n",
       "  7,\n",
       "  198,\n",
       "  244,\n",
       "  41,\n",
       "  392,\n",
       "  379,\n",
       "  22,\n",
       "  595,\n",
       "  39,\n",
       "  329,\n",
       "  149,\n",
       "  398,\n",
       "  549,\n",
       "  257,\n",
       "  379,\n",
       "  9,\n",
       "  262,\n",
       "  525,\n",
       "  325,\n",
       "  5,\n",
       "  147,\n",
       "  333,\n",
       "  244,\n",
       "  41,\n",
       "  379,\n",
       "  17,\n",
       "  480,\n",
       "  576,\n",
       "  257,\n",
       "  392,\n",
       "  14,\n",
       "  383,\n",
       "  157,\n",
       "  595,\n",
       "  32,\n",
       "  143,\n",
       "  9,\n",
       "  265,\n",
       "  619,\n",
       "  19,\n",
       "  514,\n",
       "  31,\n",
       "  117,\n",
       "  248,\n",
       "  16,\n",
       "  441,\n",
       "  259,\n",
       "  437,\n",
       "  153,\n",
       "  12,\n",
       "  349,\n",
       "  679,\n",
       "  117,\n",
       "  244,\n",
       "  1,\n",
       "  39,\n",
       "  329,\n",
       "  159,\n",
       "  649,\n",
       "  10,\n",
       "  285,\n",
       "  424,\n",
       "  518,\n",
       "  151,\n",
       "  440,\n",
       "  225,\n",
       "  257,\n",
       "  383,\n",
       "  5,\n",
       "  148,\n",
       "  356,\n",
       "  153,\n",
       "  511,\n",
       "  10,\n",
       "  291,\n",
       "  579,\n",
       "  333,\n",
       "  244,\n",
       "  4,\n",
       "  113,\n",
       "  147,\n",
       "  333,\n",
       "  255,\n",
       "  325,\n",
       "  35,\n",
       "  1,\n",
       "  45,\n",
       "  495,\n",
       "  244,\n",
       "  41,\n",
       "  392,\n",
       "  379,\n",
       "  22,\n",
       "  603,\n",
       "  265,\n",
       "  603,\n",
       "  244,\n",
       "  41,\n",
       "  11,\n",
       "  298,\n",
       "  52,\n",
       "  687,\n",
       "  329,\n",
       "  140,\n",
       "  19,\n",
       "  528,\n",
       "  421,\n",
       "  440,\n",
       "  225,\n",
       "  248,\n",
       "  2,\n",
       "  72,\n",
       "  495,\n",
       "  248,\n",
       "  147,\n",
       "  336,\n",
       "  329,\n",
       "  13,\n",
       "  352,\n",
       "  31,\n",
       "  113,\n",
       "  147,\n",
       "  333,\n",
       "  257,\n",
       "  383,\n",
       "  16,\n",
       "  437,\n",
       "  160,\n",
       "  695,\n",
       "  555,\n",
       "  419,\n",
       "  18,\n",
       "  511,\n",
       "  687,\n",
       "  329,\n",
       "  140,\n",
       "  3,\n",
       "  93,\n",
       "  325,\n",
       "  45,\n",
       "  487,\n",
       "  8,\n",
       "  217,\n",
       "  31,\n",
       "  120,\n",
       "  329,\n",
       "  160,\n",
       "  13,\n",
       "  356,\n",
       "  147,\n",
       "  325,\n",
       "  41,\n",
       "  387,\n",
       "  248,\n",
       "  13,\n",
       "  352,\n",
       "  30,\n",
       "  92,\n",
       "  302,\n",
       "  149,\n",
       "  404,\n",
       "  711,\n",
       "  248,\n",
       "  18,\n",
       "  491,\n",
       "  136,\n",
       "  34,\n",
       "  190,\n",
       "  41,\n",
       "  1,\n",
       "  31,\n",
       "  109,\n",
       "  39,\n",
       "  349,\n",
       "  689,\n",
       "  392,\n",
       "  12,\n",
       "  333,\n",
       "  255,\n",
       "  333,\n",
       "  244,\n",
       "  41,\n",
       "  379,\n",
       "  1,\n",
       "  48,\n",
       "  569,\n",
       "  72,\n",
       "  491,\n",
       "  140,\n",
       "  10,\n",
       "  271,\n",
       "  31,\n",
       "  113,\n",
       "  11,\n",
       "  298,\n",
       "  47,\n",
       "  548,\n",
       "  221,\n",
       "  153,\n",
       "  495,\n",
       "  257,\n",
       "  383,\n",
       "  9,\n",
       "  262,\n",
       "  514,\n",
       "  29,\n",
       "  59,\n",
       "  147,\n",
       "  336,\n",
       "  329,\n",
       "  14,\n",
       "  379,\n",
       "  47,\n",
       "  541,\n",
       "  39,\n",
       "  333,\n",
       "  244,\n",
       "  18,\n",
       "  487,\n",
       "  32,\n",
       "  147,\n",
       "  349,\n",
       "  689,\n",
       "  392,\n",
       "  13,\n",
       "  352,\n",
       "  45,\n",
       "  495,\n",
       "  244,\n",
       "  1,\n",
       "  47,\n",
       "  548,\n",
       "  221,\n",
       "  149,\n",
       "  379,\n",
       "  24,\n",
       "  657,\n",
       "  256,\n",
       "  356,\n",
       "  149,\n",
       "  379,\n",
       "  1,\n",
       "  45,\n",
       "  511,\n",
       "  676,\n",
       "  12,\n",
       "  329,\n",
       "  144,\n",
       "  255,\n",
       "  325,\n",
       "  41,\n",
       "  387,\n",
       "  20,\n",
       "  541,\n",
       "  52,\n",
       "  687,\n",
       "  339,\n",
       "  423,\n",
       "  6,\n",
       "  163,\n",
       "  36,\n",
       "  263,\n",
       "  548,\n",
       "  18,\n",
       "  501,\n",
       "  424,\n",
       "  518,\n",
       "  11,\n",
       "  322,\n",
       "  687,\n",
       "  333,\n",
       "  248,\n",
       "  1,\n",
       "  39,\n",
       "  329,\n",
       "  159,\n",
       "  649,\n",
       "  41,\n",
       "  382,\n",
       "  126,\n",
       "  487,\n",
       "  13,\n",
       "  352,\n",
       "  45,\n",
       "  511,\n",
       "  13,\n",
       "  352,\n",
       "  45,\n",
       "  493,\n",
       "  190,\n",
       "  45,\n",
       "  491,\n",
       "  155,\n",
       "  12,\n",
       "  349,\n",
       "  687,\n",
       "  325,\n",
       "  1,\n",
       "  46,\n",
       "  521,\n",
       "  228,\n",
       "  329,\n",
       "  160,\n",
       "  1,\n",
       "  40,\n",
       "  352,\n",
       "  52,\n",
       "  676,\n",
       "  5,\n",
       "  147,\n",
       "  333,\n",
       "  269,\n",
       "  703,\n",
       "  2,\n",
       "  72,\n",
       "  495,\n",
       "  244,\n",
       "  41,\n",
       "  392,\n",
       "  379,\n",
       "  2,\n",
       "  55,\n",
       "  36,\n",
       "  255,\n",
       "  329,\n",
       "  160,\n",
       "  1,\n",
       "  41,\n",
       "  382,\n",
       "  126,\n",
       "  491,\n",
       "  136,\n",
       "  11,\n",
       "  305,\n",
       "  228,\n",
       "  339,\n",
       "  410,\n",
       "  10,\n",
       "  271,\n",
       "  46,\n",
       "  526,\n",
       "  360,\n",
       "  257,\n",
       "  383,\n",
       "  13,\n",
       "  356,\n",
       "  147,\n",
       "  339,\n",
       "  409,\n",
       "  133,\n",
       "  9,\n",
       "  261,\n",
       "  495,\n",
       "  262,\n",
       "  9,\n",
       "  262,\n",
       "  514,\n",
       "  29,\n",
       "  59,\n",
       "  147,\n",
       "  14,\n",
       "  393,\n",
       "  423,\n",
       "  487,\n",
       "  35,\n",
       "  1,\n",
       "  41,\n",
       "  392,\n",
       "  379,\n",
       "  29,\n",
       "  59,\n",
       "  147,\n",
       "  336,\n",
       "  329,\n",
       "  22,\n",
       "  595,\n",
       "  39,\n",
       "  329,\n",
       "  153,\n",
       "  495,\n",
       "  244,\n",
       "  5,\n",
       "  148,\n",
       "  356,\n",
       "  153,\n",
       "  505,\n",
       "  528,\n",
       "  419,\n",
       "  1,\n",
       "  31,\n",
       "  109,\n",
       "  39,\n",
       "  349,\n",
       "  689,\n",
       "  18,\n",
       "  511,\n",
       "  687,\n",
       "  329,\n",
       "  144,\n",
       "  250,\n",
       "  197,\n",
       "  5,\n",
       "  139,\n",
       "  113,\n",
       "  149,\n",
       "  5,\n",
       "  148,\n",
       "  356,\n",
       "  153,\n",
       "  505,\n",
       "  538,\n",
       "  689,\n",
       "  1,\n",
       "  41,\n",
       "  379,\n",
       "  46,\n",
       "  533,\n",
       "  541,\n",
       "  46,\n",
       "  522,\n",
       "  244,\n",
       "  11,\n",
       "  298,\n",
       "  52,\n",
       "  687,\n",
       "  325,\n",
       "  1,\n",
       "  39,\n",
       "  349,\n",
       "  694,\n",
       "  532,\n",
       "  514,\n",
       "  10,\n",
       "  291,\n",
       "  579,\n",
       "  333,\n",
       "  244,\n",
       "  41,\n",
       "  379,\n",
       "  3,\n",
       "  89,\n",
       "  217,\n",
       "  45,\n",
       "  498,\n",
       "  333,\n",
       "  248,\n",
       "  5,\n",
       "  154,\n",
       "  533,\n",
       "  548,\n",
       "  221,\n",
       "  153,\n",
       "  1,\n",
       "  45,\n",
       "  495,\n",
       "  248,\n",
       "  147,\n",
       "  3,\n",
       "  86,\n",
       "  138,\n",
       "  90,\n",
       "  255,\n",
       "  333,\n",
       "  244,\n",
       "  22,\n",
       "  595,\n",
       "  39,\n",
       "  329,\n",
       "  153,\n",
       "  495,\n",
       "  248,\n",
       "  1,\n",
       "  39,\n",
       "  333,\n",
       "  257,\n",
       "  379,\n",
       "  13,\n",
       "  366,\n",
       "  417,\n",
       "  336,\n",
       "  349,\n",
       "  18,\n",
       "  491,\n",
       "  140,\n",
       "  154,\n",
       "  518,\n",
       "  1,\n",
       "  39,\n",
       "  333,\n",
       "  268,\n",
       "  676,\n",
       "  35,\n",
       "  12,\n",
       "  333,\n",
       "  255,\n",
       "  336,\n",
       "  349,\n",
       "  16,\n",
       "  433,\n",
       "  45,\n",
       "  497,\n",
       "  302,\n",
       "  153,\n",
       "  6,\n",
       "  171,\n",
       "  257,\n",
       "  390,\n",
       "  329,\n",
       "  160,\n",
       "  13,\n",
       "  366,\n",
       "  423,\n",
       "  493,\n",
       "  190,\n",
       "  41,\n",
       "  19,\n",
       "  538,\n",
       "  679,\n",
       "  122,\n",
       "  383,\n",
       "  160,\n",
       "  10,\n",
       "  285,\n",
       "  423,\n",
       "  490,\n",
       "  133,\n",
       "  689,\n",
       "  5,\n",
       "  147,\n",
       "  339,\n",
       "  414,\n",
       "  262,\n",
       "  518,\n",
       "  20,\n",
       "  558,\n",
       "  495,\n",
       "  257,\n",
       "  387,\n",
       "  263,\n",
       "  565,\n",
       "  4,\n",
       "  ...],\n",
       " [13,\n",
       "  13,\n",
       "  1,\n",
       "  0,\n",
       "  12,\n",
       "  9,\n",
       "  22,\n",
       "  9,\n",
       "  1,\n",
       "  0,\n",
       "  22,\n",
       "  1,\n",
       "  0,\n",
       "  19,\n",
       "  1,\n",
       "  2,\n",
       "  5,\n",
       "  12,\n",
       "  12,\n",
       "  1,\n",
       "  0,\n",
       "  15,\n",
       "  16,\n",
       "  8,\n",
       "  9,\n",
       "  1,\n",
       "  0,\n",
       "  8,\n",
       "  1,\n",
       "  18,\n",
       "  12,\n",
       "  15,\n",
       "  20,\n",
       "  20,\n",
       "  5,\n",
       "  0,\n",
       "  9,\n",
       "  1,\n",
       "  0,\n",
       "  13,\n",
       "  5,\n",
       "  12,\n",
       "  9,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  18,\n",
       "  16,\n",
       "  5,\n",
       "  18,\n",
       "  0,\n",
       "  22,\n",
       "  5,\n",
       "  12,\n",
       "  25,\n",
       "  14,\n",
       "  0,\n",
       "  2,\n",
       "  9,\n",
       "  7,\n",
       "  1,\n",
       "  9,\n",
       "  12,\n",
       "  0,\n",
       "  13,\n",
       "  9,\n",
       "  12,\n",
       "  25,\n",
       "  0,\n",
       "  12,\n",
       "  9,\n",
       "  26,\n",
       "  1,\n",
       "  2,\n",
       "  5,\n",
       "  20,\n",
       "  8,\n",
       "  0,\n",
       "  9,\n",
       "  12,\n",
       "  1,\n",
       "  0,\n",
       "  12,\n",
       "  12,\n",
       "  1,\n",
       "  0,\n",
       "  22,\n",
       "  5,\n",
       "  18,\n",
       "  25,\n",
       "  0,\n",
       "  15,\n",
       "  6,\n",
       "  9,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  13,\n",
       "  9,\n",
       "  12,\n",
       "  1,\n",
       "  0,\n",
       "  18,\n",
       "  9,\n",
       "  1,\n",
       "  0,\n",
       "  3,\n",
       "  1,\n",
       "  18,\n",
       "  12,\n",
       "  5,\n",
       "  20,\n",
       "  20,\n",
       "  0,\n",
       "  9,\n",
       "  3,\n",
       "  20,\n",
       "  15,\n",
       "  18,\n",
       "  9,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  4,\n",
       "  9,\n",
       "  19,\n",
       "  15,\n",
       "  14,\n",
       "  0,\n",
       "  21,\n",
       "  14,\n",
       "  1,\n",
       "  0,\n",
       "  18,\n",
       "  1,\n",
       "  3,\n",
       "  5,\n",
       "  0,\n",
       "  8,\n",
       "  12,\n",
       "  15,\n",
       "  5,\n",
       "  0,\n",
       "  5,\n",
       "  14,\n",
       "  5,\n",
       "  12,\n",
       "  15,\n",
       "  16,\n",
       "  5,\n",
       "  0,\n",
       "  1,\n",
       "  25,\n",
       "  12,\n",
       "  1,\n",
       "  0,\n",
       "  9,\n",
       "  12,\n",
       "  5,\n",
       "  25,\n",
       "  0,\n",
       "  15,\n",
       "  5,\n",
       "  25,\n",
       "  0,\n",
       "  15,\n",
       "  18,\n",
       "  1,\n",
       "  0,\n",
       "  9,\n",
       "  12,\n",
       "  25,\n",
       "  0,\n",
       "  12,\n",
       "  5,\n",
       "  1,\n",
       "  14,\n",
       "  15,\n",
       "  18,\n",
       "  0,\n",
       "  1,\n",
       "  14,\n",
       "  14,\n",
       "  1,\n",
       "  8,\n",
       "  0,\n",
       "  9,\n",
       "  12,\n",
       "  12,\n",
       "  9,\n",
       "  1,\n",
       "  14,\n",
       "  0,\n",
       "  4,\n",
       "  4,\n",
       "  9,\n",
       "  19,\n",
       "  15,\n",
       "  14,\n",
       "  0,\n",
       "  21,\n",
       "  2,\n",
       "  18,\n",
       "  5,\n",
       "  25,\n",
       "  0,\n",
       "  12,\n",
       "  12,\n",
       "  9,\n",
       "  5,\n",
       "  0,\n",
       "  20,\n",
       "  5,\n",
       "  12,\n",
       "  12,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  20,\n",
       "  1,\n",
       "  12,\n",
       "  9,\n",
       "  5,\n",
       "  0,\n",
       "  15,\n",
       "  5,\n",
       "  0,\n",
       "  5,\n",
       "  1,\n",
       "  8,\n",
       "  0,\n",
       "  1,\n",
       "  26,\n",
       "  5,\n",
       "  12,\n",
       "  0,\n",
       "  9,\n",
       "  15,\n",
       "  12,\n",
       "  5,\n",
       "  20,\n",
       "  0,\n",
       "  21,\n",
       "  18,\n",
       "  15,\n",
       "  18,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  22,\n",
       "  1,\n",
       "  14,\n",
       "  14,\n",
       "  1,\n",
       "  8,\n",
       "  0,\n",
       "  21,\n",
       "  4,\n",
       "  18,\n",
       "  5,\n",
       "  25,\n",
       "  0,\n",
       "  18,\n",
       "  15,\n",
       "  15,\n",
       "  11,\n",
       "  12,\n",
       "  25,\n",
       "  14,\n",
       "  0,\n",
       "  5,\n",
       "  12,\n",
       "  12,\n",
       "  1,\n",
       "  0,\n",
       "  12,\n",
       "  1,\n",
       "  9,\n",
       "  18,\n",
       "  5,\n",
       "  0,\n",
       "  11,\n",
       "  25,\n",
       "  12,\n",
       "  1,\n",
       "  18,\n",
       "  0,\n",
       "  21,\n",
       "  3,\n",
       "  25,\n",
       "  0,\n",
       "  1,\n",
       "  9,\n",
       "  19,\n",
       "  12,\n",
       "  5,\n",
       "  25,\n",
       "  0,\n",
       "  22,\n",
       "  5,\n",
       "  18,\n",
       "  12,\n",
       "  25,\n",
       "  0,\n",
       "  14,\n",
       "  14,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  18,\n",
       "  15,\n",
       "  12,\n",
       "  9,\n",
       "  14,\n",
       "  5,\n",
       "  0,\n",
       "  15,\n",
       "  22,\n",
       "  1,\n",
       "  0,\n",
       "  5,\n",
       "  14,\n",
       "  5,\n",
       "  19,\n",
       "  9,\n",
       "  19,\n",
       "  0,\n",
       "  13,\n",
       "  9,\n",
       "  12,\n",
       "  9,\n",
       "  1,\n",
       "  0,\n",
       "  5,\n",
       "  14,\n",
       "  14,\n",
       "  5,\n",
       "  4,\n",
       "  25,\n",
       "  0,\n",
       "  1,\n",
       "  13,\n",
       "  1,\n",
       "  14,\n",
       "  20,\n",
       "  8,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  25,\n",
       "  1,\n",
       "  0,\n",
       "  9,\n",
       "  12,\n",
       "  12,\n",
       "  15,\n",
       "  23,\n",
       "  0,\n",
       "  9,\n",
       "  14,\n",
       "  19,\n",
       "  12,\n",
       "  5,\n",
       "  25,\n",
       "  0,\n",
       "  1,\n",
       "  15,\n",
       "  13,\n",
       "  9,\n",
       "  0,\n",
       "  1,\n",
       "  12,\n",
       "  9,\n",
       "  25,\n",
       "  1,\n",
       "  8,\n",
       "  0,\n",
       "  12,\n",
       "  5,\n",
       "  14,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  18,\n",
       "  1,\n",
       "  8,\n",
       "  0,\n",
       "  18,\n",
       "  9,\n",
       "  1,\n",
       "  14,\n",
       "  1,\n",
       "  0,\n",
       "  12,\n",
       "  12,\n",
       "  9,\n",
       "  19,\n",
       "  15,\n",
       "  14,\n",
       "  0,\n",
       "  1,\n",
       "  2,\n",
       "  18,\n",
       "  9,\n",
       "  5,\n",
       "  12,\n",
       "  12,\n",
       "  1,\n",
       "  0,\n",
       "  12,\n",
       "  9,\n",
       "  3,\n",
       "  5,\n",
       "  0,\n",
       "  1,\n",
       "  4,\n",
       "  5,\n",
       "  12,\n",
       "  25,\n",
       "  14,\n",
       "  0,\n",
       "  15,\n",
       "  18,\n",
       "  1,\n",
       "  0,\n",
       "  21,\n",
       "  2,\n",
       "  25,\n",
       "  0,\n",
       "  22,\n",
       "  1,\n",
       "  0,\n",
       "  5,\n",
       "  18,\n",
       "  5,\n",
       "  14,\n",
       "  9,\n",
       "  20,\n",
       "  25,\n",
       "  0,\n",
       "  21,\n",
       "  20,\n",
       "  21,\n",
       "  13,\n",
       "  14,\n",
       "  0,\n",
       "  4,\n",
       "  5,\n",
       "  12,\n",
       "  9,\n",
       "  14,\n",
       "  5,\n",
       "  0,\n",
       "  1,\n",
       "  9,\n",
       "  12,\n",
       "  5,\n",
       "  25,\n",
       "  0,\n",
       "  9,\n",
       "  1,\n",
       "  14,\n",
       "  14,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  12,\n",
       "  5,\n",
       "  14,\n",
       "  20,\n",
       "  9,\n",
       "  14,\n",
       "  1,\n",
       "  0,\n",
       "  19,\n",
       "  12,\n",
       "  1,\n",
       "  0,\n",
       "  12,\n",
       "  9,\n",
       "  1,\n",
       "  14,\n",
       "  1,\n",
       "  0,\n",
       "  21,\n",
       "  9,\n",
       "  14,\n",
       "  14,\n",
       "  0,\n",
       "  5,\n",
       "  22,\n",
       "  1,\n",
       "  5,\n",
       "  8,\n",
       "  0,\n",
       "  22,\n",
       "  25,\n",
       "  0,\n",
       "  1,\n",
       "  4,\n",
       "  9,\n",
       "  5,\n",
       "  0,\n",
       "  9,\n",
       "  16,\n",
       "  5,\n",
       "  18,\n",
       "  0,\n",
       "  25,\n",
       "  4,\n",
       "  9,\n",
       "  1,\n",
       "  0,\n",
       "  12,\n",
       "  5,\n",
       "  24,\n",
       "  1,\n",
       "  0,\n",
       "  15,\n",
       "  19,\n",
       "  5,\n",
       "  16,\n",
       "  8,\n",
       "  9,\n",
       "  14,\n",
       "  5,\n",
       "  0,\n",
       "  13,\n",
       "  5,\n",
       "  18,\n",
       "  25,\n",
       "  0,\n",
       "  21,\n",
       "  12,\n",
       "  9,\n",
       "  1,\n",
       "  0,\n",
       "  5,\n",
       "  12,\n",
       "  9,\n",
       "  12,\n",
       "  1,\n",
       "  8,\n",
       "  0,\n",
       "  18,\n",
       "  9,\n",
       "  1,\n",
       "  14,\n",
       "  14,\n",
       "  1,\n",
       "  0,\n",
       "  9,\n",
       "  22,\n",
       "  9,\n",
       "  1,\n",
       "  14,\n",
       "  0,\n",
       "  1,\n",
       "  25,\n",
       "  12,\n",
       "  5,\n",
       "  5,\n",
       "  0,\n",
       "  15,\n",
       "  16,\n",
       "  8,\n",
       "  9,\n",
       "  5,\n",
       "  0,\n",
       "  18,\n",
       "  9,\n",
       "  5,\n",
       "  12,\n",
       "  12,\n",
       "  5,\n",
       "  0,\n",
       "  1,\n",
       "  4,\n",
       "  5,\n",
       "  12,\n",
       "  9,\n",
       "  14,\n",
       "  5,\n",
       "  0,\n",
       "  5,\n",
       "  25,\n",
       "  20,\n",
       "  15,\n",
       "  14,\n",
       "  0,\n",
       "  25,\n",
       "  12,\n",
       "  5,\n",
       "  5,\n",
       "  0,\n",
       "  12,\n",
       "  1,\n",
       "  18,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  4,\n",
       "  12,\n",
       "  5,\n",
       "  25,\n",
       "  0,\n",
       "  5,\n",
       "  12,\n",
       "  1,\n",
       "  14,\n",
       "  9,\n",
       "  5,\n",
       "  0,\n",
       "  1,\n",
       "  3,\n",
       "  11,\n",
       "  5,\n",
       "  14,\n",
       "  26,\n",
       "  9,\n",
       "  5,\n",
       "  0,\n",
       "  5,\n",
       "  1,\n",
       "  7,\n",
       "  1,\n",
       "  14,\n",
       "  0,\n",
       "  4,\n",
       "  1,\n",
       "  12,\n",
       "  25,\n",
       "  14,\n",
       "  14,\n",
       "  0,\n",
       "  9,\n",
       "  12,\n",
       "  9,\n",
       "  1,\n",
       "  14,\n",
       "  1,\n",
       "  0,\n",
       "  21,\n",
       "  2,\n",
       "  18,\n",
       "  5,\n",
       "  5,\n",
       "  0,\n",
       "  1,\n",
       "  4,\n",
       "  5,\n",
       "  0,\n",
       "  1,\n",
       "  20,\n",
       "  8,\n",
       "  5,\n",
       "  18,\n",
       "  9,\n",
       "  14,\n",
       "  5,\n",
       "  0,\n",
       "  19,\n",
       "  1,\n",
       "  2,\n",
       "  5,\n",
       "  12,\n",
       "  12,\n",
       "  5,\n",
       "  0,\n",
       "  1,\n",
       "  20,\n",
       "  1,\n",
       "  12,\n",
       "  9,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  5,\n",
       "  12,\n",
       "  25,\n",
       "  14,\n",
       "  14,\n",
       "  0,\n",
       "  1,\n",
       "  18,\n",
       "  9,\n",
       "  1,\n",
       "  0,\n",
       "  20,\n",
       "  8,\n",
       "  5,\n",
       "  14,\n",
       "  1,\n",
       "  0,\n",
       "  9,\n",
       "  13,\n",
       "  5,\n",
       "  14,\n",
       "  1,\n",
       "  0,\n",
       "  18,\n",
       "  25,\n",
       "  1,\n",
       "  0,\n",
       "  5,\n",
       "  9,\n",
       "  12,\n",
       "  1,\n",
       "  14,\n",
       "  9,\n",
       "  0,\n",
       "  1,\n",
       "  25,\n",
       "  12,\n",
       "  15,\n",
       "  18,\n",
       "  0,\n",
       "  1,\n",
       "  9,\n",
       "  20,\n",
       "  8,\n",
       "  0,\n",
       "  15,\n",
       "  19,\n",
       "  5,\n",
       "  0,\n",
       "  25,\n",
       "  12,\n",
       "  9,\n",
       "  5,\n",
       "  0,\n",
       "  12,\n",
       "  5,\n",
       "  24,\n",
       "  1,\n",
       "  14,\n",
       "  4,\n",
       "  18,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  18,\n",
       "  25,\n",
       "  0,\n",
       "  1,\n",
       "  18,\n",
       "  7,\n",
       "  1,\n",
       "  18,\n",
       "  5,\n",
       "  20,\n",
       "  0,\n",
       "  25,\n",
       "  12,\n",
       "  1,\n",
       "  0,\n",
       "  19,\n",
       "  8,\n",
       "  12,\n",
       "  5,\n",
       "  25,\n",
       "  0,\n",
       "  13,\n",
       "  1,\n",
       "  25,\n",
       "  1,\n",
       "  0,\n",
       "  12,\n",
       "  9,\n",
       "  26,\n",
       "  1,\n",
       "  0,\n",
       "  18,\n",
       "  9,\n",
       "  1,\n",
       "  14,\n",
       "  14,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  9,\n",
       "  12,\n",
       "  5,\n",
       "  25,\n",
       "  0,\n",
       "  14,\n",
       "  4,\n",
       "  18,\n",
       "  5,\n",
       "  1,\n",
       "  0,\n",
       "  8,\n",
       "  12,\n",
       "  15,\n",
       "  5,\n",
       "  0,\n",
       "  1,\n",
       "  19,\n",
       "  13,\n",
       "  9,\n",
       "  14,\n",
       "  5,\n",
       "  0,\n",
       "  5,\n",
       "  12,\n",
       "  15,\n",
       "  4,\n",
       "  25,\n",
       "  0,\n",
       "  18,\n",
       "  9,\n",
       "  19,\n",
       "  0,\n",
       "  19,\n",
       "  1,\n",
       "  2,\n",
       "  5,\n",
       "  12,\n",
       "  0,\n",
       "  15,\n",
       "  18,\n",
       "  1,\n",
       "  8,\n",
       "  0,\n",
       "  14,\n",
       "  14,\n",
       "  1,\n",
       "  2,\n",
       "  5,\n",
       "  12,\n",
       "  12,\n",
       "  5,\n",
       "  0,\n",
       "  1,\n",
       "  12,\n",
       "  5,\n",
       "  18,\n",
       "  9,\n",
       "  1,\n",
       "  0,\n",
       "  13,\n",
       "  5,\n",
       "  18,\n",
       "  19,\n",
       "  15,\n",
       "  14,\n",
       "  0,\n",
       "  4,\n",
       "  1,\n",
       "  12,\n",
       "  25,\n",
       "  14,\n",
       "  0,\n",
       "  25,\n",
       "  12,\n",
       "  5,\n",
       "  9,\n",
       "  7,\n",
       "  8,\n",
       "  0,\n",
       "  4,\n",
       "  5,\n",
       "  14,\n",
       "  0,\n",
       "  13,\n",
       "  5,\n",
       "  18,\n",
       "  19,\n",
       "  25,\n",
       "  14,\n",
       "  0,\n",
       "  14,\n",
       "  1,\n",
       "  19,\n",
       "  20,\n",
       "  1,\n",
       "  19,\n",
       "  9,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  25,\n",
       "  12,\n",
       "  1,\n",
       "  0,\n",
       "  12,\n",
       "  25,\n",
       "  19,\n",
       "  19,\n",
       "  1,\n",
       "  0,\n",
       "  21,\n",
       "  12,\n",
       "  9,\n",
       "  1,\n",
       "  14,\n",
       "  1,\n",
       "  0,\n",
       "  8,\n",
       "  1,\n",
       "  18,\n",
       "  12,\n",
       "  9,\n",
       "  5,\n",
       "  0,\n",
       "  19,\n",
       "  20,\n",
       "  8,\n",
       "  5,\n",
       "  18,\n",
       "  0,\n",
       "  18,\n",
       "  9,\n",
       "  5,\n",
       "  12,\n",
       "  0,\n",
       "  5,\n",
       "  3,\n",
       "  9,\n",
       "  12,\n",
       "  9,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  12,\n",
       "  5,\n",
       "  18,\n",
       "  9,\n",
       "  5,\n",
       "  0,\n",
       "  12,\n",
       "  9,\n",
       "  14,\n",
       "  1,\n",
       "  0,\n",
       "  15,\n",
       "  12,\n",
       "  12,\n",
       "  25,\n",
       "  0,\n",
       "  5,\n",
       "  5,\n",
       "  19,\n",
       "  5,\n",
       "  0,\n",
       "  12,\n",
       "  9,\n",
       "  25,\n",
       "  1,\n",
       "  8,\n",
       "  0,\n",
       "  9,\n",
       "  12,\n",
       "  12,\n",
       "  25,\n",
       "  0,\n",
       "  1,\n",
       "  18,\n",
       "  11,\n",
       "  5,\n",
       "  18,\n",
       "  0,\n",
       "  9,\n",
       "  14,\n",
       "  12,\n",
       "  5,\n",
       "  25,\n",
       "  0,\n",
       "  15,\n",
       "  18,\n",
       "  7,\n",
       "  1,\n",
       "  14,\n",
       "  0,\n",
       "  25,\n",
       "  4,\n",
       "  14,\n",
       "  5,\n",
       "  25,\n",
       "  0,\n",
       "  15,\n",
       "  18,\n",
       "  4,\n",
       "  25,\n",
       "  14,\n",
       "  0,\n",
       "  12,\n",
       "  15,\n",
       "  9,\n",
       "  19,\n",
       "  5,\n",
       "  0,\n",
       "  18,\n",
       "  9,\n",
       "  14,\n",
       "  9,\n",
       "  20,\n",
       "  25,\n",
       "  0,\n",
       "  1,\n",
       "  ...])"
      ]
     },
     "execution_count": 424,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs = []\n",
    "ys = []\n",
    "for w in words:\n",
    "    wdot = f'.{w}.'\n",
    "    for i in range(len(wdot) - 2):\n",
    "        pair = wdot[i] + wdot[i+1]\n",
    "        xs.append(ptoi[pair])\n",
    "        ys.append(stoi[wdot[i+2]])\n",
    "\n",
    "xs, ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = torch.tensor(xs).float()\n",
    "ys = torch.tensor(ys).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.3091, grad_fn=<DivBackward1>)\n",
      "tensor(3.1411, grad_fn=<DivBackward1>)\n",
      "tensor(3.0107, grad_fn=<DivBackward1>)\n",
      "tensor(2.9063, grad_fn=<DivBackward1>)\n",
      "tensor(2.8301, grad_fn=<DivBackward1>)\n",
      "tensor(2.7835, grad_fn=<DivBackward1>)\n",
      "tensor(2.7566, grad_fn=<DivBackward1>)\n",
      "tensor(2.7404, grad_fn=<DivBackward1>)\n",
      "tensor(2.7300, grad_fn=<DivBackward1>)\n",
      "tensor(2.7231, grad_fn=<DivBackward1>)\n",
      "tensor(2.7183, grad_fn=<DivBackward1>)\n",
      "tensor(2.7148, grad_fn=<DivBackward1>)\n",
      "tensor(2.7121, grad_fn=<DivBackward1>)\n",
      "tensor(2.7101, grad_fn=<DivBackward1>)\n",
      "tensor(2.7084, grad_fn=<DivBackward1>)\n",
      "tensor(2.7071, grad_fn=<DivBackward1>)\n",
      "tensor(2.7059, grad_fn=<DivBackward1>)\n",
      "tensor(2.7050, grad_fn=<DivBackward1>)\n",
      "tensor(2.7041, grad_fn=<DivBackward1>)\n",
      "tensor(2.7034, grad_fn=<DivBackward1>)\n",
      "tensor(2.7027, grad_fn=<DivBackward1>)\n",
      "tensor(2.7021, grad_fn=<DivBackward1>)\n",
      "tensor(2.7015, grad_fn=<DivBackward1>)\n",
      "tensor(2.7010, grad_fn=<DivBackward1>)\n",
      "tensor(2.7004, grad_fn=<DivBackward1>)\n",
      "tensor(2.6999, grad_fn=<DivBackward1>)\n",
      "tensor(2.6994, grad_fn=<DivBackward1>)\n",
      "tensor(2.6990, grad_fn=<DivBackward1>)\n",
      "tensor(2.6985, grad_fn=<DivBackward1>)\n",
      "tensor(2.6981, grad_fn=<DivBackward1>)\n",
      "tensor(2.6976, grad_fn=<DivBackward1>)\n",
      "tensor(2.6972, grad_fn=<DivBackward1>)\n",
      "tensor(2.6967, grad_fn=<DivBackward1>)\n",
      "tensor(2.6965, grad_fn=<DivBackward1>)\n",
      "tensor(2.6961, grad_fn=<DivBackward1>)\n",
      "tensor(2.6962, grad_fn=<DivBackward1>)\n",
      "tensor(2.6964, grad_fn=<DivBackward1>)\n",
      "tensor(2.6986, grad_fn=<DivBackward1>)\n",
      "tensor(2.7017, grad_fn=<DivBackward1>)\n",
      "tensor(2.7132, grad_fn=<DivBackward1>)\n",
      "tensor(2.7232, grad_fn=<DivBackward1>)\n",
      "tensor(2.7536, grad_fn=<DivBackward1>)\n",
      "tensor(2.7388, grad_fn=<DivBackward1>)\n",
      "tensor(2.7424, grad_fn=<DivBackward1>)\n",
      "tensor(2.7180, grad_fn=<DivBackward1>)\n",
      "tensor(2.7164, grad_fn=<DivBackward1>)\n",
      "tensor(2.7082, grad_fn=<DivBackward1>)\n",
      "tensor(2.7103, grad_fn=<DivBackward1>)\n",
      "tensor(2.7050, grad_fn=<DivBackward1>)\n",
      "tensor(2.7085, grad_fn=<DivBackward1>)\n",
      "tensor(2.7041, grad_fn=<DivBackward1>)\n",
      "tensor(2.7087, grad_fn=<DivBackward1>)\n",
      "tensor(2.7043, grad_fn=<DivBackward1>)\n",
      "tensor(2.7099, grad_fn=<DivBackward1>)\n",
      "tensor(2.7048, grad_fn=<DivBackward1>)\n",
      "tensor(2.7108, grad_fn=<DivBackward1>)\n",
      "tensor(2.7049, grad_fn=<DivBackward1>)\n",
      "tensor(2.7110, grad_fn=<DivBackward1>)\n",
      "tensor(2.7044, grad_fn=<DivBackward1>)\n",
      "tensor(2.7104, grad_fn=<DivBackward1>)\n",
      "tensor(2.7035, grad_fn=<DivBackward1>)\n",
      "tensor(2.7095, grad_fn=<DivBackward1>)\n",
      "tensor(2.7027, grad_fn=<DivBackward1>)\n",
      "tensor(2.7087, grad_fn=<DivBackward1>)\n",
      "tensor(2.7018, grad_fn=<DivBackward1>)\n",
      "tensor(2.7080, grad_fn=<DivBackward1>)\n",
      "tensor(2.7013, grad_fn=<DivBackward1>)\n",
      "tensor(2.7078, grad_fn=<DivBackward1>)\n",
      "tensor(2.7007, grad_fn=<DivBackward1>)\n",
      "tensor(2.7075, grad_fn=<DivBackward1>)\n",
      "tensor(2.7003, grad_fn=<DivBackward1>)\n",
      "tensor(2.7071, grad_fn=<DivBackward1>)\n",
      "tensor(2.6997, grad_fn=<DivBackward1>)\n",
      "tensor(2.7066, grad_fn=<DivBackward1>)\n",
      "tensor(2.6992, grad_fn=<DivBackward1>)\n",
      "tensor(2.7063, grad_fn=<DivBackward1>)\n",
      "tensor(2.6984, grad_fn=<DivBackward1>)\n",
      "tensor(2.7056, grad_fn=<DivBackward1>)\n",
      "tensor(2.6980, grad_fn=<DivBackward1>)\n",
      "tensor(2.7054, grad_fn=<DivBackward1>)\n",
      "tensor(2.6974, grad_fn=<DivBackward1>)\n",
      "tensor(2.7050, grad_fn=<DivBackward1>)\n",
      "tensor(2.6969, grad_fn=<DivBackward1>)\n",
      "tensor(2.7045, grad_fn=<DivBackward1>)\n",
      "tensor(2.6963, grad_fn=<DivBackward1>)\n",
      "tensor(2.7039, grad_fn=<DivBackward1>)\n",
      "tensor(2.6958, grad_fn=<DivBackward1>)\n",
      "tensor(2.7035, grad_fn=<DivBackward1>)\n",
      "tensor(2.6951, grad_fn=<DivBackward1>)\n",
      "tensor(2.7028, grad_fn=<DivBackward1>)\n",
      "tensor(2.6946, grad_fn=<DivBackward1>)\n",
      "tensor(2.7026, grad_fn=<DivBackward1>)\n",
      "tensor(2.6941, grad_fn=<DivBackward1>)\n",
      "tensor(2.7019, grad_fn=<DivBackward1>)\n",
      "tensor(2.6936, grad_fn=<DivBackward1>)\n",
      "tensor(2.7015, grad_fn=<DivBackward1>)\n",
      "tensor(2.6930, grad_fn=<DivBackward1>)\n",
      "tensor(2.7010, grad_fn=<DivBackward1>)\n",
      "tensor(2.6925, grad_fn=<DivBackward1>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[437], line 32\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m     31\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m---> 32\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m        \u001b[49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mone_hot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint64\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_pairs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m        \u001b[49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mone_hot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint64\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredictable_chars\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28mprint\u001b[39m(loss)\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;66;03m# for batch in dls.train:\u001b[39;00m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;66;03m#     xb, yb = batch\u001b[39;00m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;66;03m#     xb = to_bw_flattened(xb)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     42\u001b[0m \n\u001b[1;32m     43\u001b[0m     \u001b[38;5;66;03m# print(f\"Avg loss: {tot_loss / valid_batches}, Avg acc: {tot_acc / valid_batches}\")\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[437], line 17\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(xb, yb)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_step\u001b[39m(xb, yb):\n\u001b[0;32m---> 17\u001b[0m     preds \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_fn(preds, yb)\n\u001b[1;32m     20\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/mambaforge-pypy3/envs/ai-base/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge-pypy3/envs/ai-base/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge-pypy3/envs/ai-base/lib/python3.11/site-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/mambaforge-pypy3/envs/ai-base/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge-pypy3/envs/ai-base/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge-pypy3/envs/ai-base/lib/python3.11/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hidden_neurons = 100\n",
    "predictable_chars = len(letters) + 1\n",
    "total_pairs = len(ptoi)\n",
    "\n",
    "epochs = 100\n",
    "lr = 3\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(total_pairs, hidden_neurons),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(hidden_neurons, predictable_chars),\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr, weight_decay=0.01)\n",
    "\n",
    "def train_step(xb, yb):\n",
    "    preds = model(xb)\n",
    "    loss = loss_fn(preds, yb)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    return loss\n",
    "\n",
    "# def valid_step(xb):\n",
    "#     preds = model(xb)\n",
    "#     loss = loss_fn(preds)\n",
    "#     return loss\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    loss = train_step(\n",
    "        F.one_hot(xs.to(torch.int64), total_pairs).float(), \n",
    "        F.one_hot(ys.to(torch.int64), predictable_chars).float()\n",
    "    )\n",
    "    print(loss)\n",
    "    # for batch in dls.train:\n",
    "    #     xb, yb = batch\n",
    "    #     xb = to_bw_flattened(xb)\n",
    "    #     xb, yb = to_torch_tensor(xb, yb)\n",
    "    #     loss, acc = train_step(xb, yb)\n",
    "\n",
    "    # print(f\"Avg loss: {tot_loss / valid_batches}, Avg acc: {tot_acc / valid_batches}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.a',\n",
       " '.b',\n",
       " '.c',\n",
       " '.d',\n",
       " '.e',\n",
       " '.f',\n",
       " '.g',\n",
       " '.h',\n",
       " '.i',\n",
       " '.j',\n",
       " '.k',\n",
       " '.l',\n",
       " '.m',\n",
       " '.n',\n",
       " '.o',\n",
       " '.p',\n",
       " '.q',\n",
       " '.r',\n",
       " '.s',\n",
       " '.t',\n",
       " '.u',\n",
       " '.v',\n",
       " '.w',\n",
       " '.x',\n",
       " '.y',\n",
       " '.z']"
      ]
     },
     "execution_count": 421,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "starters = [p for p in pairs if p[0] == '.' and p != '..']\n",
    "starters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xeeoimoavnbask\n",
      "kl\n",
      "bwa\n",
      "hahihgalf\n",
      "badjapnilen\n",
      "gaaitnaue\n",
      "vuhyiarngya\n",
      "wdainadiabturs\n",
      "kslmyianeauhla\n",
      "msnvoinennerga\n",
      "keahmceavaptay\n",
      "cyxoeseiarasan\n",
      "fcllua\n",
      "kreeanneqneaaa\n",
      "ee\n",
      "heai\n",
      "llayzaf\n",
      "mrylaxlnonlt\n",
      "eeeayro\n",
      "keaelnnha\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "words_to_gen = 20\n",
    "for wi in range(words_to_gen):\n",
    "    # word_len = torch.randint(min(word_lens), max(word_lens), (1,)).item()\n",
    "    # for i in range(int(word_len)):\n",
    "    li = 0\n",
    "    next_letter = ''\n",
    "    # e.g. \".a\"\n",
    "    genned_word = starters[random.randint(0, len(starters) - 1)]\n",
    "    while next_letter != '.' and len(genned_word) < max(word_lens):\n",
    "        prior_pair = genned_word[li] + genned_word[li+1]\n",
    "        prior_pair_i = ptoi[prior_pair]\n",
    "        prior_pair_i_one_hot = F.one_hot(\n",
    "            torch.tensor(prior_pair_i), total_pairs\n",
    "        ).float()\n",
    "\n",
    "        probs = torch.nn.Softmax()(model(prior_pair_i_one_hot))\n",
    "        pred = torch.multinomial(probs, 1, replacement=True).item()\n",
    "        next_letter = itos[int(pred)]\n",
    "\n",
    "        # next_sample_p = P[prior_letter_i]\n",
    "        # next_letter_idx = torch.multinomial(\n",
    "        #         next_sample_p, 1, replacement=True, generator=g\n",
    "        #     ).item()\n",
    "        # next_letter = itos[next_letter_idx]\n",
    "        genned_word += next_letter\n",
    "        li += 1\n",
    "    print(genned_word.strip('.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Test Split\n",
    "\n",
    "E02: split up the dataset randomly into 80% train set, 10% dev set, 10% test set. Train the bigram and trigram models only on the training set. Evaluate them on dev and test splits. What can you see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordsDataset(Dataset):\n",
    "    def __init__(self, words_file_dir):\n",
    "        self.words_file_dir = words_file_dir\n",
    "        self.words = open(words_file_dir).read().splitlines()\n",
    "        \n",
    "        xs = []\n",
    "        ys = []\n",
    "        for w in self.words:\n",
    "            wdot = f'.{w}.'\n",
    "            for i in range(len(wdot) - 2):\n",
    "                pair = wdot[i] + wdot[i+1]\n",
    "                xs.append(ptoi[pair])\n",
    "                ys.append(stoi[wdot[i+2]])\n",
    "\n",
    "        self.xs = torch.tensor(xs)\n",
    "        self.ys = torch.tensor(ys)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.xs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # return F.one_hot(self.xs[idx], total_pairs).float(), F.one_hot(self.ys[idx], predictable_chars).float()\n",
    "        return F.one_hot(self.xs[idx], total_pairs).float(), self.ys[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(156891, 19611, 19611)"
      ]
     },
     "execution_count": 531,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_data = WordsDataset('names.txt')\n",
    "train_data, dev_data, test_data = torch.utils.data.random_split(\n",
    "    full_data, [0.8, 0.1, 0.1]\n",
    ")\n",
    "len(train_data), len(dev_data), len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_data, batch_size=256, shuffle=True)\n",
    "dev_dl = DataLoader(dev_data, batch_size=256, shuffle=True)\n",
    "test_dl = DataLoader(test_data, batch_size=256, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1; Train loss: 2.478865760380059; Valid loss: 2.3297139112051433\n",
      "Epoch: 2; Train loss: 2.240069600147291; Valid loss: 2.2234733507230686\n",
      "Epoch: 3; Train loss: 2.1854187545729693; Valid loss: 2.195639046755704\n",
      "Epoch: 4; Train loss: 2.1607985934075487; Valid loss: 2.176535850995547\n",
      "Epoch: 5; Train loss: 2.146448504088559; Valid loss: 2.1770438095191857\n",
      "Epoch: 6; Train loss: 2.1334736592228998; Valid loss: 2.1644874176421722\n",
      "Epoch: 7; Train loss: 2.126377756599501; Valid loss: 2.158960958579918\n",
      "Epoch: 8; Train loss: 2.11990780931507; Valid loss: 2.152849623135158\n",
      "Epoch: 9; Train loss: 2.115403890609741; Valid loss: 2.1530039852315728\n",
      "Epoch: 10; Train loss: 2.112162614335438; Valid loss: 2.163072694431652\n",
      "Epoch: 11; Train loss: 2.108507923745409; Valid loss: 2.1507596567079617\n",
      "Epoch: 12; Train loss: 2.1063981013430273; Valid loss: 2.164969112965968\n",
      "Epoch: 13; Train loss: 2.1039171353055446; Valid loss: 2.1443202077568353\n",
      "Epoch: 14; Train loss: 2.102816231682483; Valid loss: 2.143884626301852\n",
      "Epoch: 15; Train loss: 2.0999715205701297; Valid loss: 2.1414537878779623\n",
      "Epoch: 16; Train loss: 2.0992738691084147; Valid loss: 2.138731772249395\n",
      "Epoch: 17; Train loss: 2.097322661374949; Valid loss: 2.1471016190268775\n",
      "Epoch: 18; Train loss: 2.096865316977509; Valid loss: 2.1535281159661035\n",
      "Epoch: 19; Train loss: 2.0952138278457118; Valid loss: 2.1389693207555003\n",
      "Epoch: 20; Train loss: 2.09471922928124; Valid loss: 2.1490364059225304\n",
      "Test loss: 2.1206584549569465\n"
     ]
    }
   ],
   "source": [
    "hidden_neurons = 100\n",
    "predictable_chars = len(letters) + 1\n",
    "total_pairs = len(ptoi)\n",
    "\n",
    "epochs = 20\n",
    "lr = 0.9\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(total_pairs, hidden_neurons),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(hidden_neurons, predictable_chars),\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr, weight_decay=0.00001)\n",
    "\n",
    "def train_step(xb, yb):\n",
    "    preds = model(xb)\n",
    "    loss = loss_fn(preds, yb)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    return loss\n",
    "\n",
    "def valid_step(xb, yb):\n",
    "    preds = model(xb)\n",
    "    loss = loss_fn(preds, yb)\n",
    "    return loss\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    tot_train_loss = 0\n",
    "    for batch in train_dl:\n",
    "        xb, yb = batch\n",
    "        loss = train_step(xb, yb)\n",
    "        tot_train_loss += loss.item()\n",
    "\n",
    "    model.eval()\n",
    "    tot_dev_loss = 0\n",
    "    with torch.inference_mode():\n",
    "        for batch in dev_dl:\n",
    "            xb, yb = batch\n",
    "            loss = valid_step(xb, yb)\n",
    "            tot_dev_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch: {epoch+1}; Train loss: {tot_train_loss / len(train_dl)}; Valid loss: {tot_dev_loss / len(dev_dl)}\")\n",
    "\n",
    "model.eval()\n",
    "tot_test_loss = 0\n",
    "with torch.inference_mode():\n",
    "    for batch in test_dl:\n",
    "        xb, yb = batch\n",
    "        loss = valid_step(xb, yb)\n",
    "        tot_test_loss += loss.item()\n",
    "\n",
    "print(f\"Test loss: {tot_test_loss / len(test_dl)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test loss with 10 epochs:\n",
    "* lr 0.5 and weight_decay 0.01 = 2.58\n",
    "* lr 0.5 and weight_decay 0.05 = 2.76 (basically didn't train)\n",
    "* lr 0.5 and weight_decay 0.005 = 2.44 (way better)\n",
    "* lr 0.5 and weight_decay 0.001 = 2.16 (still improving)\n",
    "* lr 0.5 and weight_decay 0.0001 = 2.15 (marginal gain)\n",
    "* lr 0.5 and weight_decay 0.00001 = 2.13\n",
    "* lr 0.5 and weight_decay 0.000001 = 2.13 (sticking with prior)\n",
    "* lr 0.9 and weight_decay 0.00001 = 2.12 **best**\n",
    "* lr 1.5 and weight_decay 0.00001 = 2.135 (starting to overfit? train was still going down)\n",
    "* lr 1.5 and weight_decay 0.0001 = 2.130 (lower lr was better)\n",
    "\n",
    "Test loss with 20 epochs at best settings:\n",
    "2.12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ter\n",
      "maxona\n",
      "riazah\n",
      "gus\n",
      "ka\n",
      "jaxlesh\n",
      "arucrohermi\n",
      "wa\n",
      "bra\n",
      "farielyn\n",
      "dee\n",
      "lukand\n",
      "eli\n",
      "kyn\n",
      "nasi\n",
      "jabenn\n",
      "bramarsyra\n",
      "araemai\n",
      "kia\n",
      "wilayzriyah\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "words_to_gen = 20\n",
    "for wi in range(words_to_gen):\n",
    "    li = 0\n",
    "    next_letter = ''\n",
    "    # e.g. \".a\"\n",
    "    genned_word = starters[random.randint(0, len(starters) - 1)]\n",
    "    while next_letter != '.' and len(genned_word) < max(word_lens):\n",
    "        prior_pair = genned_word[li] + genned_word[li+1]\n",
    "        prior_pair_i = ptoi[prior_pair]\n",
    "        prior_pair_i_one_hot = F.one_hot(\n",
    "            torch.tensor(prior_pair_i), total_pairs\n",
    "        ).float()\n",
    "\n",
    "        probs = torch.nn.Softmax()(model(prior_pair_i_one_hot))\n",
    "        pred = torch.multinomial(probs, 1, replacement=True).item()\n",
    "        next_letter = itos[int(pred)]\n",
    "\n",
    "        genned_word += next_letter\n",
    "        li += 1\n",
    "    print(genned_word.strip('.'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
